export default "{\"documentCount\":263,\"nextId\":263,\"documentIds\":{\"0\":\"v-5473213a\",\"1\":\"v-5473213a#解决过程\",\"2\":\"v-4f486762\",\"3\":\"v-89132eea\",\"4\":\"v-7f6d871a\",\"5\":\"v-7f6d871a#一、封装\",\"6\":\"v-7f6d871a#使用封装的优点\",\"7\":\"v-7f6d871a#二、继承\",\"8\":\"v-7f6d871a#谨慎继承\",\"9\":\"v-7f6d871a#三、多态\",\"10\":\"v-7f6d871a#多态的实现条件\",\"11\":\"v-7f6d871a#实现多态的方法\",\"12\":\"v-7f6d871a#实例分析\",\"13\":\"v-31927cf5\",\"14\":\"v-31927cf5#一、主要研究点\",\"15\":\"v-31927cf5#二、transformer\",\"16\":\"v-31927cf5#_2-1-序列转换模型\",\"17\":\"v-31927cf5#_2-2-注意力机制\",\"18\":\"v-31927cf5#_2-3-transformer模型\",\"19\":\"v-31927cf5#三、bert\",\"20\":\"v-31927cf5#_3-1-bert与transformer\",\"21\":\"v-31927cf5#_3-2-input-embedding\",\"22\":\"v-31927cf5#_3-3-预训练bert\",\"23\":\"v-31927cf5#四、t5和gpt\",\"24\":\"v-31927cf5#_4-1-t5模型\",\"25\":\"v-31927cf5#_4-2-gpt模型\",\"26\":\"v-31927cf5#五、glm\",\"27\":\"v-31927cf5#_5-1-自回归填空\",\"28\":\"v-31927cf5#_5-2-二维位置编码\",\"29\":\"v-31927cf5#_5-3-glm与transformer\",\"30\":\"v-31927cf5#六、p-tuning-v2\",\"31\":\"v-31927cf5#_6-1-提示微调\",\"32\":\"v-31927cf5#_6-2-p-tuning-v2\",\"33\":\"v-31927cf5#参考文献\",\"34\":\"v-ee3a5d5c\",\"35\":\"v-ee3a5d5c#参考资料\",\"36\":\"v-586d9498\",\"37\":\"v-586d9498#_1-介绍\",\"38\":\"v-586d9498#_2-预训练方法\",\"39\":\"v-586d9498#_2-1-训练数据\",\"40\":\"v-586d9498#_2-2-训练详情\",\"41\":\"v-586d9498#a-2-1-额外的预训练信息之与llama-1的变化内容介绍-p-46\",\"42\":\"v-586d9498#预训练超参设置\",\"43\":\"v-586d9498#预训练tokenizer\",\"44\":\"v-586d9498#_2-3-评测\",\"45\":\"v-586d9498#_3-fine-tuning\",\"46\":\"v-586d9498#_3-1-supervised-fine-tuning-sft\",\"47\":\"v-586d9498#_3-2-rlhf\",\"48\":\"v-586d9498#_3-2-1-人类偏好数据收集\",\"49\":\"v-586d9498#_3-2-2-奖励建模\",\"50\":\"v-586d9498#_3-2-3-迭代微调\",\"51\":\"v-586d9498#_3-3-多轮一致性的指令\",\"52\":\"v-586d9498#_3-4-rlhf结果\",\"53\":\"v-586d9498#_3-4-1-基于模型的评估\",\"54\":\"v-586d9498#_3-4-2-人类评价\",\"55\":\"v-369323c1\",\"56\":\"v-369323c1#_2-1-tensorflow-extended\",\"57\":\"v-369323c1#_2-2-airflow\",\"58\":\"v-369323c1#_2-3-kubeflow-全面工具集、偏训练-⭐\",\"59\":\"v-369323c1#_2-4-mlflow-监控、模型再部署、性能监控-⭐\",\"60\":\"v-369323c1#_2-5-azure-devops-pipelines\",\"61\":\"v-369323c1#_2-6-azure-ml\",\"62\":\"v-369323c1#_3-1-comet-ml\",\"63\":\"v-369323c1#_3-2-weights-biases-wandb-监控-⭐\",\"64\":\"v-369323c1#_3-3-prefect-工作流管理-⭐\",\"65\":\"v-369323c1#_3-4-metaflow\",\"66\":\"v-369323c1#_3-5-pachyderm\",\"67\":\"v-369323c1#_3-6-data-version-control-dvc-版本控制-⭐\",\"68\":\"v-369323c1#_3-7-bentoml-模型部署与管理-⭐\",\"69\":\"v-369323c1#_3-8-cortex-模型部署与管理-⭐\",\"70\":\"v-369323c1#_3-9-evidently-监控-⭐\",\"71\":\"v-369323c1#_3-10-censius-ai\",\"72\":\"v-369323c1#_4-1-hydrosphere-性能监控\",\"73\":\"v-369323c1#_5-1-autokeras-自动化机器学习\",\"74\":\"v-29d238c2\",\"75\":\"v-29d238c2#_1-介绍\",\"76\":\"v-29d238c2#_2-retentive-networks\",\"77\":\"v-29d238c2#_2-1-retention\",\"78\":\"v-29d238c2#_2-2-gated-multi-scale-retention\",\"79\":\"v-29d238c2#_2-3-❇️retention网络的整体架构\",\"80\":\"v-29d238c2#_3-实验\",\"81\":\"v-16ee11dc\",\"82\":\"v-16ee11dc#应用层协议\",\"83\":\"v-16ee11dc#传输层协议\",\"84\":\"v-16ee11dc#网络层协议\",\"85\":\"v-5e716a47\",\"86\":\"v-5e716a47#_1-堆内存结构\",\"87\":\"v-5e716a47#_2-内存分配和回收原则\",\"88\":\"v-5e716a47#_2-1-分配原则\",\"89\":\"v-5e716a47#新的对象优先分配到新生代的伊甸园\",\"90\":\"v-5e716a47#大内存对象会被分配到老年代\",\"91\":\"v-5e716a47#长期存活的对象将会进入老年代\",\"92\":\"v-5e716a47#gc分类\",\"93\":\"v-5e716a47#空间分配担保\",\"94\":\"v-5e716a47#_2-死亡对象的判断方法\",\"95\":\"v-5e716a47#_2-1-引用计数器\",\"96\":\"v-5e716a47#_2-2-可达性分析\",\"97\":\"v-5e716a47#哪些对象可以作为gc-roots\",\"98\":\"v-5e716a47#不可达的对象一定会被垃圾回收吗\",\"99\":\"v-5e716a47#_2-3-如何判断一个常量可以被回收\",\"100\":\"v-5e716a47#_2-4-如何判断一个类可以进行回收\",\"101\":\"v-5e716a47#_3-引用类型总结\",\"102\":\"v-5e716a47#_3-1-强引用\",\"103\":\"v-5e716a47#_3-2-软引用\",\"104\":\"v-5e716a47#_3-3-弱引用\",\"105\":\"v-5e716a47#弱引用的应用\",\"106\":\"v-5e716a47#_3-4-虚引用\",\"107\":\"v-5e716a47#_4-垃圾收集算法\",\"108\":\"v-5e716a47#_4-1-标记-清除\",\"109\":\"v-5e716a47#_4-2-标记-复制\",\"110\":\"v-5e716a47#_4-3-标记-整理\",\"111\":\"v-5e716a47#_4-4-分代收集\",\"112\":\"v-5e716a47#_5-垃圾收集器\",\"113\":\"v-5e716a47#_5-1-serial-串行\",\"114\":\"v-5e716a47#_5-2-parnew\",\"115\":\"v-5e716a47#_5-3-parallel-scavenge\",\"116\":\"v-5e716a47#_5-4-cms-concurrent-mark-sweep\",\"117\":\"v-5e716a47#_5-5-g1\",\"118\":\"v-5e716a47#_5-6-zgc\",\"119\":\"v-68e2fece\",\"120\":\"v-68e2fece#_1-jvm内存结构-也叫jvm内存模型\",\"121\":\"v-68e2fece#想额外强调关于常量池的事情\",\"122\":\"v-68e2fece#_1-1-堆\",\"123\":\"v-68e2fece#_1-2-方法区\",\"124\":\"v-68e2fece#_1-3-虚拟机栈\",\"125\":\"v-68e2fece#_1-4-本地方法栈\",\"126\":\"v-68e2fece#_1-5-程序计数器\",\"127\":\"v-68e2fece#_2-java内存模型jmm\",\"128\":\"v-68e2fece#_2-1-可见性\",\"129\":\"v-68e2fece#_2-2-有序性\",\"130\":\"v-68e2fece#_2-3-原子性\",\"131\":\"v-68e2fece#_2-4-工作内存和主内存\",\"132\":\"v-61b9ecbc\",\"133\":\"v-61b9ecbc#_1-java类的加载流程\",\"134\":\"v-61b9ecbc#_1-1-加载-loading\",\"135\":\"v-61b9ecbc#_1-1-1-类模板和class对象的位置\",\"136\":\"v-61b9ecbc#_1-2-验证\",\"137\":\"v-61b9ecbc#_1-2-1-格式验证\",\"138\":\"v-61b9ecbc#_1-2-2-语义验证\",\"139\":\"v-61b9ecbc#_1-2-3-字节码验证\",\"140\":\"v-61b9ecbc#_1-2-4-引用验证-符号引用验证\",\"141\":\"v-61b9ecbc#_1-3-准备\",\"142\":\"v-61b9ecbc#_1-4-解析\",\"143\":\"v-61b9ecbc#_1-5-初始化\",\"144\":\"v-61b9ecbc#_1-6-使用\",\"145\":\"v-61b9ecbc#_1-7-卸载\",\"146\":\"v-3678b490\",\"147\":\"v-3678b490#_1-索引的介绍\",\"148\":\"v-3678b490#_2-索引的优缺点\",\"149\":\"v-3678b490#_3-索引的底层数据结构\",\"150\":\"v-3678b490#_3-1-hash\",\"151\":\"v-3678b490#_3-2-二叉查找树\",\"152\":\"v-3678b490#_3-3-平衡二叉树\",\"153\":\"v-3678b490#_3-4-红黑树\",\"154\":\"v-3678b490#_3-5-b树-b-树\",\"155\":\"v-3678b490#_4-主键索引和二级索引\",\"156\":\"v-3678b490#_5-聚簇索引和非聚簇索引\",\"157\":\"v-3678b490#_5-1-聚簇索引\",\"158\":\"v-3678b490#_5-2-非聚簇索引\",\"159\":\"v-3678b490#非聚簇索引一定会回表查询吗\",\"160\":\"v-3678b490#_6-覆盖索引和联合索引\",\"161\":\"v-3678b490#_6-1-最左前缀匹配原则\",\"162\":\"v-3678b490#_7-索引下推\",\"163\":\"v-3678b490#_8-正确使用索引的建议-感觉比较重要\",\"164\":\"v-3678b490#_8-1-选择合适的列作为索引\",\"165\":\"v-3678b490#_8-2-每张表上的索引不宜过多\",\"166\":\"v-3678b490#_8-3-删除不常用的索引\",\"167\":\"v-3678b490#_8-4-尽可能地使用联合索引\",\"168\":\"v-3678b490#_8-5-对于字符串索引-要使用前缀索引\",\"169\":\"v-3678b490#_8-6-要避免索引失效\",\"170\":\"v-3678b490#_8-7-要避免索引重复\",\"171\":\"v-7a89db98\",\"172\":\"v-7a89db98#_1-最左前缀匹配\",\"173\":\"v-7a89db98#_2-select\",\"174\":\"v-7a89db98#_3-函数\",\"175\":\"v-7a89db98#_4-计算操作\",\"176\":\"v-7a89db98#_5-like\",\"177\":\"v-7a89db98#_6-in和not-in\",\"178\":\"v-7a89db98#_7-order-by\",\"179\":\"v-7a89db98#_8-or\",\"180\":\"v-49153f24\",\"181\":\"v-49153f24#_0-tcp是哪一层的协议\",\"182\":\"v-49153f24#_1-三次握手\",\"183\":\"v-49153f24#_1-1-tcp的首部包中包括什么\",\"184\":\"v-49153f24#_1-2-tcp三次握手过程\",\"185\":\"v-49153f24#_1-3-为什么要进行三次握手-而不是两次或者四次\",\"186\":\"v-49153f24#_2-四次挥手\",\"187\":\"v-49153f24#_2-1-四次挥手过程\",\"188\":\"v-49153f24#_2-2-为什么要进行四次挥手-而不是三次挥手\",\"189\":\"v-42feb24b\",\"190\":\"v-42feb24b#什么是redis\",\"191\":\"v-42feb24b#为什么要用redis-redis的优势-redis的特点\",\"192\":\"v-42feb24b#缓存中间件有什么\",\"193\":\"v-42feb24b#redis的数据类型\",\"194\":\"v-42feb24b#redis是单进程还是多进程\",\"195\":\"v-42feb24b#redis中一个string类型的最大容量是多少\",\"196\":\"v-42feb24b#redis的持久化机制有哪些\",\"197\":\"v-42feb24b#如果有大量的key需要设置同一时间过期-需要注意什么\",\"198\":\"v-42feb24b#介绍一下如何使用redis实现一个分布式锁\",\"199\":\"v-7b7c1644\",\"200\":\"v-7b7c1644#五种基本类型\",\"201\":\"v-7b7c1644#四种常见类型\",\"202\":\"v-7b7c1644#_1-string\",\"203\":\"v-7b7c1644#_1-1-内部实现\",\"204\":\"v-7b7c1644#_1-1-1-数据结构\",\"205\":\"v-7b7c1644#_1-1-2-数据编码\",\"206\":\"v-7b7c1644#那么-embstr和raw有什么区别\",\"207\":\"v-7b7c1644#_1-2-应用场景\",\"208\":\"v-7b7c1644#_2-list\",\"209\":\"v-7b7c1644#_2-1-内部实现-数据结构\",\"210\":\"v-7b7c1644#_2-2-常用命令\",\"211\":\"v-7b7c1644#_2-3-应用场景\",\"212\":\"v-7b7c1644#_2-3-1-list如何保证消息的有序性\",\"213\":\"v-7b7c1644#_2-3-2-list如何处理重复消息\",\"214\":\"v-7b7c1644#_2-3-3-如何保证消息的可靠性\",\"215\":\"v-7b7c1644#_2-3-4-list实现消息队列有什么问题\",\"216\":\"v-7b7c1644#_3-hash\",\"217\":\"v-7b7c1644#_3-1-内部实现\",\"218\":\"v-7b7c1644#_3-2-常用命令\",\"219\":\"v-7b7c1644#_3-3-应用场景\",\"220\":\"v-7b7c1644#_4-set\",\"221\":\"v-7b7c1644#_4-1-内部实现\",\"222\":\"v-7b7c1644#_4-2-常用命令\",\"223\":\"v-7b7c1644#_4-3-应用场景\",\"224\":\"v-7b7c1644#_4-3-1-点赞\",\"225\":\"v-7b7c1644#_4-3-2-共同关注\",\"226\":\"v-7b7c1644#_4-3-3-抽奖\",\"227\":\"v-7b7c1644#_5-zset-sorted-set\",\"228\":\"v-7b7c1644#_5-1-内部实现\",\"229\":\"v-7b7c1644#_5-2-常用命令\",\"230\":\"v-7b7c1644#_5-3-应用场景\",\"231\":\"v-7b7c1644#_6-bitmap\",\"232\":\"v-7b7c1644#_6-1-底层实现\",\"233\":\"v-7b7c1644#_6-2-常用命令\",\"234\":\"v-7b7c1644#_6-2-应用场景\",\"235\":\"v-7b7c1644#_6-2-1-签到表\",\"236\":\"v-7b7c1644#_6-3-2-连续签到用户总数\",\"237\":\"v-7b7c1644#_7-hyperloglog\",\"238\":\"v-7b7c1644#_7-1-常见命令\",\"239\":\"v-7b7c1644#_7-2-应用场景\",\"240\":\"v-7b7c1644#_8-geo-地理位置\",\"241\":\"v-7b7c1644#_8-1-内部实现\",\"242\":\"v-7b7c1644#_8-2-常用命令\",\"243\":\"v-7b7c1644#_8-3-应用场景\",\"244\":\"v-7b7c1644#_8-3-1-滴滴叫车\",\"245\":\"v-19bcdf96\",\"246\":\"v-19bcdf96#解答\",\"247\":\"v-7f42d4f6\",\"248\":\"v-7bd923b8\",\"249\":\"v-18f2355e\",\"250\":\"v-012daa49\",\"251\":\"v-012daa49#题目描述\",\"252\":\"v-012daa49#解答\",\"253\":\"v-012daa49#关于system-arraycopy-的介绍\",\"254\":\"v-c1f0fcce\",\"255\":\"v-8daa1a0e\",\"256\":\"v-14c69af4\",\"257\":\"v-7449895b\",\"258\":\"v-24f2d5ba\",\"259\":\"v-e65774e8\",\"260\":\"v-dc384366\",\"261\":\"v-764ee4e0\",\"262\":\"v-31987621\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[3,10],\"1\":[1,42],\"2\":[1,4],\"3\":[5,275],\"4\":[3,1],\"5\":[2,11],\"6\":[1,5],\"7\":[2,40],\"8\":[1,7],\"9\":[2,35],\"10\":[1,10],\"11\":[1,8],\"12\":[1,48],\"13\":[3,33],\"14\":[2,85],\"15\":[2],\"16\":[3,38],\"17\":[2,41],\"18\":[3,169],\"19\":[2,39],\"20\":[3,54],\"21\":[4,27],\"22\":[2,76],\"23\":[2,5],\"24\":[3,35],\"25\":[3,54],\"26\":[2,43],\"27\":[3,104],\"28\":[3,30],\"29\":[3,28],\"30\":[4,49],\"31\":[3,27],\"32\":[5,90],\"33\":[1,303],\"34\":[1],\"35\":[1,13],\"36\":[3,5],\"37\":[2,35],\"38\":[2,25],\"39\":[3,5],\"40\":[2,18],\"41\":[7,46],\"42\":[1,15],\"43\":[1,13],\"44\":[3,2],\"45\":[3,11],\"46\":[7,43],\"47\":[3],\"48\":[4,20],\"49\":[3,145],\"50\":[3,167],\"51\":[2,99],\"52\":[3],\"53\":[4,28],\"54\":[4,44],\"55\":[1,20],\"56\":[4,29],\"57\":[2,14],\"58\":[6,47],\"59\":[7,35],\"60\":[5,19],\"61\":[4,16],\"62\":[4,16],\"63\":[7,21],\"64\":[4,28],\"65\":[3,9],\"66\":[3,34],\"67\":[8,28],\"68\":[5,28],\"69\":[5,24],\"70\":[5,30],\"71\":[4,18],\"72\":[5,55],\"73\":[5,14],\"74\":[2,31],\"75\":[2,68],\"76\":[3,45],\"77\":[3,135],\"78\":[5,60],\"79\":[3,47],\"80\":[2],\"81\":[1],\"82\":[1,22],\"83\":[1,8],\"84\":[1,2],\"85\":[2,2],\"86\":[2,8],\"87\":[2],\"88\":[3],\"89\":[1,10],\"90\":[1,2],\"91\":[1,11],\"92\":[1,11],\"93\":[1,9],\"94\":[2,2],\"95\":[3,13],\"96\":[2,10],\"97\":[2,5],\"98\":[1,17],\"99\":[3,2],\"100\":[3,15],\"101\":[2,6],\"102\":[3,5],\"103\":[3,39],\"104\":[2,8],\"105\":[1,51],\"106\":[3,8],\"107\":[2,6],\"108\":[4,8],\"109\":[4,12],\"110\":[4,9],\"111\":[2,13],\"112\":[2,8],\"113\":[4,10],\"114\":[3,6],\"115\":[4,8],\"116\":[6,25],\"117\":[2,20],\"118\":[3,4],\"119\":[2,1],\"120\":[4,6],\"121\":[2,13],\"122\":[2,4],\"123\":[3,4],\"124\":[3,2],\"125\":[3,3],\"126\":[3,2],\"127\":[2,6],\"128\":[3,5],\"129\":[2,10],\"130\":[3,4],\"131\":[3,12],\"132\":[2],\"133\":[2,25],\"134\":[4,19],\"135\":[2,5],\"136\":[3,6],\"137\":[3,8],\"138\":[3,3],\"139\":[4,2],\"140\":[5,1],\"141\":[3,9],\"142\":[3,1],\"143\":[3,1],\"144\":[3,1],\"145\":[3,4],\"146\":[2],\"147\":[2,9],\"148\":[2,12],\"149\":[2],\"150\":[3,19],\"151\":[3,14],\"152\":[2,13],\"153\":[3,18],\"154\":[4,26],\"155\":[2,22],\"156\":[2],\"157\":[3,26],\"158\":[3,10],\"159\":[2,9],\"160\":[2,6],\"161\":[3,34],\"162\":[2],\"163\":[3],\"164\":[3,8],\"165\":[3,6],\"166\":[3],\"167\":[3,4],\"168\":[4,2],\"169\":[3,2],\"170\":[3,3],\"171\":[2,5],\"172\":[2],\"173\":[3,7],\"174\":[2,10],\"175\":[2,9],\"176\":[3,1],\"177\":[3,4],\"178\":[3,4],\"179\":[2,1],\"180\":[2],\"181\":[3,1],\"182\":[2],\"183\":[3,12],\"184\":[3,21],\"185\":[4,19],\"186\":[2],\"187\":[3,19],\"188\":[3,16],\"189\":[2,3],\"190\":[1,8],\"191\":[3,19],\"192\":[1,6],\"193\":[1,13],\"194\":[1,3],\"195\":[1,1],\"196\":[1,42],\"197\":[2,5],\"198\":[1,15],\"199\":[2,5],\"200\":[2,5],\"201\":[2,12],\"202\":[2,7],\"203\":[2],\"204\":[2,34],\"205\":[3,20],\"206\":[3,18],\"207\":[3,24],\"208\":[2,6],\"209\":[4,15],\"210\":[2,18],\"211\":[3,7],\"212\":[4,11],\"213\":[3,44],\"214\":[3,8],\"215\":[4,6],\"216\":[2,9],\"217\":[3,11],\"218\":[3,21],\"219\":[2,5],\"220\":[2,9],\"221\":[3,6],\"222\":[3,34],\"223\":[3,31],\"224\":[4,1],\"225\":[4,1],\"226\":[3,30],\"227\":[5,6],\"228\":[3,22],\"229\":[3,52],\"230\":[3,1],\"231\":[2,10],\"232\":[3,7],\"233\":[3,58],\"234\":[3,8],\"235\":[4,66],\"236\":[4,98],\"237\":[2,39],\"238\":[3,15],\"239\":[3,1],\"240\":[3,23],\"241\":[3,31],\"242\":[3,35],\"243\":[3],\"244\":[4,46],\"245\":[2,34],\"246\":[1,14],\"247\":[2,72],\"248\":[2,70],\"249\":[3,65],\"250\":[6,2],\"251\":[1,65],\"252\":[1,16],\"253\":[3,79],\"254\":[1],\"255\":[1],\"256\":[1],\"257\":[1],\"258\":[1],\"259\":[1],\"260\":[1],\"261\":[3],\"262\":[1]},\"averageFieldLength\":[2.7072243346007614,25.74560955197559],\"storedFields\":{\"0\":{\"h\":\"Android Studio ADB异常重启问题解决\",\"t\":[\"在新设备上安装了Android Studio，不过其自带的adb一直处于无法使用的状态，因为它处于无限重启状态，每隔几秒钟就会重启一次，导致无法持续连接Android设备。这段时间一直采用在Android Studio启动之前手动启动一个adb程序来临时解决，不过这种办法很不方便。\"]},\"1\":{\"h\":\"解决过程\",\"t\":[\"首先通过Android Studio的菜单->Help->Show Log in Explorer选项打开Android Studio的日志文件夹，找到idea.log文件并打开，发现日志中有关adb的日志如下：\",\"2023-10-10 14:27:43,703 [ 57399] WARN - #com.android.ddmlib - Cannot reach ADB server, attempting to reconnect. \",\"这个问题可以通过Android Studio的菜单->File->Settings，找到Build, Execution, Deployment选项中的Debugger，然后取消勾选Enable adb mDNS for wireless debugging来解决。\"]},\"2\":{\"h\":\"米窗所用技术备忘\",\"t\":[\"androidx.compose.foundation 看一下\"]},\"3\":{\"h\":\"Mi-Freeform 3.0 技术相关\",\"t\":[\"米窗之前的版本一直不够稳定，且使用的VirtualDisplay会有一些问题，比如会时常跳出小窗，且部分应用无法在小窗中启动。\",\"AOSP和部分国产ROM（如MIUI）采用的小窗方式是使用DecorCaptionView，将应用程序DecorView移至DecorCaptionView，从而实现小窗功能。该方式实现最佳，不过需要修改较多的源码，并且经过证明，修改后仍然有较多问题。\",\"米窗3采用了与VirtualDisplayAdapter/OverlayDisplayAdapter平行的自定义适配器：MiFreeformDisplayAdapter，将该类在Android启动时注入framework，为此，开发者做了以下努力：\",\"如何与该自定义适配器进行通信？\",\"在Android中，用户态程序获取系统服务的方式通常是走Binder，米窗3亦是如此。米窗3定义了一个专用系统服务MiFreeformService，该类继承自IMiFreeformService.Stub，该服务是一个Binder，我们可以通过ServiceManager#addService()方法，将其添加到系统服务列表。在此我们会发现，米窗3并不能顺利添加自定义系统服务。\",\"为什么不能顺利添加自定义系统服务？\",\"SELinux限制。在Linux/Android下，除了基础的权限限制，系统还引入了SELinux，SELinux对每个角色可以执行什么操作进行了严格限制，为此，我们需要给米窗3所需要执行的内容编写SELinux规则。Magisk可以在sepolicy.rule中编写自定义的SELinux规则，米窗3所需要的规则如下：\",\"allow untrusted_app default_android_service service_manager find allow system_server default_android_service service_manager add \",\"在添加完自定义服务后，为什么用户程序仍然无法发现？\",\"获取系统服务最终需要调用ServiceManager#getService()方法：\",\"public static IBinder getService(String name) { try { IBinder service = sCache.get(name); if (service != null) { return service; } else { return Binder.allowBlocking(rawGetService(name)); } } catch (RemoteException e) { Log.e(TAG, \\\"error in getService\\\", e); } return null; } \",\"因为我们的自定义系统服务不在sCache中，所以需要走else，不过因为未知原因，这里无法通过else获取到米窗3自定义的系统服务。所以米窗3采用了将自定义系统服务添加到sCache中的做法：\",\"ServiceManager.addService(\\\"mi_freeform\\\", this); Map<String, IBinder> cache = new ArrayMap<>(); cache.put(\\\"mi_freeform\\\", this); ServiceManager.initServiceCache(cache); \",\"如何获取到DisplayManagerService？\",\"实例化MiFreeformDisplayAdapter需要DisplayManagerService的一些字段，为此米窗3需要设法获取到DMS实例。获取DMS实例常用方式是通过Xposed hook，不过这会增加用户成本，为此，米窗3选择使用Riru/Zygisk方式进行获取。因为获取DMS后执行的操作均在Java层完成，所以米窗3选择了ZygoteLoader。该库额外提供了代理SystemService的功能，米窗3利用该功能，监听display系统服务的添加。 不过，display系统服务是一个Binder，并不是DMS，米窗3如何通过display系统服务获取到DMS呢？这里注意到，display系统服务是下面类的实例：\",\"class DisplayManagerService extends SystemService { final class BinderService extends IDisplayManager.Stub { ... } } \",\"我们可以直接通过ServiceManager获取到BinderService，不过注意到，该类是DisplayManagerService的内部类，而在日常使用时，内部类是可以直接访问外部类的变量的，这是因为内部类持有外部类的实例。我们可以通过反射的方式获取到外部类的实例(this$0表示顶层外部类引用)：\",\"// get out class field Field field = service.getClass().getDeclaredField(\\\"this$0\\\"); \",\"为什么通过反射获取DisplayManagerService时会抛出NoClassDefFoundError异常？\",\"当米窗3想要实例化上述字段来获取DMS实例时，发现会抛出NoClassDefFoundError异常。这是因为当前的ClassLoader中不包括/system/framework/services.jar路径，而DMS类在该jar包中。为此，我们需要使用BinderService的实例的ClassLoader来加载DMS类：\",\"// get out class field Field field = service.getClass().getDeclaredField(\\\"this$0\\\"); ClassLoader classLoader = service.getClass().getClassLoader(); assert classLoader != null; //for find dms, we need service`s classloader Class<?> dmsClass = classLoader.loadClass(\\\"com.android.server.display.DisplayManagerService\\\"); // get DisplayManagerService Object displayManagerService = field.get(service); \",\"此时，米窗3实例化了DMS，下面需要加载MiFreeformDisplayAdapter，为了让MDA与DMS可通过同一ClassLoader加载（否则仍然会抛出NoClassDefFoundError），米窗3做了以下操作：\",\"//add MiFreeformServer dex to path classLoader.getClass().getMethod(\\\"addDexPath\\\", String.class).invoke(classLoader, \\\"/system/framework/freeform.dex\\\"); Class<?> mfClass = classLoader.loadClass(\\\"com.android.server.display.MiFreeformDisplayAdapter\\\"); Object mf = mfClass.getConstructors()[0].newInstance(mSyncRoot, mContext, mHandler, mDisplayDeviceRepo, mUiHandler); \",\"用户程序可以通过以下方式获取到mi_freeform服务：\",\"val serviceManager = Class.forName(\\\"android.os.ServiceManager\\\") val binder = HiddenApiBypass.invoke(serviceManager, null, \\\"getService\\\", \\\"mi_freeform\\\") as IBinder Log.e(TAG, \\\"mf binder $binder\\\") val mfs = IMiFreeformService.Stub.asInterface(binder) \",\"此处使用了AndroidHiddenApiBypass，传统的getSystemService(\\\"mi_freeform\\\")无法获取，因为没有注册服务：\",\"//frameworks/base/core/java/android/app/SystemServiceRegistry.java // add for infrare scan registerService(Context.INFRARE_SCAN_SERVICE, InfrareScanManager.class, new CachedServiceFetcher<InfrareScanManager>() { @Override public InfrareScanManager createService(ContextImpl ctx) throws ServiceNotFoundException { IBinder b = ServiceManager.getService(Context.INFRARE_SCAN_SERVICE); IInfrareScanManager service = IInfrareScanManager.Stub.asInterface(b); Log.d(\\\"InfrareScanManager\\\",\\\" \\\"+b+\\\" \\\"+service); return new InfrareScanManager(ctx.getOuterContext(), service); }}); // add end \",\"HiddenApi冲突？\",\"在调用android.jar中提供，但是部分内容被隐藏的类时(如SurfaceControl)，我们很难处理，这里米窗3使用了HiddenApiRefinePlugin来处理，将系统类起一个别名。\",\"在创建完DisplayDevice后，无法立即获得LogicalDisplay从而拿到displayId？\",\"调用DisplayDeviceRepository#addListener()添加监听，在添加成功后给Binder回调即可。\",\"给远程服务设置Surface不生效？\",\"编写AIDL文件时，需要给Surface设置为in，而非inout。如：\",\"void createFreeform(String name, IMiFreeformDisplayCallback callback, int width, int height, int densityDpi, boolean secure, boolean ownContentOnly, boolean shouldShowSystemDecorations, in Surface surface, float refreshRate, long presentationDeadlineNanos) = 1; void resizeFreeform(IBinder appToken, int width, int height, int densityDpi) = 2; \"]},\"4\":{\"h\":\"封装、继承、多态\",\"t\":[\"参考\"]},\"5\":{\"h\":\"一、封装\",\"t\":[\"封装从字面上来理解就是包装的意思，专业点就是信息隐藏，是指利用抽象数据类型将数据和基于数据的操作封装在一起，使其构成一个不可分割的独立实体，数据被保护在抽象数据类型的内部，尽可能地隐藏内部的细节，只保留一些对外接口使之与外部发生联系。系统的其他对象只能通过包裹在数据外面的已经授权的操作来与这个封装的对象进行交流和交互。也就是说用户是无需知道对象内部的细节，但可以通过该对象对外的提供的接口来访问该对象。\"]},\"6\":{\"h\":\"使用封装的优点\",\"t\":[\"良好的封装能够减少耦合\",\"类内部的结构可以自由修改\",\"可以对成员进行更精准的控制\",\"隐藏信息，实现细节\"]},\"7\":{\"h\":\"二、继承\",\"t\":[\"继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承我们能够非常方便地复用以前的代码，能够大大的提高开发的效率。\",\"继承所描述的是“is-a”的关系，如果有两个对象A和B，若可以描述为“A是B”，则可以表示A继承B，其中B是被继承者称之为父类或者超类，A是继承者称之为子类或者派生类。\",\"实际上继承者是被继承者的特殊化，它除了拥有被继承者的特性外，还拥有自己独有得特性。例如猫有抓老鼠、爬树等其他动物没有的特性。同时在继承关系中，继承者完全可以替换被继承者，反之则不可以，例如我们可以说猫是动物，但不能说动物是猫就是这个道理，其实对于这个我们将其称之为“向上转型”。\",\"诚然，继承定义了类如何相互关联，共享特性。对于若干个相同或者相识的类，我们可以抽象出他们共有的行为或者属相并将其定义成一个父类或者超类，然后用这些类继承该父类，他们不仅可以拥有父类的属性、方法还可以定义自己独有的属性或者方法。\",\"子类拥有父类非private的属性和方法\",\"子类可以拥有自己的属性和方法，即子类可以对父类进行扩展\",\"子类可以用自己的方式实现父类的方法\"]},\"8\":{\"h\":\"谨慎继承\",\"t\":[\"继承有以下缺陷\",\"父类变，子类就必须变\",\"继承破坏了封装，对父类而言，它的实现对子类是完全透明的\",\"继承是一种强耦合的关系\"]},\"9\":{\"h\":\"三、多态\",\"t\":[\"所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，即一个引用变量倒底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。因为在程序运行时才确定具体的类，这样，不用修改源程序代码，就可以让引用变量绑定到各种不同的类实现上，从而导致该引用调用的具体方法随之改变，即不修改程序代码就可以改变程序运行时所绑定的具体代码，让程序可以选择多个运行状态，这就是多态性。\",\"所以对于多态我们可以总结如下：\",\"指向子类的父类引用由于向上转型了，它只能访问父类中拥有的方法和属性，而对于子类中存在而父类中不存在的方法，该引用是不能使用的，尽管是重载该方法。若子类重写了父类中的某些方法，在调用该些方法的时候，必定是使用子类中定义的这些方法（动态连接、动态调用）。\",\"对于面向对象而言，多态分为编译时多态和运行时多态。其中编辑时多态是静态的，主要是指方法的重载，它是根据参数列表的不同来区分不同的函数，通过编辑之后会变成两个不同的函数，在运行时谈不上多态。而运行时多态是动态的，它是通过动态绑定来实现的，也就是我们所说的多态性。\"]},\"10\":{\"h\":\"多态的实现条件\",\"t\":[\"Java实现多态有三个必要条件：继承、重写、向上转型。\",\"继承：在多态中必须存在有继承关系的子类和父类。\",\"重写：子类对父类中某些方法进行重新定义，在调用这些方法时就会调用子类的方法。\",\"向上转型：在多态中需要将子类的引用赋给父类对象，只有这样该引用才能够具备技能调用父类的方法和子类的方法。\"]},\"11\":{\"h\":\"实现多态的方法\",\"t\":[\"在Java中，有两种方法可以实现多态：继承和接口\",\"在接口的多态中，指向接口的引用必须是指定这实现了该接口的一个类的实例程序，在运行时，根据对象引用的实际类型来执行对应的方法。\"]},\"12\":{\"h\":\"实例分析\",\"t\":[\"public class A { public String show(D obj) { return (\\\"A and D\\\"); } public String show(A obj) { return (\\\"A and A\\\"); } } public class B extends A{ public String show(B obj){ return (\\\"B and B\\\"); } public String show(A obj){ return (\\\"B and A\\\"); } } public class C extends B{ } public class D extends B{ } public class Test { public static void main(String[] args) { A a1 = new A(); A a2 = new B(); B b = new B(); C c = new C(); D d = new D(); System.out.println(\\\"1--\\\" + a1.show(b)); System.out.println(\\\"2--\\\" + a1.show(c)); System.out.println(\\\"3--\\\" + a1.show(d)); System.out.println(\\\"4--\\\" + a2.show(b)); System.out.println(\\\"5--\\\" + a2.show(c)); System.out.println(\\\"6--\\\" + a2.show(d)); System.out.println(\\\"7--\\\" + b.show(b)); System.out.println(\\\"8--\\\" + b.show(c)); System.out.println(\\\"9--\\\" + b.show(d)); } } \",\"当超类对象引用变量引用子类对象时，被引用对象的类型而不是引用变量的类型决定了调用谁的成员方法，但是这个被调用的方法必须是在超类中定义过的，也就是说被子类覆盖的方法。（但是如果强制把超类转换成子类的话，就可以调用子类中新添加而超类没有的方法了。）\",\"在继承链中对象方法的调用存在一个优先级：this.show(O)、super.show(O)、this.show((super)O)、super.show((super)O)。\"]},\"13\":{\"h\":\"从BERT到GLM，NLP经历了什么？\",\"t\":[\"2022年底，ChatGPT悄然进入大众的视线，受到了业内人士和广大群众的注意。ChatGPT的成功不是偶然的，是其总结前人经验、反复打磨多年才得以形成的。本篇文章将顺着现代自然语言处理方法和模型的脉络，即Transformer[1]、BERT[2]、T5[3]、GPT[4]、GLM[5]和P-Tuning v2[6]几个方面来介绍。其中Transformer是一种全新的序列转换模型，BERT、T5、GPT和GLM均为预训练语言模型，P-Tuning v2是一种对预训练语言模型进行高效微调的方法。通过以上几个部分，本篇文章对现代语言模型的学习的全过程：即结构、训练和微调均进行了介绍。其中因为内容相对重复，本篇文章对T5和GPT进行简要介绍。\"]},\"14\":{\"h\":\"一、主要研究点\",\"t\":[\"在ChatGPT大火之后，各种语言模型层出不穷，比如百度的文心、讯飞的星火等。但是语言模型的研究并不是最近才兴起的。早在2017年，Google Brain就发布了一个全新的序列转换模型—Transformer，后续的语言模型，基本上都与Transformer有着千丝万缕的联系。2018年，Google公开了以Transformer作为基础的语言模型BERT，轰动一时。BERT的基础模型有110M参数，在当年属于标准大小，但是其自然语言理解能力非常强。2019年，Google公开了T5模型，该模型号称是“全能模型”，即所有的自然语言理解任务都可划分为“文本到文本”的任务，T5基础模型参数量为220M，但是最大的T5模型达到了11B，是BERT_base的100倍大小。这绝对可以称之为“大模型”了。\",\"BERT和T5都是Google的精彩操作，而另一边的OpenAI也不甘落后，2018年， GPT-1公开，其参数量有117M；2019年，GPT-2公开，其参数量有1.5B；而2020年，GPT-3公布，其参数量已经达到了175B。\",\"放眼国内，清华大学在语言模型上研究较早。2021年，清华大学语言模型GLM发布，意为通用语言模型(General Language Model)。目前，最大的GLM参数量已经达到了130B。\",\"基本已经可以确定的是，小模型（低于10B参数）的能力不是很强，所以语言模型方向模型参数规模越来越大，百亿、千亿、万亿模型都不足为奇。然而，抛开预训练不谈，在如此大的规模下，大部分个人和团队都已经没有能力去做模型的全参数微调了。因此，国内外研究者先后提出了P-Tuning[7]、Prefix-Tuning[8]、P-Tuning v2[6]、Prompt Tuning[9]和LoRA[10]等部分参数微调方法。本篇文章选择了清华大学的P-Tuning v2，该方法微调效果可以与全参数微调媲美，但是微调参数量仅为全参数微调的3%左右。\",\"本篇文章会对上述模型和技术进行简要介绍，该脉络基本涉及了现代语言模型学习的全过程。\"]},\"15\":{\"h\":\"二、Transformer\"},\"16\":{\"h\":\"2.1 序列转换模型\",\"t\":[\"一个序列转换(Sequence-to-Sequence, Seq2Seq)模型一般包括一个编码器(Encoder)和一个解码器(Decoder)，如图2-1。序列转换模型并不是Transformer首次提出的。在Transformer之前，序列转换模型一般由循环神经网络(RNN)或卷积神经网络(CNN)作为编码器和解码器。然而无论是RNN也好，CNN也罢，它们都足够复杂，导致序列转换模型的效率不高。后来，有的学者尝试将注意力机制(Attention Mechanism)引入序列转换模型[11]，让序列转换模型的效率得以一定的提升。不过它们仍然没有脱离RNN或者CNN。\",\"图2-1 序列转换模型\",\"而Transformer，彻底抛弃了复杂的RNN和CNN，只依赖于注意力机制，这也是Transformer成功的关键。在介绍Transformer之前，我们首先讨论一下注意力机制。\"]},\"17\":{\"h\":\"2.2 注意力机制\",\"t\":[\"注意力机制(Attention Mechanism)是人们在机器学习模型中嵌入的一种特殊结构，用来自动学习和计算输入数据对输出数据的贡献大小。在注意力机制中，我们往往会讨论Q、K、V，它们分别代表Query、Key、Value。注意力机制就是给定一个Query，经过一系列的Key来获取Value，从而得到Attention Score。如图2-2。\",\"图2-2 注意力机制\",\"实际上，计算Attention Score的过程如下：首先，由Query和Key做向量比对，得到Query和Key的相似度，然后归一化相似度，并用相似度与Key所对应的Value做矩阵运算并求和，得到Attention Score。注意力机制公式如下：\",\"下面我们来介绍自注意力机制。自注意力机制是注意力机制的一种。在自注意力机制中，注意力集中在上述公式中Source的内部元素，如图2-3。而在计算方式上，与传统注意力机制完全相同。在Transformer中，自注意力机制得到了应用，因为Transformer需要判断序列中词与词之间的关系强度，自注意力机制正符合这一点。\",\"图2-3 自注意力机制示例\"]},\"18\":{\"h\":\"2.3 Transformer模型\",\"t\":[\"Transformer是Seq2Seq的全新尝试，其抛弃了RNN和CNN作为Encoder和Decoder，采用了注意力机制，提高了Seq2Seq的效率。\",\"图2‑4 Transformer模型\",\"Transformer模型整体结构如图2-4。其结构可以分为输入输出嵌入向量、位置编码、Encoder模块、Decoder模块等。\",\"对输入、输出进行向量化(Embedding)已经是广为应用的做法，其比独热编码(One-Hot)拥有更加优秀的能力，这里不再赘述。下面我们讨论位置编码。\",\"位置编码 。因为Transformer抛弃了RNN和CNN，这样，如果不经过特殊处理，Transformer没有办法表示序列的顺序。但是，序列的顺序中往往蕴含着一些重要信息，比如：\",\"I do not like the story of the movie, but I do like the cast.\",\"I do like the story of the movie, but I do not like the cast.\",\"上述两个句子序列的词完全相同，只不过是某些词的顺序不同。如果不考虑词在序列中的位置，那么Encoder会认为这两个序列完全相同。因此，选择一种合适的方式表示词在序列中的顺序非常重要。\",\"一个好的位置编码方案需要满足以下几个条件：1.它能为每个时间步输出一个独一无二的编码；2.不同长度的句子之间，任何两个时间步之间的距离应该保持一致；3.模型应该能毫不费力地泛化更长的句子，它的值应该是有界的；4.它必须是确定性的。\",\"位置编码可以通过训练得到，也可以通过公式计算得到。Transformer中的位置编码采用公式计算得到，公式如下：\",\"Transformer的位置编码简单但是有创新性。该编码不是一个单一的数值，而是包含句子中特定位置信息的d维向量（d_model即隐层维数）。此外，该编码没有整合进模型，而是用这个向量让每个词具有它在句子序列中的位置信息，即通过注入词的顺序信息来增强模型的输入。最后，采用三角函数来作为位置编码公式，对于相对位置的计算更加方便，因为三角函数具有周期性。\",\"Encoder 。Transformer的编码器结构如图2-5。可以看到，Encoder部分由N个Encoder单元构成。在Transformer中，N=6。一个Encoder单元，由一个多头注意力机制(Multi-Head Attention)和一个前馈网络(Feed Forward)构成。在多头注意力机制和前馈网络完成后，会计算残差和(Add)并正规化(Norm)。首先，我们来讨论多头注意力机制。\",\"图2-5 Encoder结构图\",\"在2.2节中，我们对注意力机制有了初步了解。在这里，我们进一步讨论Transformer中应用的注意力机制。Transformer中对注意力机制的体现在多头注意力机制。而多头注意力机制是由缩放点积注意力机制(Scaled Dot-Product Attention)构成，它是注意力机制的一种，其计算过程与注意力机制一致，其计算公式如下：\",\"缩放点积注意力机制在做完Query和Key的点积之后，会进行一个缩放，即除以d的开方。之所以要缩放，是因为对于输入的d大值，会导致Query和Key的点积非常大，这样会导致SoftMax产生非常小的值，为了抵消这个效果，缩放点击注意力机制会进行一个缩放。\",\"多头注意力机制如图2-6。Transformer认为，将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息。多头注意力机制就是将缩放点积注意力机制的过程做h次，再把输出合并起来。多头注意力机制的公式如下：\",\"图2-6 多头注意力机制\",\"Encoder单元中的另一个部分是一个前馈网络。Transformer在这里设计的前馈网络比较简单，为一个两层的多层感知机，第一层有一个ReLU激活函数，第二层为一个线性变换。公式如下：\",\"Decoder 。Transformer的Decoder和Encoder十分相似。Decoder中Decoder单元数N=6，与Encoder一致。Decoder和Encoder最大的区别，是Decoder单元中多了一层Masked Multi-Head Attention。\",\"什么是Masked Multi-Head Attention？其实Multi-Head Attention和上述的一致。但是单纯的Multi-Head Attention是双向的，也就是某个词既可以看到它之后的词，也可以看到它之前的词。\",\"但是在解码阶段，模型需要做的是通过已经有的信息来预测下一个位置会出现什么，如果此时模型知道了某个词之后的信息，模型就失去了“预测”，相当于看到了未来的信息。这是我们不希望发生的。所以，在解码阶段，我们希望自注意力机制是“单向”的，所以这里就用了Masked自注意力机制，组织模型看到将要预测的信息。\",\"Decoder中的Multi-Head Attention的K V是Encoder的输出计算的。\",\"至此，Transformer的模型结构已经介绍完成。Transformer对后来语言模型的影响十分深远，后续的语言模型，基本上都采用了Transformer或者基于Transformer修改的模型。\"]},\"19\":{\"h\":\"三、BERT\",\"t\":[\"Transformer是一个Seq2Seq模型，但并不是一个实际的语言模型。而首个将Transformer应用到实际语言模型中的，正是BERT。BERT全称Bidirectional Encoder Representations from Transformers。从全称中可以看出，BERT是一个双向编码模型，并且和Transformer有关。下面我们来介绍一下BERT。\",\"我们知道，GPT是一个自回归(Autoregression)语言模型，即，当前的Token只能看到它和它之前的Token，而不能看到它之后的Token。BERT在当时的条件下认为，这限制了预训练，特别是微调的能力。模型应该是双向的才好。因此，BERT诞生了，一个双向编码语言模型。\",\"当然，在现在看来，我们无法评价自回归(Autoregression)模型和自编码(Autoencoder)模型，或者说单向模型和双向模型谁好谁坏，它们各有优缺点。比如，自回归语言模型更加适合自然语言生成任务，而自编码模型更加适合自然语言理解任务。\"]},\"20\":{\"h\":\"3.1 BERT与Transformer\",\"t\":[\"BERT并没有把Transformer拿来直接用，而是只用到了Encoder部分。如图3-1，为BERT模型的结构图，其中蓝色阴影部分，就是Transformer的Encoder部分。可以看出，Transformer的Encoder模块，也是BERT的核心。\",\"图3-1 BERT模型结构图\",\"BERT只用了Transformer的Encoder部分，并且相对Transformer来说，BERT对Encoder进行了一些修改。其主要的修改如下：1.在Transformer的介绍中我们提到过，Transformer的Encoder层是由N=6的单元构成的。在BERT中，BERT_base N = 12 BERT_large N = 24。2.在Transformer中，注意力机制体现在模型中是多头注意力机制，Transformer中多头注意力机制是由h = 8，即8个缩放点积注意力叠加而成。在BERT中，BERT_base h = 12，BERT_large h = 16。3.在Transformer中，d = 512，在BERT中，BERT_base d = 768，BERT_large d = 1024。这个d其实就是模型最大能接受的Token长度。4.Embedding部分有了一些调整，多了一个Segment Embedding，Positional Embedding也有调整。我们将在下面介绍这一部分。\",\"这样看来，BERT_base的参数规模是110M，BERT_large的参数规模是340M。虽然把这个模型放到现在看起来规模不大，但是在当时，大家的参数量还没有那么夸张。\"]},\"21\":{\"h\":\"3.2 Input Embedding\",\"t\":[\"BERT相对Transformer来说，对Input Embedding部分做了一些修改，增加了一个Segment Embedding。这个是什么呢？由于BERT的主要目的是构建一个通用的预训练模型，因此难免需要兼顾到各种NLP任务场景下的输入。因此Segment Embedding的作用便是用来区分输入序列中的不同序列，其本质就是通过一个普通的词嵌入来区分每一个序列所处的位置。例如在NSP任务中，那么对于任意一个序列的每一位置都将用同一个向量来进行表示，即此时Segment词表的长度为2。\",\"此外，BERT对Positional Embedding也有调整。Transformer中的位置编码，是由三角函数计算出来的，而BERT的位置编码，是训练出来的。\",\"最终的Input Embedding，是这三个嵌入式张量的和：\",\"图3-2 BERT Input Embedding\"]},\"22\":{\"h\":\"3.3 预训练BERT\",\"t\":[\"BERT的预训练没有采用传统的自左向右或者自右向左语言模型来训练BERT，而是采用的MLM，即Masked Language Model。那么具体是怎么做的呢？\",\"MLM会随机地遮住输入的某些Token，比如：\",\"Input: 今天天气真好呀！\",\"MLM: 今天天气真[MASK]呀！\",\"遮住之后，MLM的要做的事就是根据上下文来预测被遮住的Token应该是什么，是“好”，还是“坏”？这些都是根据上下文，来计算概率的。也就是在这里，可以体现出BERT是双向模型，因为这里的上下文，既包括Token左边的，也包括Token右边的。MLM根据整个句子信息来推断被遮住的Token。\",\"根据经验，一般会MASK掉句子的15%的Token来进行训练，效果比较好（BERT的论文中没有提为什么是15%，T5论文中有对比实验，证明了15%是效果最好的）。不过，如果这15%全部把输入的Token替换成[MASK]，可能会有一个问题：这会造成预训练和微调之间产生一个不匹配的情况，因为在微调的过程中，[MASK]并不会出现，这样预测的概率可能不准确。为了解决这个问题，BERT把这15%中的80%用[MASK]替换，10%不变，10%随机替换为其他Token。\",\"接下来我们来介绍BERT中另外一个部分，Next Sentence Prediction。很多下游任务，比如问题回答、自然语言推理等，都要基于多个句子之间的关系，这个关系是没有办法被语言模型直接捕获到的。为了解决这个问题，BERT在预训练中加入了NSP。NSP是一个二分类下句预测任务。具体地，对于每个样本来说都是由A和B两句话构成，其中的情况B确实为A的下一句话（标签为IsNext），另外的的情况是B为语料中其它的随机句子（标签为NotNext），然后模型来预测B是否为A的下一句话。NSP的位置在BERT模型图中有所体现。\",\"在实验中，BERT的效果遥遥领先于同期其他语言模型，取得了喜人的成绩。这里我们不做过多介绍。\"]},\"23\":{\"h\":\"四、T5和GPT\",\"t\":[\"T5，特别是GPT，其能力大家有目共睹。本篇文章中将简单对其进行介绍。\"]},\"24\":{\"h\":\"4.1 T5模型\",\"t\":[\"T5，是Transfer Text-to-Text Transformer的简写。Transfer来自Transfer Learning，预训练模型基本上属于这个范畴，Transformer即我们在第二节中提到的，那么什么是Text-to-Text？它是T5提出的一个统一框架，用于将所有的自然语言处理(NLP)任务都转化为文本到文本(Text-to-Text)任务。\",\"图4‑1 T5模型\",\"比如，自然语言处理中常见的翻译任务，在T5模型中，只需要在给模型输入的部分加上前缀：“给我从英语翻译成汉语”，然后再加上要翻译的内容即可。通过这样的方式，就可以将NLP任务都转成Text-to-Text的形式，这样，就可以用同样的模型、同样的损失函数、同样的训练过程、同样的解码过程来完成所有的NLP任务。本文对T5模型的介绍就到这里，更多内容可以阅读原论文。\"]},\"25\":{\"h\":\"4.2 GPT模型\",\"t\":[\"说到GPT(Generative Pre-Training)语言模型，大家首先想起的一定是ChatGPT。ChatGPT是基于GPT-3.5的对话聊天机器人，其能力大家有目共睹。其成功的关键在于超大的参数规模(1750亿)和超多的预训练语料，这是普通公司和个人难以承受的。本篇文章将回到最初的GPT，来讨论GPT的基本结构。\",\"GPT模型与BERT模型不同。BERT模型是自编码模型，而GPT模型是自回归模型。自回归模型对自然语言生成有着天然的优势。不过与BERT、T5模型相同，GPT同样也抛弃了传统的RNN和CNN，转而采用Transformer结构。与BERT不同的是，GPT采用的不是Transformer的Encoder部分，而是其Decoder部分。\",\"GPT的核心部分是N=12的Transformer Decoder结构，如图4-2。\",\"图4-2 GPT模型结构\",\"同样，GPT也并没有将Transformer的Decoder拿过来直接用，而是做了一些修改。Transformer的Decoder结构中，包含了Masked Multi Self Attention和Multi Self Attention，在GPT中，只保留了Masked Multi Self Attention。其余基本没有变化。\",\"如今，GPT已经迎来了第四个大版本，GPT-4。其参数规模进一步增大，能力进一步增强，更是拥有了理解图像的能力。AI的能力在逐步增强。\"]},\"26\":{\"h\":\"五、GLM\",\"t\":[\"在国内，清华大学发布的GLM(General Language Model)应该是效果比较不错的一个模型。当然，其优良的效果源于其创新的思想和持续的研究。\",\"GLM意为通用语言模型，其通用性体现在哪里？我们知道，预训练语言模型可以分为三种：自回归模型(e.g. GPT[4])、自编码模型(e.g. BERT[2])、编码-解码模型(e.g. RoRERTa[12])。它们各有各的擅长之处。比如自回归模型擅长自然语言生成任务，而自编码模型擅长自然语言理解任务。在GLM之前，也有研究人员尝试将上述三种模型结合[13]，以胜任多种自然语言处理任务，不过因为自回归和自编码在模型结构上相差太多，所以效果不是很好（在现在看，GPT似乎有能力处理自然语言理解和自然语言生成任务，不过在当时的条件下并没有很出色的能力）。\",\"同样，GLM也尝试能够同时处理自然语言理解和自然语言生成等多种NLP任务。不同于之前研究的简单结合，GLM创新地应用了自回归填空思想。下面，我们来介绍GLM中的自回归填空思想。\"]},\"27\":{\"h\":\"5.1 自回归填空\",\"t\":[\"GLM模型应用了名为自回归填空(Autoregressive Blank Infilling)的思想。我们给定一个文本序列[x~1~, …, x ~n~ ]，在其中采用多个文本域{s ~1~ , …, s ~m~ }，其中每个文本域s~i ~都对应x中的连续Token [s ~i,1~ ,…,s ~i,li~ ]，而每一个文本域都会被一个单独的Token [MASK]所替代。也许通过文字描述比较难以理解。我们可以看图5-1：\",\"图5-1 GLM自回归填空示意图之一\",\"在图5-1(a)中，可以看到我们给定的文本序列为[x~1~, …, x~6~]，在文本序列中采样的文本域为{s ~1~ , s ~2~ }，其中s~1~对应着[x ~3~ ]，s~2~对应着[x ~5~ , x ~6~ ]（图中含有色块部分）。在图5-1(b)中可以看到，GLM把s~1~和s~2~对应的x部分替换为一个[MASK] Token，而与BERT等Masked不同，GLM没有选择直接丢失这些x，而是将其放到了Part B部分。\",\"GLM随机Masked掉一些文本，其实在BERT中也是这样做的，我们称之为Masked Language Model(MLM)。只不过，在BERT中，一般Masked掉的是一个词，而GLM中可能会Masked掉连续的多个字。个人猜测，可能是因为在中文中，一个字可能意义不如多个字组成的意义大（如“玩”和“玩笑”可能差别很大），所以Masked掉多个字，可能效果会更好一些。GLM随机Masked掉的比例为15%，沿用了BERT和T5的Masked的比例，这个比例在T5模型的论文中证明，为效果最好的。\",\"对于Masked掉的词，GLM采用自回归的方式尝试还原它们，即“自回归填空”，公式如下：\",\"其中x~corrupt~就是partA，即带MASK部分的句子，s~z<i~指的是partB的部分，不过看到它只用了z<i的部分，也就是单向的，即自回归的。不过考虑到span之间可能也有关系，所以s~z~的顺序是随机打乱的。如图5-2。\",\"在这里也可以看出，GLM预测的条件比BERT多了一个partB。在BERT中，MASK掉的15%的输入，其中只有80%被[MASK]替代，而另外10%不变，10%随机变为其他Token，BERT通过这种方式来保留一些被Masked的原始信息。但是GLM没有这样做，GLM把Masked掉的信息全部保留在了partB，这样可以进一步提高预测能力。不过partA是看不到partB的，partB可以自回归地看到已经走过的partB和全部的partA。\",\"图5-2 GLM自回归填空示意图二\"]},\"28\":{\"h\":\"5.2 二维位置编码\",\"t\":[\"在Transformer中，位置编码采用了三角函数计算的方式得到；在BERT中，位置编码采用了预训练的方式训练而得。GLM仍然是以Transformer为基础的结构，自然也没有原生的表示位置信息的能力，所以，也只能够通过位置编码的形式来获取位置信息。\",\"与Transformer和BERT的一维位置编码不同，GLM采用了二维位置编码。\",\"通过上一节的介绍我们知道，GLM把输入的文本分为两个部分：partA和partB。所谓二维编码，即对partA部分和partB部分都进行编码。如图5-3。\",\"图5-3 GLM二维编码\",\"对于Position 1，表示词在partA中的位置，Position 2表示被Masked的词在partB中的位置，如果Position 2 = 0，表示非Masked的词。\"]},\"29\":{\"h\":\"5.3 GLM与Transformer\",\"t\":[\"上述我们也提到，GLM同样是基于Transformer的结构，不过与BERT、T5一样，GLM同样对Transformer的结构进行了修改。GLM同样只使用了Transformer的Encoder部分，并且做了以下修改：1.重新调整了LN和残差连接的顺序。2.对于Token的预测输出用的是单个的线形层。3.将激活函数由ReLU调整为GeLUs，因为GeLUs效果更好。\",\"至此，GLM的基本结构已经介绍完毕。四个基于Transformer的预训练语言模型也已经介绍完毕。它们整体相似，但是都有自己的创新点。相信在未来，会有更多更好的语言模型诞生。但是，为了适应下游任务，对于预训练好的语言模型，往往需要经过下游数据进行微调后才可更好的发挥它的能力。下面一节，我们介绍微调相关技术。\"]},\"30\":{\"h\":\"六、P-Tuning v2\",\"t\":[\"训练语言模型的成本是巨大的，往往小的企业或者个人应用的，都是在一个良好的预训练语言模型上进行微调。从前，预训练语言模型的方式只有全参数微调(Fine-Tuning)，全参数微调效果相对较好，可以让微调后的预训练模型在处理下游任务时得到良好的效果。但是全参数微调的设备需求仍然很大，比如，对于GLM-130B进行全参数微调，需要10台DGX A100服务器，设备就需要千万级别。这对很多企业和个人仍然是不能接受的。\",\"为了减少微调的设备等资源的消耗，研究者着手设计部分参数微调的方法，包括P-Tuning[7]、Prefix-Tuning[8]、Prompt-Tuning[9]、LoRA[10]和P-Tuning v2[6]。但是，P-Tuning、Prefix-Tuning虽然实现了部分参数调优，让微调的资源消耗降下来了，但是其性能仍然不如全参数微调。LoRA同样也是一种部分参数微调的方法，其在挖掘语言模型的潜在能力上有着不错的成绩，并且其资源消耗极低，受到了大家的关注。LoRA在文生图领域应用广泛。\",\"Prompt-Tuning和P-Tuning v2基本上是同一时期发布的，它们均基于Prefix-Tuning进行了修改和优化，结构基本一致。这里，我们选择P-Tuning v2进行介绍。\"]},\"31\":{\"h\":\"6.1 提示微调\",\"t\":[\"上述我们介绍P-Tuning v2属于部分参数微调，更准确地说，P-Tuning v2应该属于提示微调(Prompt Tuning)。提示微调只用一个冻结的语言模型来微调连续的提示，大大减少了训练时的存储和内存使用。\",\"提示微调冻结了预训练模型的所有参数，并使用自然语言提示来查询语言模型。比如，对于情感分析问题，我们可以将样本与提示“这部电影是[MASK]”串联起来，要求预训练语言模型预测被Masked的标注。然后，我们可以使用“好”与“坏”是被Masked标注的预测概率来预测样本的标签。提示微调完全不需要训练，只需要存储一份模型参数。\"]},\"32\":{\"h\":\"6.2 P-Tuning v2\",\"t\":[\"P-Tuning v2并不是一个全新的方法，其事实上是将文本生成的Prefix-Tuning技术适配到自然语言理解任务中，其主要结果如下：1.仅精调0.1%参数量（固定语言模型(LM)参数），在330M到10B参数规模的语言模型上，均取得和Fine-Tuning相似的性能。2.将Prompt-Tuning技术首次拓展到序列标注等复杂自然语言理解(NLU)任务上。\",\"P-Tuning v2的关键所在就是引入了Prefix-Tuning。Prefix-Tuning最开始应用在自然语言生成(NLG)，由[Prefix, x, y]三部分构成，如图6-1。Prefix为前缀，\",\"图6-1 Prefix-Tuning示意图\",\"x为输入，y为输出。Prefix-Tuning将预训练LM参数固定，Prefix参数进行微调，它不仅只在Embedding层进行微调，而是在每一层都进行微调。\",\"P-Tuning v2实际上就是Prefix-Tuning，如图6-2(b)。在Prefix部分，每一层Transformer的Embedding输入都需要被微调，这一点是不同于P-Tuning的，在P-Tuning中，只有第一层Embedding才需要被微调。这样看来，P-Tuning v2可以微调的参数变多了，假设Prefix部分由50个Token组成，那么P-Tuning v2共有50*12=600个参数需要微调。可微调的参数多了，效果自然也会好一些。\",\"图6-2 P-Tuning与P-Tuning v2\",\"此外，P-Tuning v2还包括以下改进：1.移除了Reparamerization加速训练方式；2.采用了多任务学习优化：基于多任务数据集的Prompt进行预训练，然后再适配下游任务；3.舍弃了词汇Mapping的Verbalizer的使用，重新利用[CLS]和字符标签，跟全参数微调一样利用CLS或者Token的输出做NLU，以增强通用性，可以适配到序列标注任务。\",\"与GLM一样，P-Tuning v2也是清华大学发布的，那么自然GLM原生地支持P-Tuning v2，并且更推荐使用P-Tuning v2对GLM进行微调。上述我们介绍，对GLM-130B进行全参数微调，需要10台DGX A100，而如果改为使用P-Tuning v2进行微调，近似性能下可以将设备减少为1台DGX A100。而对GLM进行微调同样还可以使用LoRA，虽然所需设备与P-Tuning v2几乎一致，但是其性能并没有P-Tuning v2好。\"]},\"33\":{\"h\":\"参考文献\",\"t\":[\"[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000–6010.\",\"[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL 2019, pages 4171–4186.\",\"[3] Raffel Colin, Shazeer Noam, Roberts Adam, Lee Katherine, Narang Sharan, Matena Michael, Zhou Yanqi, Li Wei, and Liu Peter J.. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 140 (2020), 1–67. http://jmlr.org/papers/v21/20-074.html.\",\"[4] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018a. Improving Language Understanding by Generative Pre-Training.\",\"[5] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2021. All nlp tasks are generation tasks: A general pretraining framework. arXiv preprint arXiv:2103.10360.\",\"[6] Liu, X. et al. P-tuning: prompt tuning can be comparable to fine-tuning universally across scales and tasks. In Proc. the 60th Annual Meeting of the Association for Computational Linguistics. 2, 61–68 (2022).\",\"[7] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. arXiv:2103.10385.\",\"[8] Xiang Lisa Li and Percy Liang. 2021. Prefixtuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190.\",\"[9] Lester Brian, Al-Rfou Rami, and Constant Noah. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’21), Moens Marie-Francine, Huang Xuanjing, Specia Lucia, and Yih Scott Wen-tau (Eds.). Association for Computational Linguistics, 3045–3059\",\"[10] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)\",\"[11] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\",\"[12] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv e-prints.\",\"[13] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang,\\nXiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, and Hsiao-Wuen\\nHon. 2020. Unilmv2: Pseudo-masked language models for unified language model\\npre-training. In ICML 2020,volume 119, pages 642–652.\"]},\"34\":{\"h\":\"在Apple芯片上微调GLM模型\"},\"35\":{\"h\":\"参考资料\",\"t\":[\"https://github.com/THUDM/ChatGLM-6B/issues/977\\n该issue讨论了GLM在Apple芯片上微调的内容，但是最终似乎并没有成功。用CPU可以，但是用MPS会失败。\"]},\"36\":{\"h\":\"LLaMA 2 论文笔记\",\"t\":[\"该论文篇幅巨大，非常详细地介绍了LLaMA 2的预训练和微调过程，本篇笔记对其内容进行简要记录。论文链接\"]},\"37\":{\"h\":\"1 介绍\",\"t\":[\"LLaMA 2共包括两大版本：预训练模型LLaMA 2和基于它的微调模型LLaMA 2-Chat。共有7B 13B 34B 70B四种参数规模。\",\"LLaMA 2，是LLaMA 1的升级版本，在多个公开可获得数据上进行训练。相比LLaMA 1，LLaMA 2预训练的语料库增加了40%，上下文长度（即可接受的输入长度）增加一倍（最多支持4K token)，并且采用了分组查询注意力（grouped-query，后续有介绍）机制。\",\"LLaMA 2-Chat是基于LLaMA 2预训练模型微调得来的，后续有详细介绍。\",\"在第2节，本文介绍LLaMA 2的预训练方法；在第3节，本文介绍LLaMA 2-Chat的微调方法；\"]},\"38\":{\"h\":\"2 预训练方法\",\"t\":[\"LLaMA 2的预训练特性如下：\",\"LLaMA 2基本还是采用LLaMA 1的训练方法：LLaMA 1论文链接\",\"优化的自回归Transformer（归一化、激活函数等有变化，下面有介绍）\",\"具体而言，LLaMA 2执行了更稳健的数据清理，更新了数据混合，对总token进行了40%以上的训练，将上下文长度增加了一倍，并使用分组查询注意力（GQA）来提高LLaMA 2的推理可扩展性\",\"（2T = 2 trillion = 2万亿）\"]},\"39\":{\"h\":\"2.1 训练数据\",\"t\":[\"LLaMA 2的训练数据来自混合后的公开数据，共在2T token上进行训练。\"]},\"40\":{\"h\":\"2.2 训练详情\",\"t\":[\"预训练的设置和模型架构和LLaMA 1基本一致：\",\"Transformer架构\",\"归一化 RMSNorm\",\"激活函数 SwiGLU\",\"旋转位置Embedding RoPE rotary positional embeddings，这个现在都在用，包括GLM\",\"与LLaMA 1差异：增加了上下文长度、增加了GQA\"]},\"41\":{\"h\":\"A.2.1 额外的预训练信息之与LLaMA 1的变化内容介绍 p.46\",\"t\":[\"上下文长度 更长的上下文长度可以让模型处理更多信息，这可以让模型支持记住更多对话历史信息、更多的总结任务、理解更长的文本\",\"Grouped-Query Attention 自回归解码的标准做法是缓存序列中先前token的key（K）和value（V）对，从而加快注意力计算。然而，随着context window或batch size的增加，与多头注意力（MHA）模型中的KV缓存大小相关的内存成本显著增长。对于KV缓存大小成为瓶颈的大型模型，可以在多个头之间共享key和value预测，而不会导致性能大幅下降。可以使用具有单个KV投影的原始多查询格式(MQA)或具有8KV投影的分组查询注意力(GQA)变体。基于消融结果和易于缩放推断，对于34B和70B Llama 2模型，LLaMA 2选择使用GQA而不是MQA。\",\"额外发现：一个多卡并行训练的论文：Training multi-billion parameter language models using model parallelism\"]},\"42\":{\"h\":\"预训练超参设置\",\"t\":[\"AdamW优化器 β1=0.9，β2=0.95，eps=10e-5\",\"余弦学习率，warmup 2000 steps\",\"将最终学习率降低到峰值学习率的10%\",\"使用0.1的权重衰减和1.0的梯度剪裁\"]},\"43\":{\"h\":\"预训练Tokenizer\",\"t\":[\"使用与LLaMA 1相同的标记器；它采用了字节对编码（BPE）算法，使用了来自SentencePiece的实现。与LLaMA 1一样，将所有数字拆分为单个数字，并使用字节分解未知的UTF-8字符。总词汇大小为32k个标记。\"]},\"44\":{\"h\":\"2.3 评测\",\"t\":[\"评测结果如下：\"]},\"45\":{\"h\":\"3. Fine-tuning\",\"t\":[\"LLaMA 2-Chat基于LLaMA 2微调而来，包括指令微调和RLHF\",\"本节报告了使用监督微调以及初始和迭代奖励建模和RLHF进行的实验和发现。\",\"提出一种新技术，Ghost Attention (GAtt)，用于帮助控制多轮对话流\"]},\"46\":{\"h\":\"3.1 Supervised Fine-Tuning (SFT)\",\"t\":[\"开始：用公开可获得的指令微调数据，和LLaMA 1一致，使用了 Scaling Instruction-Finetuned Language Models\",\"公开的指令微调数据质量参差不齐，首先就是要收集大量的高质量SFT数据，如下图：\",\"高质量指令微调数据，即使是少量的，也可以让结果很好。万级别的好的数据足够了，Meta总共收集了27540个好数据。\",\"对于监督微调，使用了余弦学习率，LR=2e-5，权重衰减=0.1，batch size = 64，sequence lenght = 4096\",\"对于微调过程，每个样本都包含一个提示和一个答案。为了确保模型序列长度正确填充，作者将训练集中的所有提示和答案连接起来。使用一个特殊的令牌来分隔提示段和应答段。作者使用自回归目标，并从用户提示中消除令牌的损失，因此，作者只对回答令牌进行反向传播。最后，作者对模型进行了2个epochs的微调。\"]},\"47\":{\"h\":\"3.2 RLHF\"},\"48\":{\"h\":\"3.2.1 人类偏好数据收集\",\"t\":[\"与其他方案相比，作者选择了二进制比较协议，主要是因为它使作者能够最大限度地提高收集到的提示的多样性。作者的注释过程如下。作者要求注释器首先编写一个提示，然后根据提供的标准在两个采样的模型响应之间进行选择。为了最大限度地提高多样性，从两个不同的模型变量中对给定提示的两个响应进行采样，并改变温度超参数。除了给参与者一个被迫的选择之外，作者还要求注释者标注他们更喜欢自己选择的回答而不是选择的程度：要么他们的选择明显更好，要么更好，要么稍微好一点，要么好到可以忽略不计/不确定。\",\"（就是给个输入，然后有两种输出，看哪个更符合标准）\",\"用到的一些人类偏好开源数据集\"]},\"49\":{\"h\":\"3.2.2 奖励建模\",\"t\":[\"奖励建模就是拿一个模型的结果和它相关的Prompt作为输入，然后输出一个分数来表明这个结果的质量（有用性、安全性等），用这个分数，就可以在RLHF中优化模型了\",\"为了训练奖励模型，作者将收集的成对人类偏好数据转换为二元排名标签格式（即选择和拒绝），并强制选择的响应比对应的响应具有更高的分数。作者使用了二元排名损失：\",\"where rθ(x, y) is the scalar score output for prompt x and completion y with model weights θ. yc is the preferred response that annotators choose and yr is the rejected counterpart.\",\"在这种二元排名损失的基础上，作者进一步修改它，如第3.2.1节所示，利用这些信息来明确教导奖励模型为具有更多差异的世代分配更多不一致的分数可能是有用的。为此，作者在损失中进一步添加了一个margin成分：\",\"where the margin m(r) is a discrete function of the preference rating. Naturally, we use a large margin for pairs with distinct responses, and a smaller one for those with similar responses (shown in Table 27). We found this margin component can improve Helpfulness reward model accuracy especially on samples where two responses are more separable. More detailed ablation and analysis can be found in Table 28 in Appendix A.3.3.\",\"奖励建模的训练详情。LLaMA 2-Chat在训练数据上训练一个epoch。在早期的实验中，发现训练时间过长会导致过度拟合。LLaMA 2-Chat使用与基本模型相同的优化器参数。70B参数LLaMA 2-Chat的最大学习率为5×10−6，其余参数为1×10−5。学习率按余弦学习率计划降低，降至最大学习率的10%。 LLaMA 2-chat使用占总步数3%的warm-up，最少5 steps。有效batch大小固定为512对，即每batch 1024行。\",\"奖励建模的结果。在每一batch用于奖励建模的人类偏好注释上，都拿出1000个例子作为测试集来评估模型(这个都是可以学习的地方，按他的来)。作者将相应测试集的所有提示的并集分别称为“Meta Helpfulness”和“Meta Safety”。总体而言，这个奖励模型优于所有base-line，包括GPT-4。\",\"当作者在表8中按偏好评级对分数进行分组时，我们可以看到“明显更好”的测试集，并随着比较对变得更加相似而逐渐退化（例如，“稍微好一点”）。当在两个相似的模型反应之间做出决定时，由于注释者的主观性和他们对可能区分反应的细微细节的依赖，学习对人类偏好进行建模将变得具有挑战性。作者强调，对于提高LLaMA 2-Chat的性能，更明显的响应的准确性最为重要。与相似对相比，在更明显的反应上，人类偏好注释一致率也更高。\"]},\"50\":{\"h\":\"3.2.3 迭代微调\",\"t\":[\"作者通过两个主要的算法来探索RLHF微调：\",\"近端策略优化(Proximal Policy Optimization, PPO)，论文链接，该算法是RLHF文献的标准。\",\"拒绝采样微调(Rejection Sampling fine-tuning)，作者对模型中的K个输出进行采样，并用我们的奖励选择最佳候选者，这与Constitutional AI: Harmlessness from AI Feedback保持一致。Residual Energy-Based Models for Text Generation也提出了同样的LLM重新排序策略，其中奖励被视为能量函数。在这里，作者更进一步，使用选定的输出进行梯度更新。对于每个Prompt，获得最高奖励分数的样本被视为新的金标准。与Discriminative Adversarial Search for Abstractive Summarization类似，然后作者在新的一组排序样本上微调模型，以增强奖励。\",\"这两个强化学习(RL)算法的主要不同：\",\"广度。在拒绝采样中，该模型为给定提示探索K个样本，而对PPO只进行一次生成。\",\"深度。在PPO中，在步骤t的训练期间，样本是前一步骤的梯度更新后从t−1更新的模型策略的函数。在拒绝采样微调中，在应用类似于SFT的微调之前，作者在给定模型的初始策略的情况下对所有输出进行采样，以收集新的数据集。然而，由于作者应用了迭代模型更新，两种RL算法之间的基本差异就不那么明显了。\",\"拒绝采样的介绍。作者只对最大的70B LLaMA 2-Chat进行拒绝采样。所有较小的模型都根据较大模型的拒绝采样数据进行微调，从而将较大模型的能力提取到较小的模型中。在每个迭代过程，作者从最新的模型中对每个Prompt采样K个答案，作者为每个样本打分，给出实验时可访问的最佳奖励模型，然后为给定提示选择最佳答案。\",\"作者在图7中说明了拒绝采样的好处。最大曲线和中值曲线之间的增量可以解释为对最佳输出进行微调的潜在增益。正如预期的那样，这个增量随着样本的增加而增加，因为最大值增加（即，更多的样本，产生良好轨迹的机会更多），而中值保持不变。探索和作者能在样本中获得的最大回报之间有着直接的联系。温度参数对勘探也起着重要作用，因为更高的温度使作者能够对更多样的输出进行采样。\",\"在图8中，作者报告了LLaMA 2-Chat-SFT（左）和LLaMA 2-Chat-RLHF（右），N个样本（其中N∈[1，…，100]）在不同温度下的最大回报曲线。可以观察到，在迭代模型更新过程中，最佳温度不是恒定的：RLHF对重新缩放温度有直接影响。对于LLaMA 2-Chat-RLHF，当在10到100个输出之间采样时，最佳温度为T∈[1.2，1.3]。因此，在有限的计算预算下，有必要逐步重新调整温度。请注意，对于每个模型，这种温度重新缩放都会发生恒定数量的步骤，并且总是从每个新RLHF版本的基本模型开始。\",\"PPO介绍。对于所有模型，作者使用AdamW优化器，其中β1=0.9，β2=0.95，eps=10−5。使用0.1的权重衰减(weight decay)、1.0的梯度剪裁(gradient clipping)和10e−6的恒定学习率(lr) 对于每个PPO迭代，我们使用512的批量大小(batch size)、0.2的PPO剪辑阈值、64的小批量大小(mini-batch size)，并且每个小批量采取一个梯度步骤(step)。对于7B和13B模型，我们设置β=0.01（KL惩罚），对于34B和70B模型，设置β=0.005。\",\"作者为所有模型进行了200到400次迭代的训练，并对延迟的提示进行了评估，以提前停止。70B模型上的PPO每次迭代平均耗时≈330秒。为了快速进行大批量训练，作者使用PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel。这在使用O（1）正向或反向传播时是有效的，但在生成过程中会导致很大的减慢（≈20×），即使在使用大批量和KV缓存时也是如此。作者能够通过在生成之前将模型权重合并到每个节点一次，然后在生成之后释放内存，恢复训练循环的其余部分来缓解这种情况。\"]},\"51\":{\"h\":\"3.3 多轮一致性的指令\",\"t\":[\"在对话设置中，一些指示应适用于所有的对话轮数中，例如，简洁地回应，或“扮演”某个公众人物。然而，在初始版本的RLHF模型中，LLaMA 2-Chat会忘记指示，如下图左侧（右侧是使用GAtt优化后的结果）：\",\"为了解决这些限制，作者提出了Ghost Attention（GAtt），这是一种受上下文蒸馏，Constitutional AI: Harmlessness from AI Feedback启发的非常简单的方法，它可以破解微调数据，以帮助在多阶段过程中集中注意力。GAtt允许对多轮进行对话控制，如图9（右）所示。\",\"Gatt方法。假设可以访问两个人（例如，用户和助手）之间的多回合对话数据集，该数据集具有消息列表[u1，a1，…，un，an]，其中un和an分别对应于回合n的用户和助手消息。然后定义了一个指令，inst，应该贯穿在整个对话中。例如，inst可以是“扮演...”。然后可以将此指令综合连接到会话的所有用户消息。\",\"接下来可以使用最新的RLHF模型对这些合成数据进行采样。现在有了一个上下文对话和样本，可以在类似于拒绝采样的过程中对模型进行微调。可以在除第一个回合外的所有回合中放弃它，而不是用指令来增加所有上下文对话回合，但这会导致系统消息（即最后一个回合之前的所有中间辅助消息）与样本在训练时间不匹配。为了解决这个可能影响训练的问题，只需将前几轮中的所有token（包括助手的消息）的loss设置为0。\",\"对于训练指令，作者创建了一些综合约束条件：爱好（“你喜欢例如网球”）、语言（“用例如法语说话”）或公众人物（“扮演例如拿破仑”）。为了获得兴趣爱好和公众人物的列表，作者要求LLaMA 2-Chat生成它，以避免教学和模型知识之间的不匹配（例如，要求模型扮演训练中没有遇到的人）。为了使指令更加复杂和多样化，作者通过随机组合上述约束来构建最终指令。在为训练数据构建最终系统消息时，作者也会在一半的时间内修改原始指令，使其不那么冗长，例如，“从现在起始终充当拿破仑”-> “人物：拿破仑。”这些步骤生成了一个SFT数据集，可以在该数据集上微调LLaMA 2-Chat。\",\"为了说明GAtt如何在微调过程中帮助重塑注意力，作者在下图中显示了模型的最大注意力激活。每个图的左侧对应系统信息（“Act as Oscar Wilde，扮演奥斯卡·王尔德”）。我们可以看到，与没有GAtt的模型（左）相比，配备GAtt的型号（右）在对话的大部分时间里保持了对系统消息的大量注意力激活。（？差别很大吗）\"]},\"52\":{\"h\":\"3.4 RLHF结果\"},\"53\":{\"h\":\"3.4.1 基于模型的评估\",\"t\":[\"模型的进展。图11报告了作者针对安全和帮助轴的不同SFT和RLHF版本的进展，通过Meta内部的安全和帮助奖励模型进行测量。在这组评估中，RLHF-V3之后的两个轴上都优于ChatGPT（无害和有用>50%）。尽管前面提到了使用Meta的奖励作为逐点衡量标准的相关性，但可以说，它可能偏向于LLaMA 2-Chat。因此，为了进行公平的比较，作者使用GPT-4额外计算最终结果，以评估哪一代是优选的。ChatGPT和LLaMA 2-Chat输出在GPT-4提示中出现的顺序是随机交换的，以避免任何偏差。正如预期的那样，支持LLaMA 2-Chat的胜率不那么明显，尽管我们最新的LLaMA 2-Chat获得了超过60%的胜率。\"]},\"54\":{\"h\":\"3.4.2 人类评价\",\"t\":[\"人类评价通常被认为是评判自然语言生成模型（包括对话模型）的黄金标准。为了评估主要模型版本的质量，作者要求人类评估人员对其有用性和安全性进行评分。作者将LLaMA 2-Chat模型与开源模型（Falcon，MPT ，Vicuna）以及4000多个单回合和多回合提示的闭源模型（ChatGPT和PaLM）进行了比较。对于ChatGPT，作者使用gpt-3.5-turbo-0301型号。对于PaLM，作者使用chat-bison-001模型。\",\"结果如图12所示，LLaMA 2-Chat模型在单回合和多回合提示上都显著优于开源模型。特别是，LLaMA 2-Chat 7B模型在60%的提示上优于MPT-7B-Chat。LLaMA 2-Chat 34B与同等尺寸的Vicuna-33B和Falcon 40B型号相比，总体胜率超过75%。 最大的LLaMA 2-Chat模型与ChatGPT具有竞争力。LLaMA 2-Chat 70B模型相对于ChatGPT的胜率为36%，平局率为31.5%。在我们的提示集上，LLaMA 2-Chat 70B模型在很大程度上优于PaLM bison聊天模型。\"]},\"55\":{\"h\":\"MLOps\",\"t\":[\"MLOps是一门工程学科，旨在统一 ML 系统开发（dev）和 ML 系统部署（ops），以标准化过程生产高性能模型的持续交付。知乎\",\"Machine Learning Operations (MLOps) overview definition and architecture这篇论文中介绍了11个MLOps相关工具，下面挑选开源的和Azure商用的进行总结：\"]},\"56\":{\"h\":\"2.1 TensorFlow Extended\",\"t\":[\"类别：开源\",\"TFX 是一种基于 TensorFlow 的 Google 生产级机器学习 (ML) 平台。该平台提供了一个配置框架和众多共享库，用来集成定义、启动和监控机器学习系统所需的常见组件。TFX 是一个端到端平台，用于部署生产环境机器学习流水线。TensorFlow Extended（TFX）是一个配置框架，为端到端ML管道的每个任务提供库。例如，有数据验证、数据分布检查、模型训练和模型服务。\",\"相关链接：https://tensorflow.google.cn/tfx?hl=zh-cn\"]},\"57\":{\"h\":\"2.2 Airflow\",\"t\":[\"类别：开源\",\"Airflow是一种任务和工作流程编排工具，也可以用于ML工作流程编排。它还可用于编排数据工程作业。任务可以根据有向无环图（DAGs）来执行。\",\"相关链接：https://airflow.apache.org/\"]},\"58\":{\"h\":\"2.3 Kubeflow（全面工具集、偏训练） ⭐\",\"t\":[\"用途：全面、偏训练\",\"类别：开源\",\"Kubeflow是一个基于kubernetes的机器学习工具集，本身没有任何功能，靠的是其丰富的工具。 Kubeflow主要是为了简化在kubernetes上面运行机器学习任务的流程， 最终希望能够实现一套完整可用的流水线，来实现机器学习从数据到模型的一整套端到端的过程。 Kubeflow是一个基于kubernetes的端到端ML平台。每个Kubeflow组件都被包装到一个容器中，并由Kubernetes协调。此外，ML工作流管道中的每个任务都用一个容器来处理。 Kubeflow项目致力于在Kubernetes上部署机器学习（ML）工作流，简单，可移植和可扩展。我们的目标不是重新创建其他服务，而是提供一种直接的方法，将ML的最佳开源系统部署到不同的基础设施。无论你在哪里运行Kubernetes，你都应该能够运行Kubeflow。 Kubeflow 主要关注于训练的过程， 对于其他方面，则应该找寻更为合适的框架或者平台。\",\"相关链接：https://www.kubeflow.org/\",\"相关链接：https://www.bilibili.com/video/BV1G14y1972C\",\"相关链接：https://developer.aliyun.com/article/784044\",\"相关链接：https://zhuanlan.zhihu.com/p/55840115\"]},\"59\":{\"h\":\"2.4 MLflow（监控、模型再部署、性能监控） ⭐\",\"t\":[\"类别：开源\",\"类型：监控、模型再部署、性能监控\",\"MLflow是一个ML平台，它允许端到端管理ML生命周期。它提供了一个高级的实验跟踪功能、一个模型注册表和模型服务组件。MLflow是一个开源工具，可帮助您管理机器学习生命周期的核心部分。它通常用于实验跟踪，但您也可以将其用于再现性，部署和模型注册。您可以使用CLI、Python、R、Java和REST API管理机器学习实验和模型元数据。 MLflow是机器学习工程师通过实验、部署和测试来管理机器学习生命周期的开源平台。当您想要跟踪机器学习模型的性能时，MLflow就派上用场了。它就像一个仪表盘，在这里你可以：\",\"相关链接：https://mlflow.org/\",\"相关链接：https://zhuanlan.zhihu.com/p/161641400\"]},\"60\":{\"h\":\"2.5 Azure DevOps Pipelines\",\"t\":[\"类别：商用\",\"Azure DevOps管道是一个CI/CD自动化工具，以促进构建、测试和交付步骤。它还允许人们调度和管理一个ML管道的不同阶段。\",\"相关链接：https://learn.microsoft.com/zh-cn/azure/devops/pipelines/?view=azure-devops\"]},\"61\":{\"h\":\"2.6 Azure ML\",\"t\":[\"类别：商用\",\"微软的Azure结合了Azure DevOps管道和Azure ML，提供了一个端到端ML平台。\",\"在https://www.datacamp.com/blog/top-mlops-tools中提到了一些其他的MLOps的工具，下面挑选介绍：\"]},\"62\":{\"h\":\"3.1 Comet ML\",\"t\":[\"Comet ML是一个跟踪、比较、解释和优化机器学习模型和实验的平台。您可以将其与任何机器学习库一起使用，例如Scikit-learn，Pytorch，TensorFlow和HuggingFace。\",\"相关链接：https://www.comet.com/site/\"]},\"63\":{\"h\":\"3.2 Weights & Biases(wandb) （监控）⭐\",\"t\":[\"用途：监控\",\"Weights & Biases是一个用于实验跟踪、数据和模型版本控制、超参数优化和模型管理的机器学习平台。此外，您可以使用它来记录工件（数据集、模型、依赖关系、管道和结果）并可视化数据集（音频、视觉、文本和表格）。\",\"相关链接：https://wandb.ai/site\"]},\"64\":{\"h\":\"3.3 Prefect（工作流管理） ⭐\",\"t\":[\"用途：工作流管理\",\"Prefect是一个现代化的数据堆栈，用于监控、协调和编排应用程序之间的工作流。它是一个开源的轻量级工具，专为端到端机器学习pipeline而构建。您可以使用Prefect Orion UI或Prefect Cloud作为数据库。Prefect Orion UI是一个开源、本地托管的编排引擎和API服务器。它为您提供了对本地Prefect Orion实例和工作流的深入了解。Prefect Cloud是一个托管服务，可让您可视化流程、流程运行和部署。此外，您还可以管理帐户、工作区和团队协作。\",\"相关链接：https://www.prefect.io/\"]},\"65\":{\"h\":\"3.4 Metaflow\",\"t\":[\"用途：工作流管理\",\"Metaflow是一个强大的，久经考验的工作流管理工具，用于数据科学和机器学习项目。它是为数据科学家构建的，因此他们可以专注于构建模型，而不是担心MLOps工程。\"]},\"66\":{\"h\":\"3.5 Pachyderm\",\"t\":[\"用途：版本控制（没有看出具体用途）\",\"Pachyderm通过Kubernetes上的数据版本化、沿袭和端到端管道自动化数据转换。您可以与任何数据（图像，日志，视频，CSV），任何语言（Python，R，SQL，C/C++）以及任何规模（PB数据，数千个作业）集成。就像Git一样，你可以使用类似的语法来版本化你的数据。在Pachyderm中，对象的最高级别是Repository，您可以使用Commit、Branches、File、History和Provenance来跟踪和版本化数据集。\",\"相关链接：https://www.pachyderm.com/\"]},\"67\":{\"h\":\"3.6 Data Version Control (DVC) （版本控制）⭐\",\"t\":[\"用途：版本控制，可对数据、模型等进行版本控制\",\"Data Version Control是一个开源的、流行的机器学习项目工具。它与Git无缝协作，为您提供代码、数据、模型、元数据和管道版本控制。DVC不仅仅是一个数据跟踪和版本控制工具。您可以使用它：实验跟踪（模型指标、参数、版本控制）。创建、可视化和运行机器学习管道。部署和协作的工作流。数据和模型注册表。使用CML持续集成和部署机器学习。\",\"相关链接：https://dvc.org/\"]},\"68\":{\"h\":\"3.7 BentoML（模型部署与管理） ⭐\",\"t\":[\"用途：模型部署与管理\",\"BentoML使机器学习应用程序的发布变得更简单、更快。它是一个Python优先的工具，用于在生产中部署和维护API。它通过运行并行推理和自适应批处理来扩展强大的优化功能，并提供硬件加速。BentoML的交互式集中式仪表板可以在部署机器学习模型时轻松组织和监控。最好的部分是它可以与各种机器学习框架一起使用，例如Keras，ONNX，LightGBM，Pytorch和Scikit-learn。简而言之，BentoML为模型部署、服务和监控提供了完整的解决方案。\",\"相关链接：https://www.bentoml.com/\",\"相关链接：https://github.com/bentoml/bentoml\",\"相关链接：https://zhuanlan.zhihu.com/p/495814838\"]},\"69\":{\"h\":\"3.8 Cortex（模型部署与管理） ⭐\",\"t\":[\"用途：模型部署与管理\",\"Cortex允许您在生产环境中部署、管理和扩展机器学习模型。它是一个开源、灵活、多框架的模型服务和监控工具。Cortex扩展到Docker，Kubernetes，TensorFlow Serving，TorchServe和其他ML库。它通过提供可扩展的端点来管理负载。此外，您可以在单个API端点上部署多个模型，并且它支持用于保护API的自动扩展功能。它是一个MLOps工具，赠款您完全控制模型管理操作。\",\"相关链接：https://www.cortex.dev/\"]},\"70\":{\"h\":\"3.9 Evidently（监控） ⭐\",\"t\":[\"用途：监控\",\"开源Python库，用于在开发、验证和生产过程中监控ML模型。它检查数据和模型质量、数据漂移、目标漂移以及回归和分类性能。Evidently有三个主要组成部分：测试（批量模型检查）：用于执行结构化数据和模型质量检查。报告（交互式仪表板）：交互式数据漂移、模型性能和目标虚拟化。实时监控（Real-time monitoring）：监控来自已部署ML服务的数据和模型指标。\",\"相关链接：https://www.evidentlyai.com/\",\"相关链接：https://zhuanlan.zhihu.com/p/398651743\"]},\"71\":{\"h\":\"3.10 Censius AI\",\"t\":[\"模型监控（与Wandb类似）\",\"相关链接：https://censius.ai/\",\"在https://www.projectpro.io/article/best-mlops-tools-/574提到了大部分上述已有的工具，此外，还提到了一个工具：\"]},\"72\":{\"h\":\"4.1 Hydrosphere（性能监控）\",\"t\":[\"用途：性能监控（它的功能之一）\",\"Hydrosphere是一个用于在生产环境中部署、版本控制和监控机器学习模型的平台。它与语言和框架无关，支持所有主要的编程语言和框架- Python，Java，Tensorflow，Pytorch等。其组件Hydrosphere Monitoring在监控机器学习模型部署方面发挥着最关键的作用，以下是其其他关键功能： 跟踪模型性能：Hydrosphere Monitoring检查数据的变化，跟踪已部署的ML模型的性能，并在数据问题发生时向用户发送通知。 易于理解的推论：Hydrosphere提供了模型预测的简单摘要，并且不需要用户查看模型结构。此外，Hydrosphere对贡献发生变化的时间给出了明确的解释，这有助于快速了解数据出了什么问题并设计下一个行动计划。 框架不可知服务：用户可以在开源Hydrosphere Serving集群的帮助下实现、检查点和扩展机器学习模型。此外，Hydrosphere允许用户提供在任何框架中开发的模型，并通过gRPC，REST或Kafka流将它们链接起来。 简单方便：Hydrosphere SDK支持部署新训练的模型或与已部署的模型链接，并使用户能够通过监控和可解释性分析进行检查。Hydrosphere还提供与当前机器学习流程的快速合并。\",\"相关链接：https://docs.hydrosphere.io/\",\"https://neptune.ai/blog/best-open-source-mlops-tools和https://neptune.ai/blog/mlops-tools-platforms-landscape非常丰富地介绍了MLOps Tools，这里补充一些介绍：\"]},\"73\":{\"h\":\"5.1 AutoKeras（自动化机器学习）\",\"t\":[\"用途：自动化机器学习\",\"AutoKeras是一个用于自动机器学习（AutoML）的开源库。使用AutoML框架，您可以自动处理原始数据，选择机器学习模型，并优化学习算法的超参数。\",\"相关链接：http://autokeras.com/\"]},\"74\":{\"h\":\"RetNet 论文笔记\",\"t\":[\"RETNET（全称Retentive Network），是微软研究院和清华大学推出的大语言模型(LLM)基本架构。从论文题目可以看出，RETNET在LLM上要优于Transformer，同时实现了平行训练、低耗费推理和良好表现三大特性。RETNET的理论来源是连接循环和注意力，提出了对序列模型的记忆力机制，这支持三个模式，即：平行、循环和分块循环(chunkwise recurrent)。平行意味着允许平行训练；循环意味着可以在O(1)耗费下推理，这可以在不牺牲性能的情况下提高解码吞吐量、降低延迟和减少GPU内存使用；分块循环意味着便于具有线性复杂度的高效长序列建模，每个chunk都可以并行编码。相关代码见https://aka.ms/retnet。\"]},\"75\":{\"h\":\"1 介绍\",\"t\":[\"现在，Transformer已然成为LLM的首选架构。当时提出Transformer架构是为了克服基于RNN的模型的无法并行训练的问题，然而，Transformer的并行训练是有代价的，即推理时比较低效，因为每个step的复杂度都是O(N)，并且要在内存中缓存key-value。这导致了部署基于Transformer的模型不是很友好，随着序列长度的增加，GPU的内存急剧增加，推理速度急速下降。\",\"因此，也有很多人在努力，希望提出一个在保持平行训练和良好表现的前提下，能够实现具有O(1)复杂度的推理的架构。这是很难的，即所谓“不可能三角”：\",\"RETNET则可以同时实现低成本推理、高效长序列建模、和Transformer相似的性能和并行训练。具体来说，作者提出了多尺度保留机制(multi-scale retention mechanism)代替多头注意力(multi-head attention)。它有三种计算范式，即并行、循环和块递归表示。首先，并行表示使训练并行性能够充分利用GPU设备。其次，递归表示能够在内存和计算方面实现有效的O (1)推断。可以显著降低部署成本和延迟。此外，该实现比较简单，没有键值缓存。第三，分块的循环表示可以执行有效的长序列建模。作者对每个本地块进行并行编码以提高计算速度，同时递归地编码全局块以节省GPU内存。\",\"语言模型的实验结果表明，RetNet在尺度曲线和上下文学习方面都具有竞争力。此外，RetNet的推理耗费是不受序列长度影响的。对于7B型号和8k序列长度，RetNet的解码速度比具有键值缓存的Transformer快8.4倍，节省了70%的内存。在训练过程中，RetNet还比使用了FlashAttention的Transformer节省了25-50%的内存，提升了7倍的速度。此外，RetNet的推理延迟对batch size不敏感，允许巨大的吞吐量。这些特性使RetNet可以成为大型语言模型的Transformer的强大继承者。\"]},\"76\":{\"h\":\"2 Retentive Networks\",\"t\":[\"RETNET由L个相同的block组成，与Transformer相似（residual connection, and pre-LayerNorm）。每个RETNET block包括两个模块：多尺度保留模块(multi-scale retention, MSR)和前馈网络(feed-forward network, FFN)模块。\",\"具体可表示为：给定一个输入序列x=x1​...x∣x∣​，RETNET通过自回归的方式编码这个序列。输入向量{xi​}i=1x​首先被转换为X0=[x1​,...x∣x∣​]∈R∣x∣×dmodel​（dmodel​是隐层维数），说白了应该是这样形状的矩阵：\",\"​x11​...xdmodel​,1​​.........​x1,∣x∣​...xdmodel​,∣x∣​​​\",\"然后，一层一层地计算：Xl=RetNetl​(Xl−1),l∈[1,L]。\"]},\"77\":{\"h\":\"2.1 Retention\",\"t\":[\"给定输入X∈R∣x∣×dmodel​，我们把它投影到一维函数v(n)=Xn​⋅wV​上，现在，通过状态sn​来把v(n)映射到o(n)上，为了简化，我们规定vn​=v(n),on​=o(n)。那么有：\",\"sn​=Asn−1​+KnT​vn​,\",\"其中A∈Rd×d,Kn​∈R1×d,KT表示K的转置\",\"on​=Qn​sn​=m=1∑n​Qn​An−mKmT​vm​,\",\"其中Qn​∈R1×d\",\"（上面两个等式称为(1)）\",\"【注：这个Q和K，应该指的是Query和Key，即Q K V中的】\",\"接下来，我们使用投影Qn​,Kn​进行内容感知：\",\"Q=XWQ​,K=XWK​\",\"（上面的等式称为(2)）\",\"这里的WQ​,WK​指的是可学习的矩阵。\",\"下面，我们把矩阵A对角化：A=Λ(γeiθ)Λ−1，这里的γ,θ∈Rd。这样我们可以得到An−m=Λ(γeiθ)n−mΛ−1。通过吸收Λ到WQ​,WK​中，我们可以重写(1)式：\",\"on​=m=1∑n​Qn​(γeiθ)n−mKmT​vm​\",\"=m=1∑n​(Qn​(γeiθ)n)(Km​(γeiθ)−m)Tvm​\",\"（上面的等式称为(3)）\",\"其中，Qn​(γeiθ)n,Km​(γeiθ)−m称为xPos，这是Transformer的相对位置embedding。我们进一步地把γ简化为一个标量，等式(3)变为：\",\"on​=m=1∑n​γn−m(Qn​einθ)(Km​eimθ)†vm​\",\"其中†表示共轭转置。该公式在训练实例中很容易被并行化。\",\"Retention的平行表示。Retention Layer结构图如下：\",\"Layer的定义如下：\",\"与自注意力类似，并行表示使能够有效地用GPU训练模型。\",\"伪代码如下：\",\"def ParallelRetention( q, # bsz ∗ num_head ∗ len ∗ qk_dim k, # bsz ∗ num_head ∗ len ∗ qk_dim v, # bsz ∗ num_head ∗ len ∗ v_dim decay_mask # num_head ∗ len ∗ len ): retention = q @ k.transpose(−1, −2) retention = retention ∗ decay_mask output = retention @ v output = group_norm(output) return output \",\"Retention的循环表示。如图b，所提出的机制也可以写成RNN，这有利于推理。对于第n个时间步长，我们递归地得到的输出为：\",\"伪代码如下：\",\"def RecurrentRetention( q, k, v, # bsz ∗ num_head ∗ len ∗ qkv_dim past_kv, # bsz ∗ num_head ∗ qk_dim ∗ v_dim decay # num_head ∗ 1 ∗ 1 ): current_kv = decay ∗ past_kv + k.unsqueeze (−1) ∗ v.unsqueeze(−2) output = torch.sum(q.unsqueeze(−1) ∗ current_kv, dim=−2) output = group_norm(output) return output, current_kv \",\"Retention的块循环表示。一种并行表示和循环表示的混合形式可用于加速训练，特别是对于长序列。我们将输入序列划分成块。在每个块中，我们遵循并行表示（公式(5))来进行计算。相反，交叉块信息按照循环表示方式传递（公式(6))。具体来说，设B表示块的长度。我们通过以下方法计算第i个块的Retention输出：\",\"伪代码如下：\",\"def ChunkwiseRetention( q, k, v, # bsz ∗ num_head ∗ chunk_size ∗ qkv_dim past_kv, # bsz ∗ num_head ∗ qk_dim ∗ v_dim decay_mask, # num_head ∗ chunk_size ∗ chunk_size chunk_decay, # num_head ∗ 1 ∗ 1 inner_decay, # num_head ∗ chunk_size ): retention = q @ k.transpose(−1, −2) retention = retention ∗ decay_mask inner_retention = retention @ v cross_retention = (q @ past_kv) ∗ inner_decay retention = inner_retention + cross_retention output = group_norm(retention) current_kv = chunk_decay ∗ past_kv + k.transpose(−1, −2) @ v return output, current_kv \"]},\"78\":{\"h\":\"2.2 Gated Multi-Scale Retention\",\"t\":[\"作者在每个layer中使用h=dmodel​/d，其中d表示头维度(head dimension)，不同的头使用不同的参数矩阵WQ​,WK​,WV​∈Rd×d，而且，多尺度保留(multi-scale retention, MSR)对不同的头指定了不同的γ。为了简单起见，我们在不同的层之间设置了相同的γ，并保持它们不变。此外，作者引入了倾斜门(swish gate)以增加retention layers的非线性。这样，给定X，我们定义layer为：\",\"Retention分数归一化。作者用GroupNorm的尺度不变性来提高retention layers的数值精度。具体来说，在GroupNorm中乘一个标量不会影响输出和反向梯度，即GroupNorm(α∗headi​)=GroupNorm(headi​)。作者在等式(5)中实现了三个归一化因子。第一，正规化QKT为QKT/d​；第二，将D变为D~nm​=Dnm​/∑i=1n​Dni​​；第三，让R表示retention scores R=QKT⨀D，正规化R为R~nm​=Rnm​/max(∣∑i=1n​Rni​∣,1)，这样，retention的输出变为Retention(X)=R~V 。由于尺度不变的性质，上述技巧在稳定正向和反向通道的数值流动的同时，并不影响最终的结果。\"]},\"79\":{\"h\":\"2.3 ❇️Retention网络的整体架构\",\"t\":[\"对于L层的Retention网络，作者堆叠多尺度缩放retention(multi-scale retention, MSR)和前馈网络(FFN)。输入序列{xi​}i=1∣x∣​通过word embedding层转为向量，然后用这个向量X0=[x1​,...x∣x∣​]∈R∣x∣×dmodel​（dmodel​是隐层维数）作为模型的输入，并且通过下列公式计算模型的输出：\",\"上述公式中，LN表示LayerNorm，FFN(X)=gelu(XW1​)W2​，W1​,W2​是参数矩阵。\",\"训练。在训练过程中，作者使用了平行模式（公式5）和块循环模式（公式7）。这两个模式可以利用GPU加速计算。特别地，分块训练对长序列训练特别有用，这在FLOPs和内存消耗方面都是比较好的。\",\"推理。推理过程中，作者使用了循环模式（公式6），这可以较好地拟合自回归解码。并且这可以在获得相同结果的同时，以O(1)复杂度执行。\"]},\"80\":{\"h\":\"3 实验\"},\"81\":{\"h\":\"一些临时随写内容\"},\"82\":{\"h\":\"应用层协议\",\"t\":[\"HTTP 超文本传输协议\",\"SMTP 简单文件发送协议，注意只能发送，不能接收\",\"IMAP/POP3 邮件接收协议，IMAP更高级\",\"FTP 文件传输协议\",\"Telnet 远程登录协议，现在用SSH\",\"SSH 安全的网络传输协议\",\"RTP 实时传输协议 UDP\",\"DNS 域名管理系统 UDP\"]},\"83\":{\"h\":\"传输层协议\",\"t\":[\"TCP 传输控制协议，面向连接，可靠的\",\"UDP 用户数据包协议，无连接，不保证可靠\"]},\"84\":{\"h\":\"网络层协议\",\"t\":[\"IP 网际协议\"]},\"85\":{\"h\":\"6、JVM垃圾回收\",\"t\":[\"JVM的垃圾回收，主要针对的就是对堆内存的回收\"]},\"86\":{\"h\":\"1.堆内存结构\",\"t\":[\"对于JVM堆来说，主要分为以下部分：\",\"新生代，其中新生代中包括伊甸园和两个存活区\",\"老年代\",\"元空间（JDK8之后方法区的实现方式）\"]},\"87\":{\"h\":\"2.内存分配和回收原则\"},\"88\":{\"h\":\"2.1 分配原则\"},\"89\":{\"h\":\"新的对象优先分配到新生代的伊甸园\",\"t\":[\"当伊甸园区不够之后，会考虑进行一次垃圾回收 (Minor GC) ，给伊甸园腾出一些空间，之前处于伊甸园区的对象会被移动到存活区，如果存活区空间不够，那么只好通过空间分配担保，把对象提前放到老年代。\"]},\"90\":{\"h\":\"大内存对象会被分配到老年代\",\"t\":[\"对于大内存对象，会直接分配到老年代\"]},\"91\":{\"h\":\"长期存活的对象将会进入老年代\",\"t\":[\"一个对象出生在伊甸园，年龄为0，在经历过一次垃圾回收并且活下来，则会进入存活区，年龄为1，此后，每熬过一次垃圾回收，年龄都会增加一岁，直到到达了某个年龄，这个对象会进入老年代。\"]},\"92\":{\"h\":\"GC分类\",\"t\":[\"Minor/Young GC，只进行新时代回收\",\"Old GC，只进行老年代回收\",\"Full/Major GC，对堆中的所有位置都进行回收\",\"Mixed GC，只对新时代和部分老年代进行回收\"]},\"93\":{\"h\":\"空间分配担保\",\"t\":[\"空间分配担保，就是在进行Minor GC之前，JVM会进行老年代空间大小确认，只有满足了以下条件，才会进行Minor GC，否则进行Full GC：\",\"老年代剩余的连续空间大于新生代目前对象所占空间或者历次平均占用空间\"]},\"94\":{\"h\":\"2. 死亡对象的判断方法\",\"t\":[\"有两种常见的死亡对象判断方法：引用计数器和可达性分析\"]},\"95\":{\"h\":\"2.1 引用计数器\",\"t\":[\"引用计数器的想法比较简单：\",\"如果一个对象被引用了，那么引用次数加1\",\"如果一个对象被引用结束了，引用次数减1\",\"如果一个对象引用次数为0，那么该对象可以被垃圾回收了\",\"但是引用计数器有个问题，就是没用办法解决循环引用，如果两个对象相互引用，即使它们和外界没用任何联系了，它们的引用次数也都不是0，也就没有办法被回收\"]},\"96\":{\"h\":\"2.2 可达性分析\",\"t\":[\"可达性分析，就是从一系列被称之为\\\"GC Roots\\\"的对象开始，去寻找被它引用的对象链。如果一个对象最终没有被GC Roots引用到，那么就代表可以进行回收了。\"]},\"97\":{\"h\":\"哪些对象可以作为GC Roots\",\"t\":[\"虚拟机栈中引用的对象\",\"本地方法栈中引用的对象\",\"方法区中的静态变量引用的对象\",\"方法区中常量池中的变量引用的对象\",\"所有被同步锁持有的对象\"]},\"98\":{\"h\":\"不可达的对象一定会被垃圾回收吗\",\"t\":[\"不一定，一个对象被垃圾回收，至少要经过两次 可达性分析 标记过程。第一次判断不可达后，还要进行一次筛选，判断该对象是否有必要执行finalize方法，如果一个对象已经执行过一次finalize方法或者没有覆盖finalize方法，那就不会被垃圾回收。\",\"如果一个对象被判断确实有必要执行finalize方法，那么会将它放到一个队列当中去，但是也不是非死不可，在队列中只要这个对象和任何一个引用链搭上关系，它就会被移除队列，继续活下去。\"]},\"99\":{\"h\":\"2.3 如何判断一个常量可以被回收\",\"t\":[\"当一个常量没有任何引用指向它，那么便可以进行回收\"]},\"100\":{\"h\":\"2.4 如何判断一个类可以进行回收\",\"t\":[\"注意！这里说的是类，不是上面提到的对象的回收\",\"一个类要进行回收，需要满足三大条件：\",\"它的所有实例对象都已经被回收，堆中没有任何该类的实例\",\"它的类加载器ClassLoader已经被回收\",\"它的Class对象的引用都已经被回收，不会有反射可以反射到这个类\",\"满足这三大条件，这个类就可以被回收，但是不像对象那样，说回收就回收，类到这里也不会一定回收\"]},\"101\":{\"h\":\"3.引用类型总结\",\"t\":[\"总结于文章1\",\"Java中有四大引用：强引用、软引用、弱引用和虚引用，它们的强度依次下降\"]},\"102\":{\"h\":\"3.1 强引用\",\"t\":[\"强引用是我们最常见的引用，我们直接new出来的引用都是强引用。\",\"强引用不会被垃圾回收，即使内存不足，宁可抛出OOM也不会进行内存回收\"]},\"103\":{\"h\":\"3.2 软引用\",\"t\":[\"软引用比强引用弱一些，它只会在内存不足的时候才会被回收。它使用SoftReference类来声明。\",\"软引用的垃圾不足才回收的特点，可以用来做高速缓存。比如MyBatis中的SoftCache：\",\"public Object getObject(Object key) { Object result = null; SoftReference<Object> softReference = (SoftReference)this.delegate.getObject(key); if (softReference != null) { result = softReference.get(); if (result == null) { this.delegate.removeObject(key); } else { synchronized(this.hardLinksToAvoidGarbageCollection) { this.hardLinksToAvoidGarbageCollection.addFirst(result); if (this.hardLinksToAvoidGarbageCollection.size() > this.numberOfHardLinks) { this.hardLinksToAvoidGarbageCollection.removeLast(); } } } } return result;} \",\"在测试引用的时候，注意不要用 String s = \\\"a\\\" 而是要用 String s = new String(\\\"a\\\")。\"]},\"104\":{\"h\":\"3.3 弱引用\",\"t\":[\"弱引用比软引用还低一些，对于弱引用，进行垃圾回收的时候，无论内存够不够，只要弱引用所在区域进行垃圾回收了，就会将它回收掉。弱引用使用WeakReference进行创建。\"]},\"105\":{\"h\":\"弱引用的应用\",\"t\":[\"弱引用的应用包括WeakHashMap，我们在使用HashMap的时候，键是强引用的，进行垃圾回收不会把它回收掉，而WeakHashMap的键是弱引用的，遇到垃圾回收就会回收掉，适合做一些缓存表\",\"WeakHashMap测试：\",\"package d240323; import java.util.HashMap; import java.util.WeakHashMap; /** * @author KindBrave * @since 2024/3/23 */ public class Main { public static void main(String[] args) throws InterruptedException { HashMap<String, String> map = new HashMap<>(); WeakHashMap<String, String> weakHashMap = new WeakHashMap<>(); map.put(new String(\\\"key\\\"), \\\"a\\\"); weakHashMap.put(new String(\\\"key\\\"), \\\"b\\\"); System.out.println(map); //{key=a} System.out.println(weakHashMap); //{key=b} System.gc(); System.out.println(map); //{key=a} System.out.println(weakHashMap); //{} } } \"]},\"106\":{\"h\":\"3.4 虚引用\",\"t\":[\"虚引用是最弱的一种引用，它不和生命周期相关联，随时可能被回收。它必须和ReferenceQueue来做配合，主要用于判断一个对象什么时候被回收，在回收之前会将这个虚引用放到队列中，方便做一些回收前的操作。\"]},\"107\":{\"h\":\"4.垃圾收集算法\",\"t\":[\"垃圾收集算法整体来说有四种：标记-清除，标记-复制，标记-整理，分代收集\"]},\"108\":{\"h\":\"4.1 标记-清除\",\"t\":[\"标记-清除是最基本的垃圾收集算法，它的思想就是对没有引用的对象进行标记，然后清除\",\"这样清除可能导致的最大问题就是内存碎片，因为被清除的内存位置都是不连续的\",\"其次，标记和清除两个过程耗时都比较大\"]},\"109\":{\"h\":\"4.2 标记-复制\",\"t\":[\"为了解决内存碎片问题，有了标记-复制算法，标记-复制算法就是把内存分为两块区域，每次使用只使用一块，回收之前，会对没有引用的对象进行标记，回收时，会把仍然需要存活的对象复制到另一块内存区域，这一块整体清除。\",\"标记-复制的最大问题就是可用内存减半了\"]},\"110\":{\"h\":\"4.3 标记-整理\",\"t\":[\"标记-整理不会像标记-复制那样，可用内存减半，而是把仍然需要存活的对象给整理到一块，然后把后面的内存都释放掉，这样，仍然存活的对象都是连续的，后面也会空出大量的连续空间\"]},\"111\":{\"h\":\"4.4 分代收集\",\"t\":[\"其实上面的三种垃圾收集策略各有各的特点，也各有各的缺点，所以根据不同代的特点来选择不同策略是比较好的。比如：\",\"新生代经常有大量对象死亡，所以选择标记-复制\",\"老年代一般对象数量多、占用内存大，而且没有内存分配担保，所以选择标记-整理或者标记-清除\",\"这也是为什么JVM堆要进行分代的原因\"]},\"112\":{\"h\":\"5.垃圾收集器\",\"t\":[\"JVM中垃圾收集器数量众多。\",\"Serial\",\"ParNew\",\"Parallel Scavenge\",\"CMS\",\"G1\",\"ZGC\"]},\"113\":{\"h\":\"5.1 Serial 串行\",\"t\":[\"Serial是JVM第一个垃圾收集器，它是串行单线程的，也就是说，在垃圾回收的时候必须暂停工作线程。\",\"Serial使用了分代收集，\",\"对于新生代，Serial使用了标记-复制\",\"对于老年代，Serial使用了标记-整理\"]},\"114\":{\"h\":\"5.2 ParNew\",\"t\":[\"ParNew和Serial类似，就是Serial的多线程版本\",\"它是Server虚拟机的首选，因为除了Serial，只有ParNew可以和CMS配合。\"]},\"115\":{\"h\":\"5.3 Parallel Scavenge\",\"t\":[\"Parallel Scavenge是一个重视CPU效率的回收器，更加关注吞吐量，它同样采用了分代回收：\",\"新生代标记-复制\",\"老年代标记-整理\"]},\"116\":{\"h\":\"5.4 CMS Concurrent Mark Sweep\",\"t\":[\"CMS更加关注于用户的体验，也就是尽可能让回收快速进行或者和工作线程并行执行\",\"CMS没有使用分代回收，而是统一使用标记-清除\",\"它的执行过程如下：\",\"初始标记：首先暂停工作线程，标记直接和GC Roots相连接的对象，这个速度很快\",\"并发标记：工作线程恢复，同时进行可达性分析，但是因为工作线程执行过程对象的引用可能会发生变化，所以这个步骤就是记录哪些引用发生了变化\",\"重新标记：暂停工作线程，对上个步骤中发生变化的内容进行重新标记\",\"回收：工作线程恢复，同时进行垃圾回收\",\"缺点：\",\"对于回收过程中产生的引用变化无法回收\",\"标记-清除会产生大量内存碎片\"]},\"117\":{\"h\":\"5.5 G1\",\"t\":[\"G1收集器在尽可能较短时间完成回收的基础上，还注重了高吞吐量。\",\"G1的特点\",\"并发\",\"分代\",\"策略，G1整体上看是标记-整理，部分上看是标记-复制\",\"可预测的停顿\",\"G1的流程\",\"初始标记\",\"并发标记\",\"最终标记\",\"（以上三个和CMS类似）\",\"回收，G1的回收不像CMS一样是并发的，而是暂停工作线程的。\"]},\"118\":{\"h\":\"5.6 ZGC\",\"t\":[\"比较新\",\"暂停时间在几毫秒内，并且不受堆大小影响，但是会牺牲一些吞吐量\"]},\"119\":{\"h\":\"3、JVM内存结构和Java内存模型\",\"t\":[\"参考参考1参考2\"]},\"120\":{\"h\":\"1.JVM内存结构（也叫JVM内存模型）\",\"t\":[\"JVM内存结构包括\",\"堆\",\"方法区\",\"虚拟机栈\",\"本地方法栈\",\"程序计数器\"]},\"121\":{\"h\":\"想额外强调关于常量池的事情：\",\"t\":[\"JDK 1.7之前，常量池在方法区（当时方法区的实现方法是永久代）中\",\"JDK 1.7，字符串常量池单独从方法区移到堆中，其他常量还在方法区中\",\"JDK 1.8，方法区的实现方式由永久代改为元空间，所以字符串常量池在堆中，其他常量在元空间中\"]},\"122\":{\"h\":\"1.1 堆\",\"t\":[\"堆是线程共享的，是引用类型的存放位置，也是垃圾回收的主要区域\",\"堆中可以分为新生代和老年代\"]},\"123\":{\"h\":\"1.2 方法区\",\"t\":[\"方法区也是线程共享的，它会存放一些类的静态变量和类的信息（即类模型对象）\"]},\"124\":{\"h\":\"1.3 虚拟机栈\",\"t\":[\"虚拟机栈是线程私有的，虚拟机栈是存放方法执行中的基本类型和引用类型的引用的位置\"]},\"125\":{\"h\":\"1.4 本地方法栈\",\"t\":[\"本地方法栈也是线程私有的，是Java调用native原生方法执行C/C++方法的位置\"]},\"126\":{\"h\":\"1.5 程序计数器\",\"t\":[\"程序计数器也是线程私有的，它记录了虚拟机执行到了哪行字节码\"]},\"127\":{\"h\":\"2.Java内存模型JMM\",\"t\":[\"Java内存模型是给Java多线程通信准备的，整体上围绕了\",\"可见性\",\"有序性\",\"原子性\",\"三大特性展开\"]},\"128\":{\"h\":\"2.1 可见性\",\"t\":[\"所谓可见性，就是一个线程对某个变量进行操作，其他线程是可以看见的。我们可以使用volatile关键字或者synchronized代码段来保证可见性。\"]},\"129\":{\"h\":\"2.2 有序性\",\"t\":[\"为了优化性能，JVM会进行指令重排，比如写操作可能会放到最后执行。但是对于多线程来说，指令重排可能会导致一些问题，比如一个线程的读在另一个线程的写之后，为了解决这一问题，JVM引入了happens-before原则，保证了多线程操作的有序性\"]},\"130\":{\"h\":\"2.3 原子性\",\"t\":[\"原子性是指某一个操作是原子的，不会被其他线程打断。Java中使用volatile关键字标记的变量读写就不会被其他线程打断。\"]},\"131\":{\"h\":\"2.4 工作内存和主内存\",\"t\":[\"主内存是线程共有的，其中保存着一些变量。\",\"工作内存是线程私有的，是JVM虚拟出来的一块内存，它之中保存着主内存中变量的副本。\",\"一个线程对主内存的变量进行写入，先通过工作内存，然后再写入主内存。然后，另一个线程会从主内存中读取这个变量到自己的工作内存中，从而得知变化。\"]},\"132\":{\"h\":\"1、类的加载流程\"},\"133\":{\"h\":\"1.Java类的加载流程\",\"t\":[\"总结于https://zhuanlan.zhihu.com/p/268637051\",\"对于加载流程，简单来说，就是把class字节码中的内容加载到内存当中去。\",\"Java的数据类型分为基本类型和引用类型，基本类型是通过JVM加载的，而引用类型是通过类加载器加载的，我们这里说的类的加载流程，指的应该是引用类型的加载。\",\"类加载过程分为七个步骤：加载、验证、准备、解析、初始化、调用、卸载\",\"其中验证、准备、解析合称为链接部分。\"]},\"134\":{\"h\":\"1.1 加载(Loading)\",\"t\":[\"首先，第一个步骤就是加载。这个加载过程就是把class字节码的内容加载到内存当中去，并且得到类的原型--类模板对象。\",\"我们通过class字节码中的信息，来得到类模板对象，类模板对象中会存放类的信息，包括类的方法和字段等等，我们可以通过这些信息来调用类的内容。这个过程很容易联想到反射，也确实，Java的反射用到了这里。\",\"这个过程要完成三个事：\",\"根据类名来获取类的二进制数据流\",\"根据类的二进制数据流来得到类模板对象\",\"根据类模板对象来得到类的信息，并且得到Class对象 ->根据类模板对象来创建Class对象\"]},\"135\":{\"h\":\"1.1.1 类模板和Class对象的位置\",\"t\":[\"类模板放到（方法区的）元空间里\",\"Class对象是一个对象，所以放到堆中\"]},\"136\":{\"h\":\"1.2.验证\",\"t\":[\"验证阶段包括四大验证：\",\"格式验证\",\"语义验证\",\"字节码验证\",\"引用验证 符号引用验证\"]},\"137\":{\"h\":\"1.2.1 格式验证\",\"t\":[\"格式验证其实是发生在加载过程中的，只有格式验证成功了，类的二进制数据流才会加载到方法区当中，也就是才会在元空间有这个类模板对象\",\"其他三种验证就是在方法区中进行的了\",\"格式验证包括验证魔数、版本号是否受JVM支持~~、是否继承了final类等等~~\"]},\"138\":{\"h\":\"1.2.2 语义验证\",\"t\":[\"语义验证用来分析字节码文件的语义是否合规，比如是否继承了final类，抽象类是否有子类实现了等\"]},\"139\":{\"h\":\"1.2.3 字节码验证\",\"t\":[\"用来分析字节码是否合规\",\"用来分析字节码跳转位置是否正确\"]},\"140\":{\"h\":\"1.2.4 引用验证 符号引用验证\",\"t\":[\"用来判断引用是否存在\"]},\"141\":{\"h\":\"1.3.准备\",\"t\":[\"所谓准备阶段，就是为类中的静态变量分配内存空间，并且赋予默认值\",\"需要注意的是，如果一个静态变量是final类型的，它的默认值会在编译阶段进行赋予，在准备阶段只会进行分配内存空间\",\"如果以字符串格式赋给String类型的静态变量，也会在这里分配空间\"]},\"142\":{\"h\":\"1.4.解析\",\"t\":[\"解析阶段需要做的就是把符号引用解析为直接引用\"]},\"143\":{\"h\":\"1.5.初始化\",\"t\":[\"初始化过程主要是对类中的静态变量进行正确的赋值\"]},\"144\":{\"h\":\"1.6.使用\",\"t\":[\"一切完成之后就可以进行使用了\"]},\"145\":{\"h\":\"1.7.卸载\",\"t\":[\"当一个类的使命完成后，这个类就会被卸载，但是，一个类被卸载的概率很小\"]},\"146\":{\"h\":\"4、MySQL索引\"},\"147\":{\"h\":\"1.索引的介绍\",\"t\":[\"索引就是为了快速查找内容设立的，可以参考新华字典里的目录，如果说直接去后面翻来找字的话，那么时间会非常长，但是如果通过前面的目录进行拼音查找或者部首查找，那么效率就非常高了，实际上这里的拼音和部首，就是两种索引。\"]},\"148\":{\"h\":\"2.索引的优缺点\",\"t\":[\"优点：\",\"可以让查找效率变得很高\",\"通过建立唯一索引，可以让行唯一\",\"缺点：\",\"建立和修改索引，都需要大量的时间。因为每次对数据内容进行更变，都需要更变索引的内容\",\"索引是一个文件，需要存放到磁盘中，所以会占用一定的空间\"]},\"149\":{\"h\":\"3.索引的底层数据结构\"},\"150\":{\"h\":\"3.1 hash\",\"t\":[\"哈希表是一种键值对存储，我们可以把索引放到key中。对于散列函数比较优秀的哈希表来说，基本上不会产生冲突，如果产生了冲突，那就通过链表的方式来解决。正常来说，哈希表可以实现O(1)时间的查找。\",\"缺点：\",\"哈希表单查找确实很快，但是进行范围查找，比如a>100 and a<200，那就不太合适了，对于哈希表结构来说，每个值都需要进行一次查找\"]},\"151\":{\"h\":\"3.2 二叉查找树\",\"t\":[\"二叉查找树是一个二叉树，但是它节点有一些特点：每个节点的左孩子都比自己小，每个节点的右孩子都比自己大\",\"这样的话，如果查找一个值，平均只需要O(logn)的时间就可以完成，但是二叉查找树可能会退化到一条链，这是进行查找和普通链表没什么区别了，会退化成O(n)时间\"]},\"152\":{\"h\":\"3.3 平衡二叉树\",\"t\":[\"为了解决上面二叉查找树会退化为一条链的情况，平衡二叉树做了一些工作。\",\"平衡二叉树规定，对于每个节点来说，左孩子和右孩子的高度之差不能大于1，如果大于1了，就要进行多次左旋或者右旋，直到高度之差满足条件。\",\"缺点：\",\"平衡二叉树很好地解决了查找退化的问题，但是因为每次插入或者删除，都可能会进行多次的左旋或者右旋，这样消耗的时间比较大\"]},\"153\":{\"h\":\"3.4 红黑树\",\"t\":[\"接下来就是红黑树，上面说的平衡二叉树每次都可能进行多次的左旋或者右旋，而红黑树每次都只进行一次左旋或者右旋。\",\"一个红黑树有以下5个特点：\",\"每个节点不是红色就是黑色的\",\"根节点一定是黑色的\",\"叶子节点是黑色的空节点\",\"一个节点是黑色的话，那么它的孩子就是红色的，反过来不一定\",\"从根节点到叶子节点，每条路径上的黑色节点数量一定是相同的\",\"红黑树因为每次的左旋或者右旋都只进行一次，所以插入和删除的效率比平衡二叉树来说是有所提升的\",\"但是红黑树并不是完全保证平衡的，也就是对于一个节点来说，它的左孩子和右孩子的高度可能差值会大一些，这样查找效率可能会有所下降\"]},\"154\":{\"h\":\"3.5 B树/B+树\",\"t\":[\"B树叫做多路平衡查找树，B+树是B树的一个改进\",\"B树和B+树的区别如下：\",\"B树所有位置都可以存放key和data，B+树只有叶子节点存放key和data，其他位置只存放key\",\"B树的叶子节点是相互独立的，B+树的叶子节点之间是通过一个链连在一起的\",\"B树进行查找只需要查到key就可以了，因为节点中既有key又有value；而B+树必须要查到叶子节点\",\"对于范围查找来说，B树需要查找到左范围，然后通过中序遍历，直到找到右范围；而B+树通过叶子节点的链表即可\",\"这样来说，B+树有很多优势：\",\"查找更稳定，每个都需要到达叶子节点\",\"范围查找更快捷\",\"所以，MySQL的InnoDB和MyISAM(Maiˈzæm)都使用了B+树。\"]},\"155\":{\"h\":\"4.主键索引和二级索引\",\"t\":[\"主键索引就是为主键建立的索引，对于InnoDB来说，主键索引中的key是主键，data就是这一行的内容了，对于MyISAM来说，主键索引中的data也是指向数据的一个值\",\"对于InnoDB来说，如果没有指定主键所以，它会去寻找唯一非空索引，如果有，那么就把它当作主键索引，否则就创建一个6bit的主键索引\",\"对于二级索引来说，其data的值不是数据，而是主键的值，这样通过二级索引来检索数据，实际上是需要回表的，再根据主键索引查找到数据\",\"在InnoDB中，除了主键索引都是二级索引，包括\",\"普通索引\",\"唯一索引\"]},\"156\":{\"h\":\"5.聚簇索引和非聚簇索引\"},\"157\":{\"h\":\"5.1 聚簇索引\",\"t\":[\"所谓聚簇索引，就是指的索引和数据是在一起的，比如InnoDB中的主键索引。而MyISAM主键索引也是非聚簇索引，主键索引中的data也不是这一行的数据，而是指向了数据的位置\",\"下面记一下，忘记了\",\"优点：\",\"对于排序查找和范围查找很方便\",\"聚簇查找不需要进行回表查找，因为data中就是这一行的信息了。查找非常快，因为聚簇查找不需要进行回表操作，这样会减少一次IO操作\",\"缺点：\",\"依赖于有序数据，如果数据是有序的，那么建立和搜索索引都会非常顺序，如果是无序的，比如UUID什么的，那么索引之间的比较就会很耗时\",\"更新代价大。因为聚簇索引数据是放到data中的，这样的话更新索引也需要处理这些数据，耗时较大\"]},\"158\":{\"h\":\"5.2 非聚簇索引\",\"t\":[\"非聚簇索引就是索引中的data存放的不是真正的数据，而是数据的指针或者主键的值。\",\"优点：\",\"更新消耗小，因为非聚簇索引中没有真正的值\",\"缺点：\",\"依赖有序数据，同上\",\"查找需要进行回表操作，数据查询比较慢\"]},\"159\":{\"h\":\"非聚簇索引一定会回表查询吗？\",\"t\":[\"不一定，如果查的列恰好是有索引的，那对于InnoDB来说，就不需要进行回表了。\",\"对于MyISAM来说，如果查的列是主键，那么其实也不需要进行回表。\",\"这种情况就是覆盖索引了。\"]},\"160\":{\"h\":\"6.覆盖索引和联合索引\",\"t\":[\"所谓覆盖索引，就是准备查找的列恰好都是有索引的列，这样查找的时候就不需要进行回表操作。\",\"所谓联合索引，就是把多列合在一起共同作为一个索引。\"]},\"161\":{\"h\":\"6.1 最左前缀匹配原则\",\"t\":[\"忘了，看看\",\"最左前缀匹配原则，就是在使用联合索引时，根据联合索引中从左到右的顺序，先过滤掉一些不符合的数据再进行下面的匹配，这样效率会高一些。所以在进行联合索引设计时，应该把一些过滤效果好的列放到左边。\",\"过滤什么时候会停止？> <都会，>= <= between like不会\",\"所以对于联合索引 (a, b)来说，如果查询：\",\"select a, b where a=1 and b=2是可以走索引的，但是查询：\",\"select b 就是不走索引的，\",\"select a, b, c where a = 1 and b > 2 and c=1，那么c就是走不了索引的\"]},\"162\":{\"h\":\"7.索引下推\"},\"163\":{\"h\":\"8.正确使用索引的建议，感觉比较重要\"},\"164\":{\"h\":\"8.1 选择合适的列作为索引\",\"t\":[\"不应该把可空的列作为索引，因为索引很难优化NULL值\",\"频繁查找的列作为索引\",\"频繁作为条件查询的列作为索引\",\"频繁作为连接字段的列作为索引\",\"频繁更新的列不要作为索引，因为更新也需要更新索引，所以比较耗时\"]},\"165\":{\"h\":\"8.2 每张表上的索引不宜过多\",\"t\":[\"每张表上的索引最多不要大于5个，索引虽然能够加快查找，但是也需要消耗时间来维护，并且也需要占用一定的存储空间，如果索引过多，那么可能会适得其反\"]},\"166\":{\"h\":\"8.3 删除不常用的索引\"},\"167\":{\"h\":\"8.4 尽可能地使用联合索引\",\"t\":[\"我们应该尽可能地使用联合索引而不是单列索引，因为索引也是要存储的，我们可以把索引联合起来，这样只需要存一个B+树\"]},\"168\":{\"h\":\"8.5 对于字符串索引，要使用前缀索引\",\"t\":[\"因为前缀索引占用空间更小，查询速度更快\"]},\"169\":{\"h\":\"8.6 要避免索引失效\",\"t\":[\"这个另说，见另一篇\"]},\"170\":{\"h\":\"8.7 要避免索引重复\",\"t\":[\"如果已经建立了联合索引，那么单列索引就不需要了，需要删除\"]},\"171\":{\"h\":\"5、MySQL索引失效\",\"t\":[\"总结于https://juejin.cn/post/7161964571853815822\"]},\"172\":{\"h\":\"1.最左前缀匹配\"},\"173\":{\"h\":\"2.select *\",\"t\":[\"可以使用索引，除非where范围过大，但是不推荐使用select *，因为\",\"查找出来的字段可能没用，浪费网络资源\",\"数据库变化而resultMap变化就会导致报错\"]},\"174\":{\"h\":\"3. 函数\",\"t\":[\"函数本身也不会导致索引失效，但是如果改变了查询的值就会了，比如：\",\"select a where length(a)=4\",\"失效的原因是length(a)不是索引中的内容\"]},\"175\":{\"h\":\"4.计算操作\",\"t\":[\"计算操作也是同理\",\"select a where a + 1 = 4\",\"a + 1也不是索引的内容\"]},\"176\":{\"h\":\"5. like %\",\"t\":[\"左不走右走\"]},\"177\":{\"h\":\"6. in和not in\",\"t\":[\"这个不一定，如果in或者not in中数据大于30%，就不走索引了\"]},\"178\":{\"h\":\"7. order by\",\"t\":[\"order by 为了减少回表时间，直接走的全表扫描\"]},\"179\":{\"h\":\"8.or\",\"t\":[\"只有or左右都有索引才会走索引\"]},\"180\":{\"h\":\"2、TCP三次握手和四次挥手\"},\"181\":{\"h\":\"0.TCP是哪一层的协议？\",\"t\":[\"会话层\"]},\"182\":{\"h\":\"1.三次握手\"},\"183\":{\"h\":\"1.1 TCP的首部包中包括什么？\",\"t\":[\"TCP的首部包中包括：\",\"源端口号 16bit\",\"目的端口号 16bit\",\"序列号 seq 32bit\",\"应答号 ack 32bit\",\"标志位 SYN ACK等\"]},\"184\":{\"h\":\"1.2 TCP三次握手过程\",\"t\":[\"第一次握手：客户端将SYN标志位设为1，同时随机生成一个序列号seq=i，此时客户端进入到SYN_SEND状态\",\"第二次握手：服务端接收到客户端的消息后，会将SYN标志位设为1，ACK标志位为1，同时随机生成一个序列号seq=j，并且把应答号ack设为i+1，此时服务端进到SYN_RECV状态\",\"第三次握手：客户端收到服务端消息，验证了ack确实为i+1，此时客户端进入到ESTABLISHED状态，并且会发送一个ACK标志位为1，ack=j+1报文，服务端收到后，也会进入ESTABLISHED状态\"]},\"185\":{\"h\":\"1.3 为什么要进行三次握手，而不是两次或者四次\",\"t\":[\"这个我们可以通过三次握手分别能确认什么状态来考虑\",\"第一次握手，客户端告诉服务端要连接，服务端接收到消息后，可以确认客户端到服务端通信是正常的\",\"第二次握手，服务端告诉客户端收到消息了，客户端验证了收到的ack确实是i+1，客户端可以确认自己发送是正常的，接收是正常的，此时，服务端还无法确认自己的发送是正常的，所以需要第三次握手\",\"第三次握手，客户端告诉自己收到了服务端的消息，并且ack是j+1，此时服务端也可以确认自己的发送是正常的。到此，客户端和服务端都已经确认自己发送和接收都是正常的了\"]},\"186\":{\"h\":\"2.四次挥手\"},\"187\":{\"h\":\"2.1 四次挥手过程\",\"t\":[\"TCP的挥手既可以是客户端发起的，也可以是服务端发起的，以客户端发起的为例：\",\"第一次挥手：客户端发送挥手报文，报文FIN标志位设置为i\",\"第二次挥手：服务端收到客户端挥手报文，会将ack设置为i+1，至此，可以表示客户端到服务端已经不再进行通信了\",\"第三次挥手：服务端发送挥手报文，报文FIN标志位设置为j\",\"第四次挥手：客户端收到服务端报文，将ACK标志位设置为1，将ack设置为j+1，至此，服务端到客户端也已经不再进行通信了\"]},\"188\":{\"h\":\"2.2 为什么要进行四次挥手，而不是三次挥手\",\"t\":[\"我们可以考虑第三次挥手之后，客户端不再进行第四次挥手会发生什么。\",\"如果只有三次挥手，服务端发送完之后就会断开了，而这个报文可能在传输过程中丢失了，客户端一直都没有收到这条报文，不知道服务端那边要断开了，就会一直处于连接状态\",\"而如果有了第四次挥手，客户端收到服务端的第三次挥手时候，会发送一个报文，如果两倍的最长报文等待时间之后服务端还没有收到第四次挥手，那么可以认为这条报文在传输过程中丢失了，就会重发这个报文，直到接收到为止。\"]},\"189\":{\"h\":\"1.Redis总结\",\"t\":[\"总结于来源1、来源2、来源3\"]},\"190\":{\"h\":\"什么是Redis\",\"t\":[\"Redis是基于C语言开发的、一个开源的（BSD）、内存中的数据结构存储系统（是key-value数据库），可用于数据库、缓存和消息中间件\"]},\"191\":{\"h\":\"为什么要用Redis/Redis的优势/Redis的特点\",\"t\":[\"传统的关系型数据库已经不能胜任所有任务了，比如秒杀的库存扣减、首页的访问高峰，很容易让传统的数据库崩掉，所以引入了缓存中间件\",\"Redis的性能极高，读写速度都非常快\",\"Redis支持的数据类型很多（5种）\",\"原子性：Redis所有操作都是原子性的，要么成功，要么失败，多个操作也支持原子性，可以通过MULTI和EXEC指令包起来\",\"Redis支持数据持久化、数据备份\",\"特征丰富：比如过期删除等\"]},\"192\":{\"h\":\"缓存中间件有什么\",\"t\":[\"缓存中间件有Redis和Memcached等。\",\"Redis比Memcached有优势：\",\"Memcached只有简单的字符串类型，Redis有更加丰富的数据类型\",\"Redis速度比Memcached更快\",\"Redis支持持久化存储\"]},\"193\":{\"h\":\"Redis的数据类型\",\"t\":[\"Redis有5种数据类型：\",\"string\",\"list\",\"set\",\"sorted set\",\"hash\",\"此外，还有新的一些数据类型\",\"Stream\",\"GEO地理位置\",\"HyperLogLog\",\"Bitmap位图\",\"这几种的介绍见Redis数据类型\"]},\"194\":{\"h\":\"Redis是单进程还是多进程\",\"t\":[\"Redis是单进程单线程的，Redis使用队列技术把并发访问变成了串行访问。\"]},\"195\":{\"h\":\"Redis中一个string类型的最大容量是多少\",\"t\":[\"512M\"]},\"196\":{\"h\":\"Redis的持久化机制有哪些\",\"t\":[\"RDB(Redis Database) 使用数据集快照的方式半持久化模式记录Redis数据库的所有键值对。会在某个时间点将数据库中的所有键值对写到一个文件中。 \",\"优点： \",\"只有一个文件dump.rdb，方便持久化\",\"容灾性好，一个文件可以非常方便地存储到安全的磁盘中\",\"性能好，fork子进程来进行写入操作，主进程会继续处理任务\",\"数据集比较大的时候，比AOF启动更快\",\"缺点： \",\"数据安全性低，因为RDB是隔一段时间进行备份的，如果在这之前Redis崩了，从上次备份后的数据就丢失了\",\"AOF(Append-only File) 所有的命令行记录都会以Redis命令请求协议的格式完全持久化保存为aof文件 \",\"优点： \",\"数据安全，AOF可以设置appendsync属性为always，这样每执行一条命令，都会记录到aof文件中\",\"通过append模式写入文件，如果服务器宕机，也可以通过redis-check-aof工具解决数据一致性问题\",\"AOF机制的rewrite模式，在没有进行rewrite之前（文件过大会进行rewrite），可以删除一些误操作的命令\",\"缺点： \",\"文件大，恢复速度慢\",\"文件大时启动比RDB慢\"]},\"197\":{\"h\":\"如果有大量的key需要设置同一时间过期，需要注意什么\",\"t\":[\"如果大量的key的过期时间过于集中，可能会造成Redis的短暂卡顿甚至缓存雪崩，所以可以给过期时间设置一个随机数，让它不那么集中。\"]},\"198\":{\"h\":\"介绍一下如何使用Redis实现一个分布式锁\",\"t\":[\"可以使用setnx key value或者set key value nx来争夺锁，得到锁后可以使用expire key seconds来给锁设置一个过期时间（到时key会删除）\",\"追问：如果设置完锁服务崩了，此时没有设置expire怎么办？\",\"可以使用set key value nx px milliseconds来在争夺锁的同时来设置过期时间\"]},\"199\":{\"h\":\"2.Redis数据类型\",\"t\":[\"总结于来源1来源2\",\"Redis中有5中基本类型和4种常见类型（后续更新支持的），它们是：\"]},\"200\":{\"h\":\"五种基本类型：\",\"t\":[\"string\",\"list\",\"set\",\"sorted set\",\"hash\"]},\"201\":{\"h\":\"四种常见类型：\",\"t\":[\"GEO 地理位置 v3.2支持\",\"HyperLogLog v2.8支持\",\"Bitmap 位图 v2.2支持\",\"Stream v5.0支持\"]},\"202\":{\"h\":\"1.string\",\"t\":[\"string是最基本的key-value结构，key是唯一表示，value不只是字符串，也可以是数字，value最长可容纳的数据长度是512M。\"]},\"203\":{\"h\":\"1.1 内部实现\"},\"204\":{\"h\":\"1.1.1 数据结构\",\"t\":[\"string底层的数据结构主要是int和SDS（简单动态字符串）。\",\"下面之所以和C语言来比，是因为Redis的底层实现是C。\",\"SDS和C标准字符串不太一样，SDS相比C标准字符串，支持：\",\"SDS不仅可以保存文本数据，还可以保存二进制数据。 原因：SDS使用一个参数len来保存字符串长度，而不是通过终止符('\\\\0')来判断字符串是否结束。而且，SDS中的所有API都会以处理二进制的方式来处理存储在buf[]数组中的数据。所以，SDS不仅可以存储文本，还可以存储图片、音频、视频、压缩包等二进制数据。\",\"SDS中获取字符串长度的时间复杂度为O(1)。 原因：因为SDS使用参数len来保存字符串长度，C字符串通过遍历的方式来获取字符串长度，所以时间复杂度为O(n)。\",\"SDS的API是安全的。SDS中拼接字符串不会导致缓冲区的溢出，因为SDS会在拼接之前检查空间是否满足要求，如果不满足就会进行扩容。\"]},\"205\":{\"h\":\"1.1.2 数据编码\",\"t\":[\"string内部编码有int、raw、embstr，其中int对应数据结构int，raw和embstr对应数据结构SDS。\",\"如果一个字符串对象保存的是整数值，并且这个值可以用long来表示，那么字符串对象会将整数值保存在字符串对象结构的ptr属性里，并将字符串对象的编码设置为int。\",\"如果一个字符串对象保存的是一个字符串，且字符串长度小于等于某个字节（v2.x是32字节），那么字符串对象会使用SDS来保存这个字符串），并且将对象编码设置为embstr，embstr是专门用于存储短字符串的一种优化编码。\",\"如果一个字符串对象保存的是一个字符串，且字符串长度大于某个字节，那么字符串对象仍然使用SDS数据结构来保存这个字符串，但是对象编码是raw。此时可以看的ptr是指向SDS的地址的\"]},\"206\":{\"h\":\"那么，embstr和raw有什么区别？\",\"t\":[\"embstr会通过一次内存分配函数来分配一块连续的内存空间来保存redisObject和SDS，而raw会通过两次内存分配来分别分配两块内存空间来保存redisObject和SDS。这样做的好处：\",\"embstr将创建字符串对象所需的内存分配次数从两次降低到了一次\",\"释放embstr编码的字符串同样也只需要调用一次内存释放函数\",\"因为embstr编码的字符串对象的所有数据都保存在一块连续的内存里面，可以更好地利用CPU缓存提升性能\",\"当然，这样做也有缺点：\",\"如果字符串长度增加需要重新分配内存，redisObject和SDS都需要重新分配，所以，embstr编码的字符串实际上是只读的，Redis没有为embstr编码的字符串对象提供相应的修改程序。当我们对embstr编码的字符串进行append操作的时候，Redis会将embstr编码转换成raw编码，然后再进行修改。\"]},\"207\":{\"h\":\"1.2 应用场景\",\"t\":[\"缓存对象\",\"计数\",\"分布式锁 Redis可以实现分布式锁，因为SET命令有个参数NX，可以实现“只有不存在才插入”，这样就可以以插入成功作为加锁成功，插入失败作为加锁失败。同时，SET命令还有一个参数PX，表示这个key多久(ms)后会过期（删掉这个key），可以防止上锁后忘记解锁。\",\"共享Session 在单服务器中，Session可以存到服务器上，但是分布式部署的就不行了，因为下一次分配到的服务器可能不是上一次的，就没有Session信息了，所以可以把Session存到Redis中。\"]},\"208\":{\"h\":\"2.list\",\"t\":[\"list是字符串列表，按插入顺序进行排序，可以进行头插和尾插，列表最大长度位2^32-1。\"]},\"209\":{\"h\":\"2.1 内部实现-数据结构\",\"t\":[\"在v3.2之前，list底层数据结构为双向链表或压缩列表\",\"如果列表长度小于512（默认），列表中数据长度均小于64字节（默认），那么就使用压缩列表。\",\"否则，使用双向链表。\",\"v3.2之后，就只有quicklist了。有关quicklist，可以参考这篇文章。\"]},\"210\":{\"h\":\"2.2 常用命令\",\"t\":[\"lpush key value lpop key rpush key value rpop key # 返回指定区间内的元素 lrange key start stop # 从key列表表头弹出一个元素，没有就阻塞timeout秒，如果timeout=0则一直阻塞 blpop key [key ...] timeout # 从key列表表尾弹出一个元素，没有就阻塞timeout秒，如果timeout=0则一直阻塞 brpop key [key ...] timeout \"]},\"211\":{\"h\":\"2.3 应用场景\",\"t\":[\"消息队列是list的常见应用场景。\",\"消息队列在存取消息时，需要保证三个需求：消息保序、重复消息的处理、保证消息的可靠性。\"]},\"212\":{\"h\":\"2.3.1 list如何保证消息的有序性\",\"t\":[\"list本身就是一个先进先出的数据类型，只需要通过lpush和rpop的组合就可以保证消息的有序性了。\",\"但是这样可能会有一个问题，就是如果list中没有数据，那么消费者就需要一直尝试读取，这样效率比较低，所以Redis提供了一个方法brpop/blpop，可以在list中没有数据时进行阻塞，这样效率会好一些。\"]},\"213\":{\"h\":\"2.3.2 list如何处理重复消息\",\"t\":[\"这个还挺抽象的，为什么需要处理重复消息？因为重复的消息代表已经被处理过了，不需要再处理了，所以需要进行判断。\",\"消费者要实现重复消息的判断，需要 2 个方面的要求：\",\"每个消息都有一个全局的 ID。\",\"消费者要记录已经处理过的消息的 ID。当收到一条消息后，消费者程序就可以对比收到的消息 ID 和记录的已处理过的消息 ID，来判断当前收到的消息有没有经过处理。如果已经处理过，那么，消费者程序就不再进行处理了。\",\"但是List 并不会为每个消息生成 ID 号，所以我们需要自行为每个消息生成一个全局唯一ID，生成之后，我们在用 LPUSH 命令把消息插入 List 时，需要在消息中包含这个全局唯一 ID。\",\"例如，我们执行以下命令，就把一条全局 ID 为 111000102、库存量为 99 的消息插入了消息队列：\",\"> LPUSH mq \\\"111000102:stock:99\\\" (integer) 1 \"]},\"214\":{\"h\":\"2.3.3 如何保证消息的可靠性\",\"t\":[\"所谓的可靠性，就是一个消息执行了，但是失败了，不能就不执行了，还得回到队列中等待执行，list通过brpoplpush把当前读到的数据存到另一个list中进行备份，这样就可以在执行失败后读备份list中的数据执行了。\"]},\"215\":{\"h\":\"2.3.4 list实现消息队列有什么问题\",\"t\":[\"list不支持多个消费者消费同一条数据，因为一条数据一旦被处理，就从list中消失了，其他消费者读不到了。Redis的Stream类型可以解决这个问题。\"]},\"216\":{\"h\":\"3.hash\",\"t\":[\"hash是k-v存储的数据格式，它的value是[{key:value},{key,value}...]这种。hash特别适用于存储对象，要比string方便不少。\"]},\"217\":{\"h\":\"3.1 内部实现\",\"t\":[\"hash类型的底层数据结构是压缩列表或者哈希表\",\"如果哈希类型的元素个数小于512个，所有值小于64字节，Redis会使用压缩列表\",\"否则，会使用哈希表。\",\"v7.0，压缩列表被弃用，使用listpack数据结构了。\"]},\"218\":{\"h\":\"3.2 常用命令\",\"t\":[\"# 存储一个哈希表key的键值 HSET key field value # 获取哈希表key对应的field键值 HGET key field # 在一个哈希表key中存储多个键值对 HMSET key field value [field value...] # 批量获取哈希表key中多个field键值 HMGET key field [field ...] # 删除哈希表key中的field键值 HDEL key field [field ...] # 返回哈希表key中field的数量 HLEN key # 返回哈希表key中所有的键值 HGETALL key # 为哈希表key中field键的值加上增量n HINCRBY key field n \"]},\"219\":{\"h\":\"3.3 应用场景\",\"t\":[\"缓存对象\",\"购物车，以用户id为key，商品id为field，商品数量为value\"]},\"220\":{\"h\":\"4.set\",\"t\":[\"set是一个无序且唯一的键值集合，不会按照插入的先后顺序进行存储。最多可以存储2^32-1个值\",\"set和list的区别\",\"list可以存储重复数据，set只能存储非重复数据\",\"list有序，set无序\"]},\"221\":{\"h\":\"4.1 内部实现\",\"t\":[\"set底层数据结构有两种：哈希表和整数集合\",\"如果集合中的元素都是整数且长度小于512，那么底层数据结构就是整数集合\",\"否则，使用哈希表来存储\"]},\"222\":{\"h\":\"4.2 常用命令\",\"t\":[\"# 往集合key中存入元素，元素存在则忽略，若key不存在则新建 SADD key member [member ...] # 从集合key中删除元素 SREM key member [member ...] # 获取集合key中所有元素 SMEMBERS key # 获取集合key中的元素个数 SCARD key # 判断member元素是否存在于集合key中 SISMEMBER key member # 从集合key中随机选出count个元素，元素不从key中删除 SRANDMEMBER key [count] # 从集合key中随机选出count个元素，元素从key中删除 SPOP key [count] # 交集运算 SINTER key [key ...] # 将交集结果存入新集合destination中 SINTERSTORE destination key [key ...] # 并集运算 SUNION key [key ...] # 将并集结果存入新集合destination中 SUNIONSTORE destination key [key ...] # 差集运算 SDIFF key [key ...] # 将差集结果存入新集合destination中 SDIFFSTORE destination key [key ...] \"]},\"223\":{\"h\":\"4.3 应用场景\",\"t\":[\"集合的主要几个特性，无序、不可重复、支持并交差等操作。\",\"因此 Set 类型比较适合用来数据去重和保障数据的唯一性，还可以用来统计多个集合的交集、错集和并集等，当我们存储的数据是无序并且需要去重的情况下，比较适合使用集合类型进行存储。\",\"但是要提醒你一下，这里有一个潜在的风险。 Set 的差集、并集和交集的计算复杂度较高，在数据量较大的情况下，如果直接执行这些计算，会导致 Redis 实例阻塞 。\",\"在主从集群中，为了避免主库因为 Set 做聚合计算（交集、差集、并集）时导致主库被阻塞，我们可以选择一个从库完成聚合统计，或者把数据返回给客户端，由客户端来完成聚合统计。\"]},\"224\":{\"h\":\"4.3.1 点赞\",\"t\":[\"set可以保证一个id只点赞一次\"]},\"225\":{\"h\":\"4.3.2 共同关注\",\"t\":[\"可以用set的交集来做共同关注功能\"]},\"226\":{\"h\":\"4.3.3 抽奖\",\"t\":[\">SADD lucky Tom Jerry John Sean Marry Lindy Sary Mark (integer) 5 \",\"如果允许重复中奖，可以使用 SRANDMEMBER 命令。\",\"# 抽取 1 个一等奖： > SRANDMEMBER lucky 1 1) \\\"Tom\\\" # 抽取 2 个二等奖： > SRANDMEMBER lucky 2 1) \\\"Mark\\\" 2) \\\"Jerry\\\" # 抽取 3 个三等奖： > SRANDMEMBER lucky 3 1) \\\"Sary\\\" 2) \\\"Tom\\\" 3) \\\"Jerry\\\" \",\"如果不允许重复中奖，可以使用 SPOP 命令。\",\"# 抽取一等奖1个 > SPOP lucky 1 1) \\\"Sary\\\" # 抽取二等奖2个 > SPOP lucky 2 1) \\\"Jerry\\\" 2) \\\"Mark\\\" # 抽取三等奖3个 > SPOP lucky 3 1) \\\"John\\\" 2) \\\"Sean\\\" 3) \\\"Lindy\\\" \"]},\"227\":{\"h\":\"5. zset (sorted set)\",\"t\":[\"zset有序集合，相当于在set的基础上多一个score属性，这样就可以根据score进行排序了，然后也有set的不可重复（score是可以重复）的特点\"]},\"228\":{\"h\":\"5.1 内部实现\",\"t\":[\"Zset 类型的底层数据结构是由压缩列表或跳表实现的：\",\"如果有序集合的元素个数小于 128 个，并且每个元素的值小于 64 字节时，Redis 会使用压缩列表作为 Zset 类型的底层数据结构；\",\"如果有序集合的元素不满足上面的条件，Redis 会使用跳表作为 Zset 类型的底层数据结构；\",\"在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。\"]},\"229\":{\"h\":\"5.2 常用命令\",\"t\":[\"# 往有序集合key中加入带分值元素 ZADD key score member [[score member]...] # 往有序集合key中删除元素 ZREM key member [member...] # 返回有序集合key中元素member的分值 ZSCORE key member # 返回有序集合key中元素个数 ZCARD key # 为有序集合key中元素member的分值加上increment ZINCRBY key increment member # 正序获取有序集合key从start下标到stop下标的元素 ZRANGE key start stop [WITHSCORES] # 倒序获取有序集合key从start下标到stop下标的元素 ZREVRANGE key start stop [WITHSCORES] # 返回有序集合中指定分数区间内的成员，分数由低到高排序。 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] # 返回指定成员区间内的成员，按字典正序排列, 分数必须相同。 ZRANGEBYLEX key min max [LIMIT offset count] # 返回指定成员区间内的成员，按字典倒序排列, 分数必须相同 ZREVRANGEBYLEX key max min [LIMIT offset count] \",\"Zset 运算操作（相比于 Set 类型，ZSet 类型没有支持差集运算）：\",\"# 并集计算(相同元素分值相加)，numberkeys一共多少个key，WEIGHTS每个key对应的分值乘积 ZUNIONSTORE destkey numberkeys key [key...] # 交集计算(相同元素分值相加)，numberkeys一共多少个key，WEIGHTS每个key对应的分值乘积 ZINTERSTORE destkey numberkeys key [key...] \"]},\"230\":{\"h\":\"5.3 应用场景\",\"t\":[\"排行榜\"]},\"231\":{\"h\":\"6. bitmap\",\"t\":[\"bitmap，位图，是一串连续的二进制数组，可以通过偏移量来定位元素\",\"bitmap通过计算机中的最小单位bit来进行0 1设置，可以用来表示某个元素的值或者状态\",\"因为bit占用空间非常小，所以非常适合一些数据量大并且只需要二值统计的场景。\"]},\"232\":{\"h\":\"6.1 底层实现\",\"t\":[\"bitmap底层通过string来实现。\",\"string类型会保存为二进制的字节数组，所以，Redis就把字节数组的每个bit位利用起来，用来表示一个元素的二值状态。bitmap可以看作是一个bit数组。\"]},\"233\":{\"h\":\"6.2 常用命令\",\"t\":[\"bitmap 基本操作：\",\"# 设置值，其中value只能是 0 和 1 SETBIT key offset value # 获取值 GETBIT key offset # 获取指定范围内值为 1 的个数 # start 和 end 以字节为单位 BITCOUNT key start end \",\"bitmap 运算操作：\",\"# BitMap间的运算 # operations 位移操作符，枚举值 AND 与运算 & OR 或运算 | XOR 异或 ^ NOT 取反 ~ # result 计算的结果，会存储在该key中 # key1 … keyn 参与运算的key，可以有多个，空格分割，not运算只能一个key # 当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 0。返回值是保存到 destkey 的字符串的长度（以字节byte为单位），和输入 key 中最长的字符串长度相等。 BITOP [operations] [result] [key1] [keyn…] # 返回指定key中第一次出现指定value(0/1)的位置 BITPOS [key] [value] \"]},\"234\":{\"h\":\"6.2 应用场景\",\"t\":[\"bitmap适合存储一些数据量很大，但是只需要进行二值存储的内容\",\"比如签到表。可以用0表示未签到，1表示签到，这样一个月也就30多bit，一年也就365bit。\"]},\"235\":{\"h\":\"6.2.1 签到表\",\"t\":[\"假设我们要统计 ID 100 的用户在 2022 年 6 月份的签到情况，就可以按照下面的步骤进行操作。\",\"第一步，执行下面的命令，记录该用户 6 月 3 号已签到。\",\"SETBIT uid:sign:100:202206 2 1 \",\"第二步，检查该用户 6 月 3 日是否签到。\",\"GETBIT uid:sign:100:202206 2 \",\"第三步，统计该用户在 6 月份的签到次数。\",\"BITCOUNT uid:sign:100:202206 \",\"这样，我们就知道该用户在 6 月份的签到情况了。\",\"如何统计这个月首次打卡时间呢？\",\"Redis 提供了 BITPOS key bitValue [start] [end]指令，返回数据表示 Bitmap 中第一个值为 bitValue 的 offset 位置。\",\"在默认情况下， 命令将检测整个位图， 用户可以通过可选的 start 参数和 end 参数指定要检测的范围。所以我们可以通过执行这条命令来获取 userID = 100 在 2022 年 6 月份首次打卡日期：\",\"BITPOS uid:sign:100:202206 1 \",\"需要注意的是，因为 offset 从 0 开始的，所以我们需要将返回的 value + 1 。\"]},\"236\":{\"h\":\"6.3.2 连续签到用户总数\",\"t\":[\"如何统计出这连续 7 天连续打卡用户总数呢？\",\"我们把每天的日期作为 Bitmap 的 key，userId 作为 offset，若是打卡则将 offset 位置的 bit 设置成 1。\",\"key 对应的集合的每个 bit 位的数据则是一个用户在该日期的打卡记录。\",\"一共有 7 个这样的 Bitmap，如果我们能对这 7 个 Bitmap 的对应的 bit 位做 『与』运算。同样的 UserID offset 都是一样的，当一个 userID 在 7 个 Bitmap 对应对应的 offset 位置的 bit = 1 就说明该用户 7 天连续打卡。\",\"结果保存到一个新 Bitmap 中，我们再通过 BITCOUNT 统计 bit = 1 的个数便得到了连续打卡 7 天的用户总数了。\",\"Redis 提供了 BITOP operation destkey key [key ...]这个指令用于对一个或者多个 key 的 Bitmap 进行位元操作。\",\"operation 可以是 and、OR、NOT、XOR。当 BITOP 处理不同长度的字符串时，较短的那个字符串所缺少的部分会被看作 0 。空的 key 也被看作是包含 0 的字符串序列。\",\"假设要统计 3 天连续打卡的用户数，则是将三个 bitmap 进行 AND 操作，并将结果保存到 destmap 中，接着对 destmap 执行 BITCOUNT 统计，如下命令：\",\"# 与操作 BITOP AND destmap bitmap:01 bitmap:02 bitmap:03 # 统计 bit 位 = 1 的个数 BITCOUNT destmap \",\"即使一天产生一个亿的数据，Bitmap 占用的内存也不大，大约占 12 MB 的内存（10^8/8/1024/1024），7 天的 Bitmap 的内存开销约为 84 MB。同时我们最好给 Bitmap 设置过期时间，让 Redis 删除过期的打卡数据，节省内存。\"]},\"237\":{\"h\":\"7.HyperLogLog\",\"t\":[\"Redis HyperLogLog 是 Redis 2.8.9 版本新增的数据类型，是一种用于「统计基数」的数据集合类型，基数统计就是指统计一个集合中不重复的元素个数。但要注意，HyperLogLog 是统计规则是基于概率完成的，不是非常准确，标准误算率是 0.81%。\",\"所以，简单来说 HyperLogLog 提供不精确的去重计数 。\",\"HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的内存空间总是固定的、并且是很小的。\",\"在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近2^64个不同元素的基数 ，和元素越多就越耗费内存的 Set 和 Hash 类型相比，HyperLogLog 就非常节省空间。\"]},\"238\":{\"h\":\"7.1 常见命令\",\"t\":[\"# 添加指定元素到 HyperLogLog 中 PFADD key element [element ...] # 返回给定 HyperLogLog 的基数估算值。 PFCOUNT key [key ...] # 将多个 HyperLogLog 合并为一个 HyperLogLog PFMERGE destkey sourcekey [sourcekey ...] \"]},\"239\":{\"h\":\"7.2 应用场景\",\"t\":[\"百万级网页UV计数\"]},\"240\":{\"h\":\"8. GEO 地理位置\",\"t\":[\"Redis GEO 是 Redis 3.2 版本新增的数据类型，主要用于存储地理位置信息，并对存储的信息进行操作。\",\"在日常生活中，我们越来越依赖搜索“附近的餐馆”、在打车软件上叫车，这些都离不开基于位置信息服务（Location-Based Service，LBS）的应用。LBS 应用访问的数据是和人或物关联的一组经纬度信息，而且要能查询相邻的经纬度范围，GEO 就非常适合应用在 LBS 服务的场景中。\"]},\"241\":{\"h\":\"8.1 内部实现\",\"t\":[\"GEO 本身并没有设计新的底层数据结构，而是直接使用了 Sorted Set 集合类型。\",\"GEO 类型使用 GeoHash 编码方法实现了经纬度到 Sorted Set 中元素权重分数的转换，这其中的两个关键机制就是「对二维地图做区间划分」和「对区间进行编码」。一组经纬度落在某个区间后，就用区间的编码值来表示，并把编码值作为 Sorted Set 元素的权重分数。\",\"这样一来，我们就可以把经纬度保存到 Sorted Set 中，利用 Sorted Set 提供的“按权重进行有序范围查找”的特性，实现 LBS 服务中频繁使用的“搜索附近”的需求。\"]},\"242\":{\"h\":\"8.2 常用命令\",\"t\":[\"# 存储指定的地理空间位置，可以将一个或多个经度(longitude)、纬度(latitude)、位置名称(member)添加到指定的 key 中。 GEOADD key longitude latitude member [longitude latitude member ...] # 从给定的 key 里返回所有指定名称(member)的位置（经度和纬度），不存在的返回 nil。 GEOPOS key member [member ...] # 返回两个给定位置之间的距离。 GEODIST key member1 member2 [m|km|ft|mi] # 根据用户给定的经纬度坐标来获取指定范围内的地理位置集合。 GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] \"]},\"243\":{\"h\":\"8.3 应用场景\"},\"244\":{\"h\":\"8.3.1 滴滴叫车\",\"t\":[\"这里以滴滴叫车的场景为例，介绍下具体如何使用 GEO 命令：GEOADD 和 GEORADIUS 这两个命令。\",\"假设车辆 ID 是 33，经纬度位置是（116.034579，39.030452），我们可以用一个 GEO 集合保存所有车辆的经纬度，集合 key 是 cars:locations。\",\"执行下面的这个命令，就可以把 ID 号为 33 的车辆的当前经纬度位置存入 GEO 集合中：\",\"GEOADD cars:locations 116.034579 39.030452 33 \",\"当用户想要寻找自己附近的网约车时，LBS 应用就可以使用 GEORADIUS 命令。\",\"例如，LBS 应用执行下面的命令时，Redis 会根据输入的用户的经纬度信息（116.054579，39.030452 ），查找以这个经纬度为中心的 5 公里内的车辆信息，并返回给 LBS 应用。\",\"GEORADIUS cars:locations 116.054579 39.030452 5 km ASC COUNT 10 \"]},\"245\":{\"h\":\"5.多数元素\",\"t\":[\"这是一道数组题，不过我愿意分类为哈希表题。\",\"给定一个大小为 n 的数组 nums ，返回其中的多数元素。多数元素是指在数组中出现次数 大于⌊ n/2 ⌋ 的元素。\",\"你可以假设数组是非空的，并且给定的数组总是存在多数元素。\",\"示例 1：\",\"输入：nums = [3,2,3] 输出：3\",\"示例 2：\",\"输入：nums = [2,2,1,1,1,2,2] 输出：2 \",\"提示：* n == nums.length\",\"1 <= n <= 5 * 10<sup>4</sup>\",\"-10<sup>9</sup> <= nums[i] <= 10<sup>9</sup>\",\"进阶： 尝试设计时间复杂度为 O(n)、空间复杂度为 O(1) 的算法解决此问题。\"]},\"246\":{\"h\":\"解答\",\"t\":[\"这道题想要做出来并不复杂，只需对其排序，然后取length/2位置即可，但是如果尝试使用O(n)时间复杂度，O(1)空间复杂度来解答，则需要考虑到哈希表。\",\"哈希表操作并不难，只需要考虑到即可。\"]},\"247\":{\"h\":\"3. 删除有序数组中的重复项\",\"t\":[\"该题来自力扣26题，注意是双指针题就可以了。\",\"给你一个 非严格递增排列 的数组 nums ，请你** 原地** 删除重复出现的元素，使每个元素 只出现一次 ，返回删除后数组的新长度。元素的 相对顺序 应该保持 一致 。然后返回 nums 中唯一元素的个数。\",\"考虑 nums 的唯一元素的数量为 k ，你需要做以下事情确保你的题解可以被通过：\",\"更改数组 nums ，使 nums 的前 k 个元素包含唯一元素，并按照它们最初在 nums 中出现的顺序排列。nums 的其余元素与 nums 的大小不重要。\",\"返回 k 。\",\"判题标准:\",\"系统会用下面的代码来测试你的题解:\",\"int[] nums = [...]; // 输入数组 int[] expectedNums = [...]; // 长度正确的期望答案 int k = removeDuplicates(nums); // 调用 assert k == expectedNums.length; for (int i = 0; i < k; i++) { assert nums[i] == expectedNums[i]; } \",\"如果所有断言都通过，那么您的题解将被 通过 。\",\"示例 1：\",\"输入：nums = [1,1,2] 输出：2, nums = [1,2,_] 解释：函数应该返回新的长度 2 ，并且原数组 nums 的前两个元素被修改为 1, 2 。不需要考虑数组中超出新长度后面的元素。 \",\"示例 2：\",\"输入：nums = [0,0,1,1,1,2,2,3,3,4] 输出：5, nums = [0,1,2,3,4] 解释：函数应该返回新的长度 5 ， 并且原数组 nums 的前五个元素被修改为 0, 1, 2, 3, 4 。不需要考虑数组中超出新长度后面的元素。 \",\"提示：\",\"1 <= nums.length <= 3 * 10<sup>4</sup>\",\"-10<sup>4</sup> <= nums[i] <= 10<sup>4</sup>\",\"nums 已按 非严格递增 排列\"]},\"248\":{\"h\":\"2. 移除元素\",\"t\":[\"该题来自力扣27题\",\"给你一个数组 nums 和一个值 val，你需要 原地 移除所有数值等于 val 的元素，并返回移除后数组的新长度。\",\"不要使用额外的数组空间，你必须仅使用 O(1) 额外空间并 原地 修改输入数组 。\",\"元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。\",\"说明:\",\"为什么返回数值是整数，但输出的答案是数组呢?\",\"请注意，输入数组是以 「引用」 方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。\",\"你可以想象内部操作如下:\",\"// nums 是以“引用”方式传递的。也就是说，不对实参作任何拷贝 int len = removeElement(nums, val); // 在函数里修改输入数组对于调用者是可见的。 // 根据你的函数返回的长度, 它会打印出数组中 该长度范围内 的所有元素。 for (int i = 0; i < len; i++) { print(nums[i]); } \",\"示例 1：\",\"输入：nums = [3,2,2,3], val = 3 输出：2, nums = [2,2] 解释：函数应该返回新的长度 2, 并且 nums中的前两个元素均为 2。你不需要考虑数组中超出新长度后面的元素。例如，函数返回的新长度为 2 ，而 nums = [2,2,3,3] 或 nums = [2,2,0,0]，也会被视作正确答案。 \",\"示例 2：\",\"输入：nums = [0,1,2,2,3,0,4,2], val = 2 输出：5, nums = [0,1,3,0,4] 解释：函数应该返回新的长度 5, 并且 nums 中的前五个元素为 0, 1, 3, 0, 4。注意这五个元素可为任意顺序。你不需要考虑数组中超出新长度后面的元素。 \",\"提示：\",\"0 <= nums.length <= 100\",\"0 <= nums[i] <= 50\",\"0 <= val <= 100\",\"要注意这是一道双指针题目，简单题没啥说的\"]},\"249\":{\"h\":\"4.删除有序数组中的重复项 II\",\"t\":[\"这道题来自力扣80题，这道题其实做完力扣26题后并不难，就是一个双指针问题，但是如果从头开始看，可能觉得稍微困难。\",\"需要具备双指针的思想，逻辑和26题完全一样。\",\"给你一个有序数组 nums ，请你** 原地** 删除重复出现的元素，使得出现次数超过两次的元素只出现两次 ，返回删除后数组的新长度。\",\"不要使用额外的数组空间，你必须在 **原地 修改输入数组 **并在使用 O(1) 额外空间的条件下完成。\",\"说明：\",\"为什么返回数值是整数，但输出的答案是数组呢？\",\"请注意，输入数组是以 「引用」 方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。\",\"你可以想象内部操作如下:\",\"// nums 是以“引用”方式传递的。也就是说，不对实参做任何拷贝 int len = removeDuplicates(nums); // 在函数里修改输入数组对于调用者是可见的。 // 根据你的函数返回的长度, 它会打印出数组中 该长度范围内 的所有元素。 for (int i = 0; i < len; i++) { print(nums[i]); } \",\"示例 1：\",\"输入：nums = [1,1,1,2,2,3] 输出：5, nums = [1,1,2,2,3] 解释：函数应返回新长度 length = 5, 并且原数组的前五个元素被修改为 1, 1, 2, 2, 3。 不需要考虑数组中超出新长度后面的元素。 \",\"示例 2：\",\"输入：nums = [0,0,1,1,1,1,2,3,3] 输出：7, nums = [0,0,1,1,2,3,3] 解释：函数应返回新长度 length = 7, 并且原数组的前五个元素被修改为 0, 0, 1, 1, 2, 3, 3。不需要考虑数组中超出新长度后面的元素。 \",\"提示：\",\"1 <= nums.length <= 3 * 10<sup>4</sup>\",\"-10<sup>4</sup> <= nums[i] <= 10<sup>4</sup>\",\"nums 已按升序排列\"]},\"250\":{\"h\":\"1. 合并两个有序数组（System#arraycopy()介绍)\",\"t\":[\"该题来自力扣第88题。\"]},\"251\":{\"h\":\"题目描述\",\"t\":[\"给你两个按 非递减顺序 排列的整数数组 nums1 和 nums2，另有两个整数 m 和 n ，分别表示 nums1 和 nums2 中的元素数目。\",\"请你 合并nums2 到 nums1 中，使合并后的数组同样按 非递减顺序 排列。\",\"注意： 最终，合并后数组不应由函数返回，而是存储在数组 nums1 中。为了应对这种情况，nums1 的初始长度为 m + n，其中前 m 个元素表示应合并的元素，后 n 个元素为 0 ，应忽略。nums2 的长度为 n 。\",\"示例 1：\",\"输入：nums1 = [1,2,3,0,0,0], m = 3, nums2 = [2,5,6], n = 3 输出：[1,2,2,3,5,6] 解释：需要合并 [1,2,3] 和 [2,5,6] 。 合并结果是 [1,2,2,3,5,6] ，其中斜体加粗标注的为 nums1 中的元素。 \",\"示例 2：\",\"输入：nums1 = [1], m = 1, nums2 = [], n = 0 输出：[1] 解释：需要合并 [1] 和 [] 。 合并结果是 [1] 。 \",\"示例 3：\",\"输入：nums1 = [0], m = 0, nums2 = [1], n = 1 输出：[1] 解释：需要合并的数组是 [] 和 [1] 。 合并结果是 [1] 。 注意，因为 m = 0 ，所以 nums1 中没有元素。nums1 中仅存的 0 仅仅是为了确保合并结果可以顺利存放到 nums1 中。 \",\"提示：\",\"nums1.length == m + n\",\"nums2.length == n\",\"0 <= m, n <= 200\",\"1 <= m + n <= 200\",\"-10<sup>9</sup> <= nums1[i], nums2[j] <= 10<sup>9</sup>\",\"进阶： 你可以设计实现一个时间复杂度为 O(m + n) 的算法解决此问题吗？\"]},\"252\":{\"h\":\"解答\",\"t\":[\"这是一道比较简单的双指针问题。需要注意的是边界问题处理。比如：当nums1处理完之后，如果nums2中仍然还有数据没有处理怎么办？\",\"此外，为了减少处理时间耗费，我们可以针对特殊情况--nums1的实际数组长度为0时，只需要将nums2复制到nums1即可，而无需逐一比较。为了实现快速复制，我们可以调用System#arraycopy()方法。\"]},\"253\":{\"h\":\"关于System#arraycopy()的介绍\",\"t\":[\"该方法位于java.lang包下的System类中，该方法定义如下：\",\"@IntrinsicCandidate public static native void arraycopy(Object src, int srcPos, Object dest, int destPos, int length); \",\"可以看到这是一个native方法，效率自然会高一些。不过这个native方法有些特殊，我们稍后介绍。这个方法共接收5个参数：\",\"src：源数组\",\"srcPos：从源数组的哪个位置开始复制\",\"dest：要复制到的数组\",\"destPos：从要复制的数组哪里开始复制\",\"length：复制的长度\",\"使用方法比较简单，比如，现在有两个数组int[] nums1 = {1, 2, 3, 4}, int[] nums2 = {0, 0, 0, 0}。我们现在想要将nums1的内容复制到nums2中，只需要调用 System.arraycopy(nums1, 0, nums2, 0, nums1.length) 即可。\",\"可以看到该方法有一个注解@IntrinsicCandidate（JDK17)，JDK介绍其用于HotSpot VM，这个注解可以标记（但是不一定）这个方法属于HotSpot VM内部，而属于HotSpot VM内部的方法，HotSpot VM会对其进行一些优化，比如手动编写汇编或者手动编写编译器中间语言来替换该方法的实现。\",\"所以，虽然这里被声明为native方法，但是它跟JDK中其他的native方法实现地方不同。这个注解标记的方法会在JVM内部实现，而其他的会在JDK库中实现。在调用方面，由于直接调用JVM内部实现，不走常规的JNI lookup，所以也省了一些开销。\",\"该算法题的实现如下：\",\"class Solution { public void merge(int[] nums1, int m, int[] nums2, int n) { if (m == 0) { System.arraycopy(nums2, 0, nums1, 0, n); return; } int i = m + n - 1; while (m > 0 && n > 0) { if (nums1[m - 1] > nums2[n - 1]) { nums1[i] = nums1[m - 1]; --m; } else { nums1[i] = nums2[n - 1]; --n; } --i; } while (n > 0) { nums1[i--] = nums2[--n]; } } } \"]},\"254\":{\"h\":\"Android\"},\"255\":{\"h\":\"\"},\"256\":{\"h\":\"Java\"},\"257\":{\"h\":\"Llm\"},\"258\":{\"h\":\"Network\"},\"259\":{\"h\":\"Offer\"},\"260\":{\"h\":\"Redis\"},\"261\":{\"h\":\"Leetcode 150 Shuzu\"},\"262\":{\"h\":\"Leetcode\"}},\"dirtCount\":0,\"index\":[[\"源数组\",{\"1\":{\"253\":1}}],[\"源端口号\",{\"1\":{\"183\":1}}],[\"效率自然会高一些\",{\"1\":{\"253\":1}}],[\"效果自然也会好一些\",{\"1\":{\"32\":1}}],[\"效果比较好\",{\"1\":{\"22\":1}}],[\"关于system\",{\"0\":{\"253\":1}}],[\"仅仅是为了确保合并结果可以顺利存放到\",{\"1\":{\"251\":1}}],[\"仅精调0\",{\"1\":{\"32\":1}}],[\"题目描述\",{\"0\":{\"251\":1}}],[\"合并结果是\",{\"1\":{\"251\":3}}],[\"合并后数组不应由函数返回\",{\"1\":{\"251\":1}}],[\"合并nums2\",{\"1\":{\"251\":1}}],[\"合并两个有序数组\",{\"0\":{\"250\":1}}],[\"合并为一个\",{\"1\":{\"238\":1}}],[\"逻辑和26题完全一样\",{\"1\":{\"249\":1}}],[\"修改输入数组\",{\"1\":{\"248\":1,\"249\":1}}],[\"修改后仍然有较多问题\",{\"1\":{\"3\":1}}],[\"移除所有数值等于\",{\"1\":{\"248\":1}}],[\"移除元素\",{\"0\":{\"248\":1}}],[\"移除了reparamerization加速训练方式\",{\"1\":{\"32\":1}}],[\"排列的整数数组\",{\"1\":{\"251\":1}}],[\"排列\",{\"1\":{\"247\":1,\"251\":1}}],[\"排行榜\",{\"1\":{\"230\":1}}],[\"已按升序排列\",{\"1\":{\"249\":1}}],[\"已按\",{\"1\":{\"247\":1}}],[\"已经是广为应用的做法\",{\"1\":{\"18\":1}}],[\"长度正确的期望答案\",{\"1\":{\"247\":1}}],[\"长期存活的对象将会进入老年代\",{\"0\":{\"91\":1}}],[\"判题标准\",{\"1\":{\"247\":1}}],[\"判断member元素是否存在于集合key中\",{\"1\":{\"222\":1}}],[\"判断该对象是否有必要执行finalize方法\",{\"1\":{\"98\":1}}],[\"考虑\",{\"1\":{\"247\":1}}],[\"请你\",{\"1\":{\"247\":1,\"249\":1,\"251\":1}}],[\"请注意\",{\"1\":{\"50\":1,\"248\":1,\"249\":1}}],[\"尝试设计时间复杂度为\",{\"1\":{\"245\":1}}],[\"进阶\",{\"1\":{\"245\":1,\"251\":1}}],[\"进行\",{\"1\":{\"236\":1}}],[\"进行位元操作\",{\"1\":{\"236\":1}}],[\"进行垃圾回收不会把它回收掉\",{\"1\":{\"105\":1}}],[\"进行垃圾回收的时候\",{\"1\":{\"104\":1}}],[\"进行了比较\",{\"1\":{\"54\":1}}],[\"示例\",{\"1\":{\"245\":2,\"247\":2,\"248\":2,\"249\":2,\"251\":3}}],[\"滴滴叫车\",{\"0\":{\"244\":1}}],[\"经纬度位置是\",{\"1\":{\"244\":1}}],[\"经度和纬度\",{\"1\":{\"242\":1}}],[\"经过一系列的key来获取value\",{\"1\":{\"17\":1}}],[\"里返回所有指定名称\",{\"1\":{\"242\":1}}],[\"里面\",{\"1\":{\"237\":1}}],[\"纬度\",{\"1\":{\"242\":1}}],[\"存储指定的地理空间位置\",{\"1\":{\"242\":1}}],[\"存储一个哈希表key的键值\",{\"1\":{\"218\":1}}],[\"搜索附近\",{\"1\":{\"241\":1}}],[\"利用\",{\"1\":{\"241\":1}}],[\"利用这些信息来明确教导奖励模型为具有更多差异的世代分配更多不一致的分数可能是有用的\",{\"1\":{\"49\":1}}],[\"附近的餐馆\",{\"1\":{\"240\":1}}],[\"百万级网页uv计数\",{\"1\":{\"239\":1}}],[\"百亿\",{\"1\":{\"14\":1}}],[\"添加到指定的\",{\"1\":{\"242\":1}}],[\"添加指定元素到\",{\"1\":{\"238\":1}}],[\"添加监听\",{\"1\":{\"3\":1}}],[\"键只需要花费\",{\"1\":{\"237\":1}}],[\"键是强引用的\",{\"1\":{\"105\":1}}],[\"节省内存\",{\"1\":{\"236\":1}}],[\"节省了70\",{\"1\":{\"75\":1}}],[\"占用的内存也不大\",{\"1\":{\"236\":1}}],[\"占用内存大\",{\"1\":{\"111\":1}}],[\"执行下面的这个命令\",{\"1\":{\"244\":1}}],[\"执行下面的命令\",{\"1\":{\"235\":1}}],[\"执行\",{\"1\":{\"236\":1}}],[\"操作\",{\"1\":{\"236\":1}}],[\"天的\",{\"1\":{\"236\":1}}],[\"天的用户总数了\",{\"1\":{\"236\":1}}],[\"天连续打卡的用户数\",{\"1\":{\"236\":1}}],[\"天连续打卡\",{\"1\":{\"236\":1}}],[\"天连续打卡用户总数呢\",{\"1\":{\"236\":1}}],[\"统计基数\",{\"1\":{\"237\":1}}],[\"统计\",{\"1\":{\"236\":3}}],[\"统计该用户在\",{\"1\":{\"235\":1}}],[\"运算\",{\"1\":{\"236\":1}}],[\"运算操作\",{\"1\":{\"229\":1,\"233\":1}}],[\"连续签到用户总数\",{\"0\":{\"236\":1}}],[\"日是否签到\",{\"1\":{\"235\":1}}],[\"日志\",{\"1\":{\"66\":1}}],[\"检查该用户\",{\"1\":{\"235\":1}}],[\"检查点和扩展机器学习模型\",{\"1\":{\"72\":1}}],[\"月份首次打卡日期\",{\"1\":{\"235\":1}}],[\"月份的签到次数\",{\"1\":{\"235\":1}}],[\"月份的签到情况了\",{\"1\":{\"235\":1}}],[\"月份的签到情况\",{\"1\":{\"235\":1}}],[\"月\",{\"1\":{\"235\":2}}],[\"记录该用户\",{\"1\":{\"235\":1}}],[\"年\",{\"1\":{\"235\":2}}],[\"年龄都会增加一岁\",{\"1\":{\"91\":1}}],[\"年龄为1\",{\"1\":{\"91\":1}}],[\"年龄为0\",{\"1\":{\"91\":1}}],[\"签到表\",{\"0\":{\"235\":1}}],[\"较短的那个字符串所缺少的部分会被看作\",{\"1\":{\"233\":1,\"236\":1}}],[\"处理不同长度的字符串时\",{\"1\":{\"233\":1,\"236\":1}}],[\"空间复杂度来解答\",{\"1\":{\"246\":1}}],[\"空间复杂度为\",{\"1\":{\"245\":1}}],[\"空间分配担保\",{\"0\":{\"93\":1},\"1\":{\"93\":1}}],[\"空的\",{\"1\":{\"236\":1}}],[\"空格分割\",{\"1\":{\"233\":1}}],[\"取反\",{\"1\":{\"233\":1}}],[\"取得了喜人的成绩\",{\"1\":{\"22\":1}}],[\"^\",{\"1\":{\"233\":1}}],[\"异或\",{\"1\":{\"233\":1}}],[\"|\",{\"1\":{\"233\":1}}],[\"枚举值\",{\"1\":{\"233\":1}}],[\"底层实现\",{\"0\":{\"232\":1}}],[\"倒序获取有序集合key从start下标到stop下标的元素\",{\"1\":{\"229\":1}}],[\"命令将检测整个位图\",{\"1\":{\"235\":1}}],[\"命令\",{\"1\":{\"226\":2,\"244\":2}}],[\"命令把消息插入\",{\"1\":{\"213\":1}}],[\"抽取三等奖3个\",{\"1\":{\"226\":1}}],[\"抽取二等奖2个\",{\"1\":{\"226\":1}}],[\"抽取一等奖1个\",{\"1\":{\"226\":1}}],[\"抽取\",{\"1\":{\"226\":3}}],[\"抽奖\",{\"0\":{\"226\":1}}],[\"抽象类是否有子类实现了等\",{\"1\":{\"138\":1}}],[\"点赞\",{\"0\":{\"224\":1}}],[\"做聚合计算\",{\"1\":{\"223\":1}}],[\"错集和并集等\",{\"1\":{\"223\":1}}],[\"集合中\",{\"1\":{\"244\":1}}],[\"集合\",{\"1\":{\"244\":1}}],[\"集合保存所有车辆的经纬度\",{\"1\":{\"244\":1}}],[\"集合类型\",{\"1\":{\"241\":1}}],[\"集合的主要几个特性\",{\"1\":{\"223\":1}}],[\"集成\",{\"1\":{\"66\":1}}],[\"差集\",{\"1\":{\"223\":1}}],[\"差集运算\",{\"1\":{\"222\":1}}],[\"差别很大吗\",{\"1\":{\"51\":1}}],[\"往有序集合key中删除元素\",{\"1\":{\"229\":1}}],[\"往有序集合key中加入带分值元素\",{\"1\":{\"229\":1}}],[\"往集合key中存入元素\",{\"1\":{\"222\":1}}],[\"往往小的企业或者个人应用的\",{\"1\":{\"30\":1}}],[\"往往需要经过下游数据进行微调后才可更好的发挥它的能力\",{\"1\":{\"29\":1}}],[\"商品数量为value\",{\"1\":{\"219\":1}}],[\"商品id为field\",{\"1\":{\"219\":1}}],[\"商用\",{\"1\":{\"60\":1,\"61\":1}}],[\"购物车\",{\"1\":{\"219\":1}}],[\"返回\",{\"1\":{\"247\":1}}],[\"返回删除后数组的新长度\",{\"1\":{\"247\":1,\"249\":1}}],[\"返回其中的多数元素\",{\"1\":{\"245\":1}}],[\"返回两个给定位置之间的距离\",{\"1\":{\"242\":1}}],[\"返回给定\",{\"1\":{\"238\":1}}],[\"返回数据表示\",{\"1\":{\"235\":1}}],[\"返回值是保存到\",{\"1\":{\"233\":1}}],[\"返回指定key中第一次出现指定value\",{\"1\":{\"233\":1}}],[\"返回指定成员区间内的成员\",{\"1\":{\"229\":2}}],[\"返回指定区间内的元素\",{\"1\":{\"210\":1}}],[\"返回有序集合中指定分数区间内的成员\",{\"1\":{\"229\":1}}],[\"返回有序集合key中元素个数\",{\"1\":{\"229\":1}}],[\"返回有序集合key中元素member的分值\",{\"1\":{\"229\":1}}],[\"返回哈希表key中所有的键值\",{\"1\":{\"218\":1}}],[\"返回哈希表key中field的数量\",{\"1\":{\"218\":1}}],[\"批量获取哈希表key中多个field键值\",{\"1\":{\"218\":1}}],[\"批量模型检查\",{\"1\":{\"70\":1}}],[\"压缩列表数据结构已经废弃了\",{\"1\":{\"228\":1}}],[\"压缩列表被弃用\",{\"1\":{\"217\":1}}],[\"压缩包等二进制数据\",{\"1\":{\"204\":1}}],[\"库存量为\",{\"1\":{\"213\":1}}],[\"时导致主库被阻塞\",{\"1\":{\"223\":1}}],[\"时\",{\"1\":{\"213\":1}}],[\"时间复杂度\",{\"1\":{\"246\":1}}],[\"时间\",{\"1\":{\"151\":1}}],[\"时间的查找\",{\"1\":{\"150\":1}}],[\"生成之后\",{\"1\":{\"213\":1}}],[\"生产级机器学习\",{\"1\":{\"56\":1}}],[\"号为\",{\"1\":{\"244\":1}}],[\"号已签到\",{\"1\":{\"235\":1}}],[\"号\",{\"1\":{\"213\":1}}],[\"个元素为\",{\"1\":{\"251\":1}}],[\"个元素表示应合并的元素\",{\"1\":{\"251\":1}}],[\"个元素包含唯一元素\",{\"1\":{\"247\":1}}],[\"个这样的\",{\"1\":{\"236\":1}}],[\"个\",{\"1\":{\"228\":1,\"236\":2}}],[\"个三等奖\",{\"1\":{\"226\":1}}],[\"个二等奖\",{\"1\":{\"226\":1}}],[\"个一等奖\",{\"1\":{\"226\":1}}],[\"个方面的要求\",{\"1\":{\"213\":1}}],[\"个人猜测\",{\"1\":{\"27\":1}}],[\"消费者程序就不再进行处理了\",{\"1\":{\"213\":1}}],[\"消费者程序就可以对比收到的消息\",{\"1\":{\"213\":1}}],[\"消费者要记录已经处理过的消息的\",{\"1\":{\"213\":1}}],[\"消费者要实现重复消息的判断\",{\"1\":{\"213\":1}}],[\"消息保序\",{\"1\":{\"211\":1}}],[\"消息队列在存取消息时\",{\"1\":{\"211\":1}}],[\"消息队列是list的常见应用场景\",{\"1\":{\"211\":1}}],[\"保证消息的可靠性\",{\"1\":{\"211\":1}}],[\"保证了多线程操作的有序性\",{\"1\":{\"129\":1}}],[\"常见命令\",{\"0\":{\"238\":1}}],[\"常用命令\",{\"0\":{\"210\":1,\"218\":1,\"222\":1,\"229\":1,\"233\":1,\"242\":1}}],[\"常量池在方法区\",{\"1\":{\"121\":1}}],[\"列表中数据长度均小于64字节\",{\"1\":{\"209\":1}}],[\"列表最大长度位2^32\",{\"1\":{\"208\":1}}],[\"默认\",{\"1\":{\"209\":2}}],[\"按权重进行有序范围查找\",{\"1\":{\"241\":1}}],[\"按字典倒序排列\",{\"1\":{\"229\":1}}],[\"按字典正序排列\",{\"1\":{\"229\":1}}],[\"按插入顺序进行排序\",{\"1\":{\"208\":1}}],[\"按他的来\",{\"1\":{\"49\":1}}],[\"删除重复出现的元素\",{\"1\":{\"247\":1,\"249\":1}}],[\"删除有序数组中的重复项\",{\"0\":{\"247\":1,\"249\":1}}],[\"删除过期的打卡数据\",{\"1\":{\"236\":1}}],[\"删除哈希表key中的field键值\",{\"1\":{\"218\":1}}],[\"删除不常用的索引\",{\"0\":{\"166\":1}}],[\"删掉这个key\",{\"1\":{\"207\":1}}],[\"插入失败作为加锁失败\",{\"1\":{\"207\":1}}],[\"计数\",{\"1\":{\"207\":1}}],[\"计算基数所需的内存空间总是固定的\",{\"1\":{\"237\":1}}],[\"计算的结果\",{\"1\":{\"233\":1}}],[\"计算操作也是同理\",{\"1\":{\"175\":1}}],[\"计算操作\",{\"0\":{\"175\":1}}],[\"计算attention\",{\"1\":{\"17\":1}}],[\"释放embstr编码的字符串同样也只需要调用一次内存释放函数\",{\"1\":{\"206\":1}}],[\"原地\",{\"1\":{\"247\":1,\"248\":2,\"249\":2}}],[\"原因\",{\"1\":{\"204\":2}}],[\"原子性是指某一个操作是原子的\",{\"1\":{\"130\":1}}],[\"原子性\",{\"0\":{\"130\":1},\"1\":{\"127\":1,\"191\":1}}],[\"位\",{\"1\":{\"236\":1}}],[\"位做\",{\"1\":{\"236\":1}}],[\"位的数据则是一个用户在该日期的打卡记录\",{\"1\":{\"236\":1}}],[\"位置名称\",{\"1\":{\"242\":1}}],[\"位置的\",{\"1\":{\"236\":2}}],[\"位置\",{\"1\":{\"235\":1}}],[\"位置编码采用了预训练的方式训练而得\",{\"1\":{\"28\":1}}],[\"位置编码采用了三角函数计算的方式得到\",{\"1\":{\"28\":1}}],[\"位置编码可以通过训练得到\",{\"1\":{\"18\":1}}],[\"位置编码\",{\"1\":{\"18\":2}}],[\"位移操作符\",{\"1\":{\"233\":1}}],[\"位图\",{\"1\":{\"201\":1,\"231\":1}}],[\"地理位置\",{\"0\":{\"240\":1},\"1\":{\"201\":1}}],[\"追问\",{\"1\":{\"198\":1}}],[\"到\",{\"1\":{\"251\":1}}],[\"到时key会删除\",{\"1\":{\"198\":1}}],[\"到此\",{\"1\":{\"185\":1}}],[\"恢复速度慢\",{\"1\":{\"196\":1}}],[\"恢复训练循环的其余部分来缓解这种情况\",{\"1\":{\"50\":1}}],[\"性能好\",{\"1\":{\"196\":1}}],[\"性能监控\",{\"0\":{\"59\":1,\"72\":1},\"1\":{\"59\":1,\"72\":1}}],[\"容灾性好\",{\"1\":{\"196\":1}}],[\"缓存对象\",{\"1\":{\"207\":1,\"219\":1}}],[\"缓存中间件有redis和memcached等\",{\"1\":{\"192\":1}}],[\"缓存中间件有什么\",{\"0\":{\"192\":1}}],[\"缓存和消息中间件\",{\"1\":{\"190\":1}}],[\"特征丰富\",{\"1\":{\"191\":1}}],[\"特别地\",{\"1\":{\"79\":1}}],[\"特别是对于长序列\",{\"1\":{\"77\":1}}],[\"特别是\",{\"1\":{\"54\":1}}],[\"特别是gpt\",{\"1\":{\"23\":1}}],[\"特别是微调的能力\",{\"1\":{\"19\":1}}],[\"读写速度都非常快\",{\"1\":{\"191\":1}}],[\"很容易让传统的数据库崩掉\",{\"1\":{\"191\":1}}],[\"很多下游任务\",{\"1\":{\"22\":1}}],[\"首页的访问高峰\",{\"1\":{\"191\":1}}],[\"首先暂停工作线程\",{\"1\":{\"116\":1}}],[\"首先就是要收集大量的高质量sft数据\",{\"1\":{\"46\":1}}],[\"首先\",{\"1\":{\"17\":1,\"18\":1,\"75\":1,\"134\":1}}],[\"首先通过android\",{\"1\":{\"1\":1}}],[\"什么是redis\",{\"0\":{\"190\":1}}],[\"什么是masked\",{\"1\":{\"18\":1}}],[\"报文fin标志位设置为j\",{\"1\":{\"187\":1}}],[\"报文fin标志位设置为i\",{\"1\":{\"187\":1}}],[\"报告\",{\"1\":{\"70\":1}}],[\"接着对\",{\"1\":{\"236\":1}}],[\"接收是正常的\",{\"1\":{\"185\":1}}],[\"接下来就是红黑树\",{\"1\":{\"153\":1}}],[\"接下来\",{\"1\":{\"77\":1}}],[\"接下来可以使用最新的rlhf模型对这些合成数据进行采样\",{\"1\":{\"51\":1}}],[\"接下来我们来介绍bert中另外一个部分\",{\"1\":{\"22\":1}}],[\"客户端一直都没有收到这条报文\",{\"1\":{\"188\":1}}],[\"客户端不再进行第四次挥手会发生什么\",{\"1\":{\"188\":1}}],[\"客户端收到服务端的第三次挥手时候\",{\"1\":{\"188\":1}}],[\"客户端收到服务端报文\",{\"1\":{\"187\":1}}],[\"客户端收到服务端消息\",{\"1\":{\"184\":1}}],[\"客户端发送挥手报文\",{\"1\":{\"187\":1}}],[\"客户端和服务端都已经确认自己发送和接收都是正常的了\",{\"1\":{\"185\":1}}],[\"客户端告诉自己收到了服务端的消息\",{\"1\":{\"185\":1}}],[\"客户端告诉服务端要连接\",{\"1\":{\"185\":1}}],[\"客户端可以确认自己发送是正常的\",{\"1\":{\"185\":1}}],[\"客户端验证了收到的ack确实是i+1\",{\"1\":{\"185\":1}}],[\"客户端将syn标志位设为1\",{\"1\":{\"184\":1}}],[\"服务中频繁使用的\",{\"1\":{\"241\":1}}],[\"服务的场景中\",{\"1\":{\"240\":1}}],[\"服务端发送完之后就会断开了\",{\"1\":{\"188\":1}}],[\"服务端发送挥手报文\",{\"1\":{\"187\":1}}],[\"服务端到客户端也已经不再进行通信了\",{\"1\":{\"187\":1}}],[\"服务端收到客户端挥手报文\",{\"1\":{\"187\":1}}],[\"服务端收到后\",{\"1\":{\"184\":1}}],[\"服务端还无法确认自己的发送是正常的\",{\"1\":{\"185\":1}}],[\"服务端告诉客户端收到消息了\",{\"1\":{\"185\":1}}],[\"服务端接收到消息后\",{\"1\":{\"185\":1}}],[\"服务端接收到客户端的消息后\",{\"1\":{\"184\":1}}],[\"服务和监控提供了完整的解决方案\",{\"1\":{\"68\":1}}],[\"直接走的全表扫描\",{\"1\":{\"178\":1}}],[\"直到接收到为止\",{\"1\":{\"188\":1}}],[\"直到找到右范围\",{\"1\":{\"154\":1}}],[\"直到高度之差满足条件\",{\"1\":{\"152\":1}}],[\"直到到达了某个年龄\",{\"1\":{\"91\":1}}],[\"失效的原因是length\",{\"1\":{\"174\":1}}],[\"函数应返回新长度\",{\"1\":{\"249\":2}}],[\"函数应该返回新的长度\",{\"1\":{\"247\":2,\"248\":2}}],[\"函数返回的新长度为\",{\"1\":{\"248\":1}}],[\"函数本身也不会导致索引失效\",{\"1\":{\"174\":1}}],[\"函数\",{\"0\":{\"174\":1}}],[\"浪费网络资源\",{\"1\":{\"173\":1}}],[\"除非where范围过大\",{\"1\":{\"173\":1}}],[\"除了主键索引都是二级索引\",{\"1\":{\"155\":1}}],[\"除了给参与者一个被迫的选择之外\",{\"1\":{\"48\":1}}],[\"除了基础的权限限制\",{\"1\":{\"3\":1}}],[\"见另一篇\",{\"1\":{\"169\":1}}],[\"查询速度更快\",{\"1\":{\"168\":1}}],[\"查找以这个经纬度为中心的\",{\"1\":{\"244\":1}}],[\"查找出来的字段可能没用\",{\"1\":{\"173\":1}}],[\"查找需要进行回表操作\",{\"1\":{\"158\":1}}],[\"查找非常快\",{\"1\":{\"157\":1}}],[\"查找更稳定\",{\"1\":{\"154\":1}}],[\"频繁更新的列不要作为索引\",{\"1\":{\"164\":1}}],[\"频繁作为连接字段的列作为索引\",{\"1\":{\"164\":1}}],[\"频繁作为条件查询的列作为索引\",{\"1\":{\"164\":1}}],[\"频繁查找的列作为索引\",{\"1\":{\"164\":1}}],[\"感觉比较重要\",{\"0\":{\"163\":1}}],[\"<\",{\"1\":{\"247\":1,\"248\":1,\"249\":1}}],[\"<=\",{\"1\":{\"161\":1,\"245\":4,\"247\":4,\"248\":6,\"249\":4,\"251\":6}}],[\"<都会\",{\"1\":{\"161\":1}}],[\"过滤什么时候会停止\",{\"1\":{\"161\":1}}],[\"先过滤掉一些不符合的数据再进行下面的匹配\",{\"1\":{\"161\":1}}],[\"先通过工作内存\",{\"1\":{\"131\":1}}],[\"忘了\",{\"1\":{\"161\":1}}],[\"忘记了\",{\"1\":{\"157\":1}}],[\"覆盖索引和联合索引\",{\"0\":{\"160\":1}}],[\"非递减顺序\",{\"1\":{\"251\":2}}],[\"非严格递增\",{\"1\":{\"247\":1}}],[\"非严格递增排列\",{\"1\":{\"247\":1}}],[\"非聚簇索引一定会回表查询吗\",{\"0\":{\"159\":1}}],[\"非聚簇索引就是索引中的data存放的不是真正的数据\",{\"1\":{\"158\":1}}],[\"非聚簇索引\",{\"0\":{\"158\":1}}],[\"非常详细地介绍了llama\",{\"1\":{\"36\":1}}],[\"耗时较大\",{\"1\":{\"157\":1}}],[\"耗费下推理\",{\"1\":{\"74\":1}}],[\"依赖有序数据\",{\"1\":{\"158\":1}}],[\"依赖于有序数据\",{\"1\":{\"157\":1}}],[\"依赖关系\",{\"1\":{\"63\":1}}],[\"聚簇查找不需要进行回表查找\",{\"1\":{\"157\":1}}],[\"聚簇索引\",{\"0\":{\"157\":1}}],[\"聚簇索引和非聚簇索引\",{\"0\":{\"156\":1}}],[\"唯一索引\",{\"1\":{\"155\":1}}],[\"普通索引\",{\"1\":{\"155\":1}}],[\"再根据主键索引查找到数据\",{\"1\":{\"155\":1}}],[\"再把输出合并起来\",{\"1\":{\"18\":1}}],[\"范围查找更快捷\",{\"1\":{\"154\":1}}],[\"叶子节点是黑色的空节点\",{\"1\":{\"153\":1}}],[\"根节点一定是黑色的\",{\"1\":{\"153\":1}}],[\"根据你的函数返回的长度\",{\"1\":{\"248\":1,\"249\":1}}],[\"根据用户给定的经纬度坐标来获取指定范围内的地理位置集合\",{\"1\":{\"242\":1}}],[\"根据联合索引中从左到右的顺序\",{\"1\":{\"161\":1}}],[\"根据类模板对象来得到类的信息\",{\"1\":{\"134\":1}}],[\"根据类的二进制数据流来得到类模板对象\",{\"1\":{\"134\":1}}],[\"根据类名来获取类的二进制数据流\",{\"1\":{\"134\":1}}],[\"根据经验\",{\"1\":{\"22\":1}}],[\"根据对象引用的实际类型来执行对应的方法\",{\"1\":{\"11\":1}}],[\"红黑树因为每次的左旋或者右旋都只进行一次\",{\"1\":{\"153\":1}}],[\"红黑树\",{\"0\":{\"153\":1}}],[\"哈希表操作并不难\",{\"1\":{\"246\":1}}],[\"哈希表和整数集合\",{\"1\":{\"221\":1}}],[\"哈希表单查找确实很快\",{\"1\":{\"150\":1}}],[\"哈希表可以实现o\",{\"1\":{\"150\":1}}],[\"哈希表是一种键值对存储\",{\"1\":{\"150\":1}}],[\"建立和修改索引\",{\"1\":{\"148\":1}}],[\"优点\",{\"1\":{\"148\":1,\"157\":1,\"158\":1,\"196\":2}}],[\"优化的自回归transformer\",{\"1\":{\"38\":1}}],[\"索引虽然能够加快查找\",{\"1\":{\"165\":1}}],[\"索引下推\",{\"0\":{\"162\":1}}],[\"索引是一个文件\",{\"1\":{\"148\":1}}],[\"索引的底层数据结构\",{\"0\":{\"149\":1}}],[\"索引的优缺点\",{\"0\":{\"148\":1}}],[\"索引的介绍\",{\"0\":{\"147\":1}}],[\"索引就是为了快速查找内容设立的\",{\"1\":{\"147\":1}}],[\"版本新增的数据类型\",{\"1\":{\"237\":1,\"240\":1}}],[\"版本号是否受jvm支持~~\",{\"1\":{\"137\":1}}],[\"版本控制和监控机器学习模型的平台\",{\"1\":{\"72\":1}}],[\"版本控制\",{\"0\":{\"67\":1},\"1\":{\"66\":1,\"67\":2}}],[\"符号引用验证\",{\"0\":{\"140\":1},\"1\":{\"136\":1}}],[\"字节时\",{\"1\":{\"228\":1}}],[\"字节码验证\",{\"0\":{\"139\":1},\"1\":{\"136\":1}}],[\"字符串常量池单独从方法区移到堆中\",{\"1\":{\"121\":1}}],[\"语义验证用来分析字节码文件的语义是否合规\",{\"1\":{\"138\":1}}],[\"语义验证\",{\"0\":{\"138\":1},\"1\":{\"136\":1}}],[\"语言\",{\"1\":{\"51\":1}}],[\"语言模型的实验结果表明\",{\"1\":{\"75\":1}}],[\"语言模型\",{\"1\":{\"19\":1,\"25\":1}}],[\"格式验证包括验证魔数\",{\"1\":{\"137\":1}}],[\"格式验证其实是发生在加载过程中的\",{\"1\":{\"137\":1}}],[\"格式验证\",{\"0\":{\"137\":1},\"1\":{\"136\":1}}],[\"卸载\",{\"0\":{\"145\":1},\"1\":{\"133\":1}}],[\"调用\",{\"1\":{\"133\":1,\"247\":1}}],[\"调用displaydevicerepository\",{\"1\":{\"3\":1}}],[\"初始化过程主要是对类中的静态变量进行正确的赋值\",{\"1\":{\"143\":1}}],[\"初始化\",{\"0\":{\"143\":1},\"1\":{\"133\":1}}],[\"初始标记\",{\"1\":{\"116\":1,\"117\":1}}],[\"准备\",{\"0\":{\"141\":1},\"1\":{\"133\":2}}],[\"验证了ack确实为i+1\",{\"1\":{\"184\":1}}],[\"验证阶段包括四大验证\",{\"1\":{\"136\":1}}],[\"验证\",{\"0\":{\"136\":1},\"1\":{\"133\":1}}],[\"验证和生产过程中监控ml模型\",{\"1\":{\"70\":1}}],[\"加载\",{\"0\":{\"134\":1},\"1\":{\"133\":1}}],[\"另有两个整数\",{\"1\":{\"251\":1}}],[\"另一个线程会从主内存中读取这个变量到自己的工作内存中\",{\"1\":{\"131\":1}}],[\"另外的的情况是b为语料中其它的随机句子\",{\"1\":{\"22\":1}}],[\"主进程会继续处理任务\",{\"1\":{\"196\":1}}],[\"主键索引中的data也不是这一行的数据\",{\"1\":{\"157\":1}}],[\"主键索引中的data也是指向数据的一个值\",{\"1\":{\"155\":1}}],[\"主键索引中的key是主键\",{\"1\":{\"155\":1}}],[\"主键索引就是为主键建立的索引\",{\"1\":{\"155\":1}}],[\"主键索引和二级索引\",{\"0\":{\"155\":1}}],[\"主内存是线程共有的\",{\"1\":{\"131\":1}}],[\"主要用于存储地理位置信息\",{\"1\":{\"240\":1}}],[\"主要用于判断一个对象什么时候被回收\",{\"1\":{\"106\":1}}],[\"主要分为以下部分\",{\"1\":{\"86\":1}}],[\"主要针对的就是对堆内存的回收\",{\"1\":{\"85\":1}}],[\"主要关注于训练的过程\",{\"1\":{\"58\":1}}],[\"主要是因为它使作者能够最大限度地提高收集到的提示的多样性\",{\"1\":{\"48\":1}}],[\"主要是指方法的重载\",{\"1\":{\"9\":1}}],[\"主要研究点\",{\"0\":{\"14\":1}}],[\"指令\",{\"1\":{\"235\":1}}],[\"指令重排可能会导致一些问题\",{\"1\":{\"129\":1}}],[\"指的应该是引用类型的加载\",{\"1\":{\"133\":1}}],[\"指向接口的引用必须是指定这实现了该接口的一个类的实例程序\",{\"1\":{\"11\":1}}],[\"指向子类的父类引用由于向上转型了\",{\"1\":{\"9\":1}}],[\"整体上围绕了\",{\"1\":{\"127\":1}}],[\"整理或者标记\",{\"1\":{\"111\":1}}],[\"整理不会像标记\",{\"1\":{\"110\":1}}],[\"整理\",{\"0\":{\"110\":1},\"1\":{\"107\":1,\"113\":1,\"115\":1,\"117\":1}}],[\"想额外强调关于常量池的事情\",{\"0\":{\"121\":1}}],[\"程序计数器也是线程私有的\",{\"1\":{\"126\":1}}],[\"程序计数器\",{\"0\":{\"126\":1},\"1\":{\"120\":1}}],[\"暂停时间在几毫秒内\",{\"1\":{\"118\":1}}],[\"暂停工作线程\",{\"1\":{\"116\":1}}],[\"部分上看是标记\",{\"1\":{\"117\":1}}],[\"部署和协作的工作流\",{\"1\":{\"67\":1}}],[\"部署和测试来管理机器学习生命周期的开源平台\",{\"1\":{\"59\":1}}],[\"部署和模型注册\",{\"1\":{\"59\":1}}],[\"策略\",{\"1\":{\"117\":1}}],[\"缺点\",{\"1\":{\"116\":1,\"148\":1,\"150\":1,\"152\":1,\"157\":1,\"158\":1,\"196\":2}}],[\"串行\",{\"0\":{\"113\":1}}],[\"串联起来\",{\"1\":{\"31\":1}}],[\"垃圾收集器\",{\"0\":{\"112\":1}}],[\"垃圾收集算法整体来说有四种\",{\"1\":{\"107\":1}}],[\"垃圾收集算法\",{\"0\":{\"107\":1}}],[\"仍然存活的对象都是连续的\",{\"1\":{\"110\":1}}],[\"回收\",{\"1\":{\"116\":1,\"117\":1}}],[\"回收时\",{\"1\":{\"109\":1}}],[\"回收之前\",{\"1\":{\"109\":1}}],[\"复制的长度\",{\"1\":{\"253\":1}}],[\"复制的最大问题就是可用内存减半了\",{\"1\":{\"109\":1}}],[\"复制那样\",{\"1\":{\"110\":1}}],[\"复制算法就是把内存分为两块区域\",{\"1\":{\"109\":1}}],[\"复制算法\",{\"1\":{\"109\":1}}],[\"复制\",{\"0\":{\"109\":1},\"1\":{\"107\":1,\"111\":1,\"113\":1,\"115\":1,\"117\":1}}],[\"复杂度执行\",{\"1\":{\"79\":1}}],[\"复杂度的推理的架构\",{\"1\":{\"75\":1}}],[\"清除会产生大量内存碎片\",{\"1\":{\"116\":1}}],[\"清除是最基本的垃圾收集算法\",{\"1\":{\"108\":1}}],[\"清除\",{\"0\":{\"108\":1},\"1\":{\"107\":1,\"111\":1,\"116\":1}}],[\"清华大学发布的glm\",{\"1\":{\"26\":1}}],[\"清华大学语言模型glm发布\",{\"1\":{\"14\":1}}],[\"清华大学在语言模型上研究较早\",{\"1\":{\"14\":1}}],[\"方式传递的\",{\"1\":{\"248\":2,\"249\":2}}],[\"方便持久化\",{\"1\":{\"196\":1}}],[\"方便做一些回收前的操作\",{\"1\":{\"106\":1}}],[\"方法区的\",{\"1\":{\"135\":1}}],[\"方法区的实现方式由永久代改为元空间\",{\"1\":{\"121\":1}}],[\"方法区也是线程共享的\",{\"1\":{\"123\":1}}],[\"方法区\",{\"0\":{\"123\":1},\"1\":{\"120\":1}}],[\"方法区中常量池中的变量引用的对象\",{\"1\":{\"97\":1}}],[\"方法区中的静态变量引用的对象\",{\"1\":{\"97\":1}}],[\"方法还可以定义自己独有的属性或者方法\",{\"1\":{\"7\":1}}],[\"方法\",{\"1\":{\"3\":2,\"252\":1}}],[\"虚拟机栈是存放方法执行中的基本类型和引用类型的引用的位置\",{\"1\":{\"124\":1}}],[\"虚拟机栈是线程私有的\",{\"1\":{\"124\":1}}],[\"虚拟机栈\",{\"0\":{\"124\":1},\"1\":{\"120\":1}}],[\"虚拟机栈中引用的对象\",{\"1\":{\"97\":1}}],[\"虚引用是最弱的一种引用\",{\"1\":{\"106\":1}}],[\"虚引用\",{\"0\":{\"106\":1}}],[\"适合做一些缓存表\",{\"1\":{\"105\":1}}],[\"遇到垃圾回收就会回收掉\",{\"1\":{\"105\":1}}],[\"弱引用的应用包括weakhashmap\",{\"1\":{\"105\":1}}],[\"弱引用的应用\",{\"0\":{\"105\":1}}],[\"弱引用使用weakreference进行创建\",{\"1\":{\"104\":1}}],[\"弱引用比软引用还低一些\",{\"1\":{\"104\":1}}],[\"弱引用\",{\"0\":{\"104\":1}}],[\"弱引用和虚引用\",{\"1\":{\"101\":1}}],[\"宁可抛出oom也不会进行内存回收\",{\"1\":{\"102\":1}}],[\"软引用的垃圾不足才回收的特点\",{\"1\":{\"103\":1}}],[\"软引用比强引用弱一些\",{\"1\":{\"103\":1}}],[\"软引用\",{\"0\":{\"103\":1},\"1\":{\"101\":1}}],[\"强引用不会被垃圾回收\",{\"1\":{\"102\":1}}],[\"强引用是我们最常见的引用\",{\"1\":{\"102\":1}}],[\"强引用\",{\"0\":{\"102\":1},\"1\":{\"101\":1}}],[\"满足这三大条件\",{\"1\":{\"100\":1}}],[\"堆中可以分为新生代和老年代\",{\"1\":{\"122\":1}}],[\"堆中没有任何该类的实例\",{\"1\":{\"100\":1}}],[\"堆是线程共享的\",{\"1\":{\"122\":1}}],[\"堆\",{\"0\":{\"122\":1},\"1\":{\"120\":1}}],[\"堆内存结构\",{\"0\":{\"86\":1}}],[\"继续活下去\",{\"1\":{\"98\":1}}],[\"继承和接口\",{\"1\":{\"11\":1}}],[\"继承是一种强耦合的关系\",{\"1\":{\"8\":1}}],[\"继承是使用已存在的类的定义作为基础建立新类的技术\",{\"1\":{\"7\":1}}],[\"继承破坏了封装\",{\"1\":{\"8\":1}}],[\"继承有以下缺陷\",{\"1\":{\"8\":1}}],[\"继承定义了类如何相互关联\",{\"1\":{\"7\":1}}],[\"继承者完全可以替换被继承者\",{\"1\":{\"7\":1}}],[\"继承所描述的是\",{\"1\":{\"7\":1}}],[\"继承\",{\"0\":{\"4\":1,\"7\":1},\"1\":{\"10\":2}}],[\"那对于innodb来说\",{\"1\":{\"159\":1}}],[\"那就不太合适了\",{\"1\":{\"150\":1}}],[\"那就不会被垃圾回收\",{\"1\":{\"98\":1}}],[\"那就通过链表的方式来解决\",{\"1\":{\"150\":1}}],[\"那么您的题解将被\",{\"1\":{\"247\":1}}],[\"那么底层数据结构就是整数集合\",{\"1\":{\"221\":1}}],[\"那么消费者就需要一直尝试读取\",{\"1\":{\"212\":1}}],[\"那么\",{\"0\":{\"206\":1},\"1\":{\"213\":1}}],[\"那么字符串对象仍然使用sds数据结构来保存这个字符串\",{\"1\":{\"205\":1}}],[\"那么字符串对象会使用sds来保存这个字符串\",{\"1\":{\"205\":1}}],[\"那么字符串对象会将整数值保存在字符串对象结构的ptr属性里\",{\"1\":{\"205\":1}}],[\"那么可以认为这条报文在传输过程中丢失了\",{\"1\":{\"188\":1}}],[\"那么可能会适得其反\",{\"1\":{\"165\":1}}],[\"那么单列索引就不需要了\",{\"1\":{\"170\":1}}],[\"那么c就是走不了索引的\",{\"1\":{\"161\":1}}],[\"那么其实也不需要进行回表\",{\"1\":{\"159\":1}}],[\"那么索引之间的比较就会很耗时\",{\"1\":{\"157\":1}}],[\"那么建立和搜索索引都会非常顺序\",{\"1\":{\"157\":1}}],[\"那么就使用压缩列表\",{\"1\":{\"209\":1}}],[\"那么就把它当作主键索引\",{\"1\":{\"155\":1}}],[\"那么就代表可以进行回收了\",{\"1\":{\"96\":1}}],[\"那么它的孩子就是红色的\",{\"1\":{\"153\":1}}],[\"那么效率就非常高了\",{\"1\":{\"147\":1}}],[\"那么时间会非常长\",{\"1\":{\"147\":1}}],[\"那么便可以进行回收\",{\"1\":{\"99\":1}}],[\"那么会将它放到一个队列当中去\",{\"1\":{\"98\":1}}],[\"那么该对象可以被垃圾回收了\",{\"1\":{\"95\":1}}],[\"那么引用次数加1\",{\"1\":{\"95\":1}}],[\"那么只好通过空间分配担保\",{\"1\":{\"89\":1}}],[\"那么有\",{\"1\":{\"77\":1}}],[\"那么自然glm原生地支持p\",{\"1\":{\"32\":1}}],[\"那么p\",{\"1\":{\"32\":1}}],[\"那么什么是text\",{\"1\":{\"24\":1}}],[\"那么具体是怎么做的呢\",{\"1\":{\"22\":1}}],[\"那么对于任意一个序列的每一位置都将用同一个向量来进行表示\",{\"1\":{\"21\":1}}],[\"那么encoder会认为这两个序列完全相同\",{\"1\":{\"18\":1}}],[\"标准误算率是\",{\"1\":{\"237\":1}}],[\"标志位\",{\"1\":{\"183\":1}}],[\"标记直接和gc\",{\"1\":{\"116\":1}}],[\"标记和清除两个过程耗时都比较大\",{\"1\":{\"108\":1}}],[\"标记\",{\"0\":{\"108\":1,\"109\":1,\"110\":1},\"1\":{\"107\":3,\"108\":1,\"109\":2,\"110\":1,\"116\":1}}],[\"标记过程\",{\"1\":{\"98\":1}}],[\"标签为notnext\",{\"1\":{\"22\":1}}],[\"标签为isnext\",{\"1\":{\"22\":1}}],[\"至少要经过两次\",{\"1\":{\"98\":1}}],[\"至此\",{\"1\":{\"18\":1,\"29\":1,\"187\":2}}],[\"哪些对象可以作为gc\",{\"0\":{\"97\":1}}],[\"去寻找被它引用的对象链\",{\"1\":{\"96\":1}}],[\"引用\",{\"1\":{\"248\":2,\"249\":2}}],[\"引用验证\",{\"0\":{\"140\":1},\"1\":{\"136\":1}}],[\"引用类型总结\",{\"0\":{\"101\":1}}],[\"引用次数减1\",{\"1\":{\"95\":1}}],[\"引用计数器的想法比较简单\",{\"1\":{\"95\":1}}],[\"引用计数器\",{\"0\":{\"95\":1}}],[\"引用计数器和可达性分析\",{\"1\":{\"94\":1}}],[\"引入序列转换模型\",{\"1\":{\"16\":1}}],[\"死亡对象的判断方法\",{\"0\":{\"94\":1}}],[\"否则\",{\"1\":{\"209\":1,\"217\":1,\"221\":1}}],[\"否则就创建一个6bit的主键索引\",{\"1\":{\"155\":1}}],[\"否则进行full\",{\"1\":{\"93\":1}}],[\"否则仍然会抛出noclassdeffounderror\",{\"1\":{\"3\":1}}],[\"才会进行minor\",{\"1\":{\"93\":1}}],[\"把对象提前放到老年代\",{\"1\":{\"89\":1}}],[\"分别表示\",{\"1\":{\"251\":1}}],[\"分数必须相同\",{\"1\":{\"229\":2}}],[\"分数由低到高排序\",{\"1\":{\"229\":1}}],[\"分布式锁\",{\"1\":{\"207\":1}}],[\"分代\",{\"1\":{\"117\":1}}],[\"分代收集\",{\"0\":{\"111\":1},\"1\":{\"107\":1}}],[\"分配原则\",{\"0\":{\"88\":1}}],[\"分块训练对长序列训练特别有用\",{\"1\":{\"79\":1}}],[\"分块的循环表示可以执行有效的长序列建模\",{\"1\":{\"75\":1}}],[\"分块循环意味着便于具有线性复杂度的高效长序列建模\",{\"1\":{\"74\":1}}],[\"内部实现\",{\"0\":{\"203\":1,\"209\":1,\"217\":1,\"221\":1,\"228\":1,\"241\":1}}],[\"内部类是可以直接访问外部类的变量的\",{\"1\":{\"3\":1}}],[\"内存\",{\"1\":{\"237\":1}}],[\"内存中的数据结构存储系统\",{\"1\":{\"190\":1}}],[\"内存分配和回收原则\",{\"0\":{\"87\":1}}],[\"元素的顺序可以改变\",{\"1\":{\"248\":1}}],[\"元素的\",{\"1\":{\"247\":1}}],[\"元素的权重分数\",{\"1\":{\"241\":1}}],[\"元素从key中删除\",{\"1\":{\"222\":1}}],[\"元素不从key中删除\",{\"1\":{\"222\":1}}],[\"元素存在则忽略\",{\"1\":{\"222\":1}}],[\"元空间里\",{\"1\":{\"135\":1}}],[\"元空间\",{\"1\":{\"86\":1}}],[\"元数据和管道版本控制\",{\"1\":{\"67\":1}}],[\"老年代标记\",{\"1\":{\"115\":1}}],[\"老年代一般对象数量多\",{\"1\":{\"111\":1}}],[\"老年代剩余的连续空间大于新生代目前对象所占空间或者历次平均占用空间\",{\"1\":{\"93\":1}}],[\"老年代\",{\"1\":{\"86\":1}}],[\"新的对象优先分配到新生代的伊甸园\",{\"0\":{\"89\":1}}],[\"新生代标记\",{\"1\":{\"115\":1}}],[\"新生代经常有大量对象死亡\",{\"1\":{\"111\":1}}],[\"新生代\",{\"1\":{\"86\":1}}],[\"新类的定义可以增加新的数据或新的功能\",{\"1\":{\"7\":1}}],[\"网际协议\",{\"1\":{\"84\":1}}],[\"网络层协议\",{\"0\":{\"84\":1}}],[\"面向连接\",{\"1\":{\"83\":1}}],[\"传统的关系型数据库已经不能胜任所有任务了\",{\"1\":{\"191\":1}}],[\"传统的getsystemservice\",{\"1\":{\"3\":1}}],[\"传输控制协议\",{\"1\":{\"83\":1}}],[\"传输层协议\",{\"0\":{\"83\":1}}],[\"域名管理系统\",{\"1\":{\"82\":1}}],[\"安全的网络传输协议\",{\"1\":{\"82\":1}}],[\"安全性等\",{\"1\":{\"49\":1}}],[\"远程登录协议\",{\"1\":{\"82\":1}}],[\"文件大时启动比rdb慢\",{\"1\":{\"196\":1}}],[\"文件大\",{\"1\":{\"196\":1}}],[\"文件过大会进行rewrite\",{\"1\":{\"196\":1}}],[\"文件传输协议\",{\"1\":{\"82\":1}}],[\"文本和表格\",{\"1\":{\"63\":1}}],[\"文本到文本\",{\"1\":{\"14\":1}}],[\"邮件接收协议\",{\"1\":{\"82\":1}}],[\"超文本传输协议\",{\"1\":{\"82\":1}}],[\"超参数优化和模型管理的机器学习平台\",{\"1\":{\"63\":1}}],[\"应忽略\",{\"1\":{\"251\":1}}],[\"应用\",{\"1\":{\"244\":1}}],[\"应用执行下面的命令时\",{\"1\":{\"244\":1}}],[\"应用就可以使用\",{\"1\":{\"244\":1}}],[\"应用访问的数据是和人或物关联的一组经纬度信息\",{\"1\":{\"240\":1}}],[\"应用场景\",{\"0\":{\"207\":1,\"211\":1,\"219\":1,\"223\":1,\"230\":1,\"234\":1,\"239\":1,\"243\":1}}],[\"应用层协议\",{\"0\":{\"82\":1}}],[\"应答号\",{\"1\":{\"183\":1}}],[\"应该保持\",{\"1\":{\"247\":1}}],[\"应该把一些过滤效果好的列放到左边\",{\"1\":{\"161\":1}}],[\"应该指的是query和key\",{\"1\":{\"77\":1}}],[\"应该贯穿在整个对话中\",{\"1\":{\"51\":1}}],[\"应该是效果比较不错的一个模型\",{\"1\":{\"26\":1}}],[\"❇️retention网络的整体架构\",{\"0\":{\"79\":1}}],[\"∣∑i=1n​rni​∣\",{\"1\":{\"78\":1}}],[\"∣x∣​​​\",{\"1\":{\"76\":1}}],[\"∣x∣​\",{\"1\":{\"76\":1}}],[\"∑i=1n​dni​​\",{\"1\":{\"78\":1}}],[\"α∗headi​\",{\"1\":{\"78\":1}}],[\"交由\",{\"1\":{\"228\":1}}],[\"交集计算\",{\"1\":{\"229\":1}}],[\"交集\",{\"1\":{\"223\":1}}],[\"交集运算\",{\"1\":{\"222\":1}}],[\"交叉块信息按照循环表示方式传递\",{\"1\":{\"77\":1}}],[\"交互式数据漂移\",{\"1\":{\"70\":1}}],[\"交互式仪表板\",{\"1\":{\"70\":1}}],[\"−2\",{\"1\":{\"77\":4}}],[\"−1\",{\"1\":{\"77\":5}}],[\"−m称为xpos\",{\"1\":{\"77\":1}}],[\"−m\",{\"1\":{\"77\":1}}],[\"∗\",{\"1\":{\"77\":37}}],[\"伪代码如下\",{\"1\":{\"77\":3}}],[\"变为\",{\"1\":{\"77\":1}}],[\"变体\",{\"1\":{\"41\":1}}],[\"等式\",{\"1\":{\"77\":1}}],[\"等部分参数微调方法\",{\"1\":{\"14\":1}}],[\"式\",{\"1\":{\"77\":1}}],[\"λ−1\",{\"1\":{\"77\":1}}],[\"γeiθ\",{\"1\":{\"77\":7}}],[\"注意这五个元素可为任意顺序\",{\"1\":{\"248\":1}}],[\"注意是双指针题就可以了\",{\"1\":{\"247\":1}}],[\"注意不要用\",{\"1\":{\"103\":1}}],[\"注意\",{\"1\":{\"100\":1,\"251\":2}}],[\"注意只能发送\",{\"1\":{\"82\":1}}],[\"注意力集中在上述公式中source的内部元素\",{\"1\":{\"17\":1}}],[\"注意力机制体现在模型中是多头注意力机制\",{\"1\":{\"20\":1}}],[\"注意力机制公式如下\",{\"1\":{\"17\":1}}],[\"注意力机制就是给定一个query\",{\"1\":{\"17\":1}}],[\"注意力机制\",{\"0\":{\"17\":1},\"1\":{\"17\":2}}],[\"注\",{\"1\":{\"77\":1}}],[\"映射到o\",{\"1\":{\"77\":1}}],[\"​x1\",{\"1\":{\"76\":1}}],[\"​x11​\",{\"1\":{\"76\":1}}],[\"说明\",{\"1\":{\"248\":1,\"249\":1}}],[\"说回收就回收\",{\"1\":{\"100\":1}}],[\"说白了应该是这样形状的矩阵\",{\"1\":{\"76\":1}}],[\"说到gpt\",{\"1\":{\"25\":1}}],[\"∈r∣x∣×dmodel​\",{\"1\":{\"76\":1,\"79\":1}}],[\"输出\",{\"1\":{\"245\":2,\"247\":2,\"248\":2,\"249\":2,\"251\":3}}],[\"输出进行向量化\",{\"1\":{\"18\":1}}],[\"输入数组是以\",{\"1\":{\"248\":1,\"249\":1}}],[\"输入数组\",{\"1\":{\"247\":1}}],[\"输入\",{\"1\":{\"245\":2,\"247\":2,\"248\":2,\"249\":2,\"251\":3}}],[\"输入序列\",{\"1\":{\"79\":1}}],[\"输入向量\",{\"1\":{\"76\":1}}],[\"模块\",{\"1\":{\"76\":1}}],[\"模型监控\",{\"1\":{\"71\":1}}],[\"模型性能和目标虚拟化\",{\"1\":{\"70\":1}}],[\"模型部署与管理\",{\"0\":{\"68\":1,\"69\":1},\"1\":{\"68\":1,\"69\":1}}],[\"模型指标\",{\"1\":{\"67\":1}}],[\"模型等进行版本控制\",{\"1\":{\"67\":1}}],[\"模型再部署\",{\"0\":{\"59\":1},\"1\":{\"59\":1}}],[\"模型训练和模型服务\",{\"1\":{\"56\":1}}],[\"模型的进展\",{\"1\":{\"53\":1}}],[\"模型中的kv缓存大小相关的内存成本显著增长\",{\"1\":{\"41\":1}}],[\"模型\",{\"1\":{\"19\":1,\"63\":1,\"67\":1}}],[\"模型和自编码\",{\"1\":{\"19\":1}}],[\"模型应该是双向的才好\",{\"1\":{\"19\":1}}],[\"模型应该能毫不费力地泛化更长的句子\",{\"1\":{\"18\":1}}],[\"模型就失去了\",{\"1\":{\"18\":1}}],[\"模型需要做的是通过已经有的信息来预测下一个位置会出现什么\",{\"1\":{\"18\":1}}],[\"模型一般包括一个编码器\",{\"1\":{\"16\":1}}],[\"允许巨大的吞吐量\",{\"1\":{\"75\":1}}],[\"没有就阻塞timeout秒\",{\"1\":{\"210\":2}}],[\"没有键值缓存\",{\"1\":{\"75\":1}}],[\"没有看出具体用途\",{\"1\":{\"66\":1}}],[\"推理过程中\",{\"1\":{\"79\":1}}],[\"推理\",{\"1\":{\"79\":1}}],[\"推理速度急速下降\",{\"1\":{\"75\":1}}],[\"推断\",{\"1\":{\"75\":1}}],[\"递归表示能够在内存和计算方面实现有效的o\",{\"1\":{\"75\":1}}],[\"代替多头注意力\",{\"1\":{\"75\":1}}],[\"高效长序列建模\",{\"1\":{\"75\":1}}],[\"高质量指令微调数据\",{\"1\":{\"46\":1}}],[\"希望提出一个在保持平行训练和良好表现的前提下\",{\"1\":{\"75\":1}}],[\"现在有两个数组int\",{\"1\":{\"253\":1}}],[\"现在有了一个上下文对话和样本\",{\"1\":{\"51\":1}}],[\"现在用ssh\",{\"1\":{\"82\":1}}],[\"现在\",{\"1\":{\"75\":1,\"77\":1}}],[\"降低延迟和减少gpu内存使用\",{\"1\":{\"74\":1}}],[\"降至最大学习率的10\",{\"1\":{\"49\":1}}],[\"循环和块递归表示\",{\"1\":{\"75\":1}}],[\"循环和分块循环\",{\"1\":{\"74\":1}}],[\"循环意味着可以在o\",{\"1\":{\"74\":1}}],[\"低耗费推理和良好表现三大特性\",{\"1\":{\"74\":1}}],[\"低于10b参数\",{\"1\":{\"14\":1}}],[\"选择合适的列作为索引\",{\"0\":{\"164\":1}}],[\"选择机器学习模型\",{\"1\":{\"73\":1}}],[\"选择一种合适的方式表示词在序列中的顺序非常重要\",{\"1\":{\"18\":1}}],[\"框架不可知服务\",{\"1\":{\"72\":1}}],[\"易于理解的推论\",{\"1\":{\"72\":1}}],[\"跟踪已部署的ml模型的性能\",{\"1\":{\"72\":1}}],[\"跟踪模型性能\",{\"1\":{\"72\":1}}],[\"跟全参数微调一样利用cls或者token的输出做nlu\",{\"1\":{\"32\":1}}],[\"支持并交差等操作\",{\"1\":{\"223\":1}}],[\"支持\",{\"1\":{\"204\":1}}],[\"支持所有主要的编程语言和框架\",{\"1\":{\"72\":1}}],[\"支持llama\",{\"1\":{\"53\":1}}],[\"测试\",{\"1\":{\"70\":1}}],[\"测试和交付步骤\",{\"1\":{\"60\":1}}],[\"目的端口号\",{\"1\":{\"183\":1}}],[\"目标漂移以及回归和分类性能\",{\"1\":{\"70\":1}}],[\"目前\",{\"1\":{\"14\":1}}],[\"赠款您完全控制模型管理操作\",{\"1\":{\"69\":1}}],[\"灵活\",{\"1\":{\"69\":1}}],[\"管理和扩展机器学习模型\",{\"1\":{\"69\":1}}],[\"管道和结果\",{\"1\":{\"63\":1}}],[\"创建\",{\"1\":{\"67\":1}}],[\"流行的机器学习项目工具\",{\"1\":{\"67\":1}}],[\"流程运行和部署\",{\"1\":{\"64\":1}}],[\"数组中的数据\",{\"1\":{\"204\":1}}],[\"数千个作业\",{\"1\":{\"66\":1}}],[\"数据编码\",{\"0\":{\"205\":1}}],[\"数据结构来实现了\",{\"1\":{\"228\":1}}],[\"数据结构\",{\"0\":{\"204\":1,\"209\":1}}],[\"数据安全\",{\"1\":{\"196\":1}}],[\"数据安全性低\",{\"1\":{\"196\":1}}],[\"数据备份\",{\"1\":{\"191\":1}}],[\"数据库变化而resultmap变化就会导致报错\",{\"1\":{\"173\":1}}],[\"数据查询比较慢\",{\"1\":{\"158\":1}}],[\"数据漂移\",{\"1\":{\"70\":1}}],[\"数据和模型注册表\",{\"1\":{\"67\":1}}],[\"数据和模型版本控制\",{\"1\":{\"63\":1}}],[\"数据\",{\"1\":{\"67\":1}}],[\"数据集比较大的时候\",{\"1\":{\"196\":1}}],[\"数据集\",{\"1\":{\"63\":1}}],[\"数据分布检查\",{\"1\":{\"56\":1}}],[\"数据被保护在抽象数据类型的内部\",{\"1\":{\"5\":1}}],[\"视频\",{\"1\":{\"66\":1,\"204\":1}}],[\"视觉\",{\"1\":{\"63\":1}}],[\"沿袭和端到端管道自动化数据转换\",{\"1\":{\"66\":1}}],[\"沿用了bert和t5的masked的比例\",{\"1\":{\"27\":1}}],[\"久经考验的工作流管理工具\",{\"1\":{\"65\":1}}],[\"工作内存是线程私有的\",{\"1\":{\"131\":1}}],[\"工作内存和主内存\",{\"0\":{\"131\":1}}],[\"工作线程恢复\",{\"1\":{\"116\":2}}],[\"工作区和团队协作\",{\"1\":{\"64\":1}}],[\"工作流管理\",{\"0\":{\"64\":1},\"1\":{\"64\":1,\"65\":1}}],[\"工作流\",{\"1\":{\"58\":1}}],[\"您还可以管理帐户\",{\"1\":{\"64\":1}}],[\"您可以自动处理原始数据\",{\"1\":{\"73\":1}}],[\"您可以在单个api端点上部署多个模型\",{\"1\":{\"69\":1}}],[\"您可以与任何数据\",{\"1\":{\"66\":1}}],[\"您可以使用它\",{\"1\":{\"67\":1}}],[\"您可以使用它来记录工件\",{\"1\":{\"63\":1}}],[\"您可以使用commit\",{\"1\":{\"66\":1}}],[\"您可以使用cli\",{\"1\":{\"59\":1}}],[\"您可以使用prefect\",{\"1\":{\"64\":1}}],[\"您可以将其与任何机器学习库一起使用\",{\"1\":{\"62\":1}}],[\"专为端到端机器学习pipeline而构建\",{\"1\":{\"64\":1}}],[\"专业点就是信息隐藏\",{\"1\":{\"5\":1}}],[\"协调和编排应用程序之间的工作流\",{\"1\":{\"64\":1}}],[\"音频\",{\"1\":{\"63\":1,\"204\":1}}],[\"比aof启动更快\",{\"1\":{\"196\":1}}],[\"比较适合使用集合类型进行存储\",{\"1\":{\"223\":1}}],[\"比较新\",{\"1\":{\"118\":1}}],[\"比较\",{\"1\":{\"62\":1}}],[\"比如手动编写汇编或者手动编写编译器中间语言来替换该方法的实现\",{\"1\":{\"253\":1}}],[\"比如签到表\",{\"1\":{\"234\":1}}],[\"比如过期删除等\",{\"1\":{\"191\":1}}],[\"比如秒杀的库存扣减\",{\"1\":{\"191\":1}}],[\"比如uuid什么的\",{\"1\":{\"157\":1}}],[\"比如innodb中的主键索引\",{\"1\":{\"157\":1}}],[\"比如a>100\",{\"1\":{\"150\":1}}],[\"比如是否继承了final类\",{\"1\":{\"138\":1}}],[\"比如一个线程的读在另一个线程的写之后\",{\"1\":{\"129\":1}}],[\"比如写操作可能会放到最后执行\",{\"1\":{\"129\":1}}],[\"比如mybatis中的softcache\",{\"1\":{\"103\":1}}],[\"比如自回归模型擅长自然语言生成任务\",{\"1\":{\"26\":1}}],[\"比如问题回答\",{\"1\":{\"22\":1}}],[\"比如\",{\"1\":{\"18\":1,\"19\":1,\"22\":1,\"24\":1,\"30\":1,\"31\":1,\"111\":1,\"174\":1,\"252\":1,\"253\":1}}],[\"比如百度的文心\",{\"1\":{\"14\":1}}],[\"比如会时常跳出小窗\",{\"1\":{\"3\":1}}],[\"微软的azure结合了azure\",{\"1\":{\"61\":1}}],[\"监控来自已部署ml服务的数据和模型指标\",{\"1\":{\"70\":1}}],[\"监控\",{\"0\":{\"59\":1,\"63\":1,\"70\":1},\"1\":{\"59\":1,\"63\":1,\"70\":1}}],[\"监听display系统服务的添加\",{\"1\":{\"3\":1}}],[\"则需要考虑到哈希表\",{\"1\":{\"246\":1}}],[\"则是将三个\",{\"1\":{\"236\":1}}],[\"则会进入存活区\",{\"1\":{\"91\":1}}],[\"则应该找寻更为合适的框架或者平台\",{\"1\":{\"58\":1}}],[\"则可以表示a继承b\",{\"1\":{\"7\":1}}],[\"你必须在\",{\"1\":{\"249\":1}}],[\"你必须仅使用\",{\"1\":{\"248\":1}}],[\"你不需要考虑数组中超出新长度后面的元素\",{\"1\":{\"248\":3}}],[\"你需要\",{\"1\":{\"248\":1}}],[\"你需要做以下事情确保你的题解可以被通过\",{\"1\":{\"247\":1}}],[\"你可以设计实现一个时间复杂度为\",{\"1\":{\"251\":1}}],[\"你可以想象内部操作如下\",{\"1\":{\"248\":1,\"249\":1}}],[\"你可以假设数组是非空的\",{\"1\":{\"245\":1}}],[\"你可以使用类似的语法来版本化你的数据\",{\"1\":{\"66\":1}}],[\"你都应该能够运行kubeflow\",{\"1\":{\"58\":1}}],[\"你喜欢例如网球\",{\"1\":{\"51\":1}}],[\"简而言之\",{\"1\":{\"68\":1}}],[\"简单题没啥说的\",{\"1\":{\"248\":1}}],[\"简单动态字符串\",{\"1\":{\"204\":1}}],[\"简单来说\",{\"1\":{\"133\":1,\"237\":1}}],[\"简单文件发送协议\",{\"1\":{\"82\":1}}],[\"简单方便\",{\"1\":{\"72\":1}}],[\"简单\",{\"1\":{\"58\":1}}],[\"简洁地回应\",{\"1\":{\"51\":1}}],[\"靠的是其丰富的工具\",{\"1\":{\"58\":1}}],[\"⭐\",{\"0\":{\"58\":1,\"59\":1,\"63\":1,\"64\":1,\"67\":1,\"68\":1,\"69\":1,\"70\":1}}],[\"偏训练\",{\"0\":{\"58\":1},\"1\":{\"58\":1}}],[\"启动和监控机器学习系统所需的常见组件\",{\"1\":{\"56\":1}}],[\"平衡二叉树很好地解决了查找退化的问题\",{\"1\":{\"152\":1}}],[\"平衡二叉树规定\",{\"1\":{\"152\":1}}],[\"平衡二叉树做了一些工作\",{\"1\":{\"152\":1}}],[\"平衡二叉树\",{\"0\":{\"152\":1}}],[\"平均只需要o\",{\"1\":{\"151\":1}}],[\"平行意味着允许平行训练\",{\"1\":{\"74\":1}}],[\"平行\",{\"1\":{\"74\":1}}],[\"平台\",{\"1\":{\"56\":1}}],[\"平局率为31\",{\"1\":{\"54\":1}}],[\"类的二进制数据流才会加载到方法区当中\",{\"1\":{\"137\":1}}],[\"类的加载流程\",{\"0\":{\"132\":1}}],[\"类模板放到\",{\"1\":{\"135\":1}}],[\"类模板和class对象的位置\",{\"0\":{\"135\":1}}],[\"类模板对象中会存放类的信息\",{\"1\":{\"134\":1}}],[\"类模板对象\",{\"1\":{\"134\":1}}],[\"类加载过程分为七个步骤\",{\"1\":{\"133\":1}}],[\"类到这里也不会一定回收\",{\"1\":{\"100\":1}}],[\"类型使用\",{\"1\":{\"241\":1}}],[\"类型相比\",{\"1\":{\"237\":1}}],[\"类型没有支持差集运算\",{\"1\":{\"229\":1}}],[\"类型的底层数据结构\",{\"1\":{\"228\":2}}],[\"类型的底层数据结构是由压缩列表或跳表实现的\",{\"1\":{\"228\":1}}],[\"类型比较适合用来数据去重和保障数据的唯一性\",{\"1\":{\"223\":1}}],[\"类型\",{\"1\":{\"59\":1,\"229\":1}}],[\"类别\",{\"1\":{\"56\":1,\"57\":1,\"58\":1,\"59\":1,\"60\":1,\"61\":1}}],[\"类内部的结构可以自由修改\",{\"1\":{\"6\":1}}],[\"知乎\",{\"1\":{\"55\":1}}],[\"旨在统一\",{\"1\":{\"55\":1}}],[\"结果保存到一个新\",{\"1\":{\"236\":1}}],[\"结果如图12所示\",{\"1\":{\"54\":1}}],[\"结构基本一致\",{\"1\":{\"30\":1}}],[\"无序\",{\"1\":{\"223\":1}}],[\"无论内存够不够\",{\"1\":{\"104\":1}}],[\"无论你在哪里运行kubernetes\",{\"1\":{\"58\":1}}],[\"无连接\",{\"1\":{\"83\":1}}],[\"无害和有用>50\",{\"1\":{\"53\":1}}],[\"无法立即获得logicaldisplay从而拿到displayid\",{\"1\":{\"3\":1}}],[\"无法获取\",{\"1\":{\"3\":1}}],[\"配备gatt的型号\",{\"1\":{\"51\":1}}],[\"王尔德\",{\"1\":{\"51\":1}}],[\"拿破仑\",{\"1\":{\"51\":1}}],[\"人类评价通常被认为是评判自然语言生成模型\",{\"1\":{\"54\":1}}],[\"人类评价\",{\"0\":{\"54\":1}}],[\"人类偏好注释一致率也更高\",{\"1\":{\"49\":1}}],[\"人类偏好数据收集\",{\"0\":{\"48\":1}}],[\"人物\",{\"1\":{\"51\":1}}],[\"爱好\",{\"1\":{\"51\":1}}],[\"之前处于伊甸园区的对象会被移动到存活区\",{\"1\":{\"89\":1}}],[\"之间的多回合对话数据集\",{\"1\":{\"51\":1}}],[\"之所以要缩放\",{\"1\":{\"18\":1}}],[\"假设车辆\",{\"1\":{\"244\":1}}],[\"假设要统计\",{\"1\":{\"236\":1}}],[\"假设我们要统计\",{\"1\":{\"235\":1}}],[\"假设可以访问两个人\",{\"1\":{\"51\":1}}],[\"假设prefix部分由50个token组成\",{\"1\":{\"32\":1}}],[\"某个公众人物\",{\"1\":{\"51\":1}}],[\"扮演奥斯卡\",{\"1\":{\"51\":1}}],[\"扮演例如拿破仑\",{\"1\":{\"51\":1}}],[\"扮演\",{\"1\":{\"51\":2}}],[\"≈20×\",{\"1\":{\"50\":1}}],[\"设置过期时间\",{\"1\":{\"236\":1}}],[\"设置成\",{\"1\":{\"236\":1}}],[\"设置值\",{\"1\":{\"233\":1}}],[\"设置β=0\",{\"1\":{\"50\":1}}],[\"设b表示块的长度\",{\"1\":{\"77\":1}}],[\"设备就需要千万级别\",{\"1\":{\"30\":1}}],[\"右侧是使用gatt优化后的结果\",{\"1\":{\"51\":1}}],[\"右\",{\"1\":{\"50\":1,\"51\":2}}],[\"左不走右走\",{\"1\":{\"176\":1}}],[\"左孩子和右孩子的高度之差不能大于1\",{\"1\":{\"152\":1}}],[\"左\",{\"1\":{\"50\":1,\"51\":1}}],[\"左右\",{\"1\":{\"14\":1}}],[\"温度参数对勘探也起着重要作用\",{\"1\":{\"50\":1}}],[\"探索和作者能在样本中获得的最大回报之间有着直接的联系\",{\"1\":{\"50\":1}}],[\"产生良好轨迹的机会更多\",{\"1\":{\"50\":1}}],[\"正序获取有序集合key从start下标到stop下标的元素\",{\"1\":{\"229\":1}}],[\"正确使用索引的建议\",{\"0\":{\"163\":1}}],[\"正常来说\",{\"1\":{\"150\":1}}],[\"正规化r为r~nm​=rnm​\",{\"1\":{\"78\":1}}],[\"正规化qkt为qkt\",{\"1\":{\"78\":1}}],[\"正向或反向传播时是有效的\",{\"1\":{\"50\":1}}],[\"正如预期的那样\",{\"1\":{\"50\":1,\"53\":1}}],[\"正是bert\",{\"1\":{\"19\":1}}],[\"拒绝采样的介绍\",{\"1\":{\"50\":1}}],[\"拒绝采样微调\",{\"1\":{\"50\":1}}],[\"两种rl算法之间的基本差异就不那么明显了\",{\"1\":{\"50\":1}}],[\"样本是前一步骤的梯度更新后从t−1更新的模型策略的函数\",{\"1\":{\"50\":1}}],[\"深度\",{\"1\":{\"50\":1}}],[\"广度\",{\"1\":{\"50\":1}}],[\"获得最高奖励分数的样本被视为新的金标准\",{\"1\":{\"50\":1}}],[\"获取指定范围内值为\",{\"1\":{\"233\":1}}],[\"获取值\",{\"1\":{\"233\":1}}],[\"获取集合key中的元素个数\",{\"1\":{\"222\":1}}],[\"获取集合key中所有元素\",{\"1\":{\"222\":1}}],[\"获取哈希表key对应的field键值\",{\"1\":{\"218\":1}}],[\"获取dms实例常用方式是通过xposed\",{\"1\":{\"3\":1}}],[\"获取系统服务最终需要调用servicemanager\",{\"1\":{\"3\":1}}],[\"近端策略优化\",{\"1\":{\"50\":1}}],[\"近似性能下可以将设备减少为1台dgx\",{\"1\":{\"32\":1}}],[\"迭代微调\",{\"0\":{\"50\":1}}],[\"学习对人类偏好进行建模将变得具有挑战性\",{\"1\":{\"49\":1}}],[\"学习率按余弦学习率计划降低\",{\"1\":{\"49\":1}}],[\"稍微好一点\",{\"1\":{\"49\":1}}],[\"明显更好\",{\"1\":{\"49\":1}}],[\"总结于来源1来源2\",{\"1\":{\"199\":1}}],[\"总结于来源1\",{\"1\":{\"189\":1}}],[\"总结于https\",{\"1\":{\"133\":1,\"171\":1}}],[\"总结于文章1\",{\"1\":{\"101\":1}}],[\"总体胜率超过75\",{\"1\":{\"54\":1}}],[\"总体而言\",{\"1\":{\"49\":1}}],[\"总词汇大小为32k个标记\",{\"1\":{\"43\":1}}],[\"θ∈rd\",{\"1\":{\"77\":1}}],[\"θ\",{\"1\":{\"49\":1}}],[\"奖励建模的结果\",{\"1\":{\"49\":1}}],[\"奖励建模的训练详情\",{\"1\":{\"49\":1}}],[\"奖励建模就是拿一个模型的结果和它相关的prompt作为输入\",{\"1\":{\"49\":1}}],[\"奖励建模\",{\"0\":{\"49\":1}}],[\"看看\",{\"1\":{\"161\":1}}],[\"看哪个更符合标准\",{\"1\":{\"48\":1}}],[\"看一下\",{\"1\":{\"2\":1}}],[\"要复制到的数组\",{\"1\":{\"253\":1}}],[\"要注意这是一道双指针题目\",{\"1\":{\"248\":1}}],[\"要比string方便不少\",{\"1\":{\"216\":1}}],[\"要避免索引重复\",{\"0\":{\"170\":1}}],[\"要避免索引失效\",{\"0\":{\"169\":1}}],[\"要使用前缀索引\",{\"0\":{\"168\":1}}],[\"要求模型扮演训练中没有遇到的人\",{\"1\":{\"51\":1}}],[\"要求预训练语言模型预测被masked的标注\",{\"1\":{\"31\":1}}],[\"要么失败\",{\"1\":{\"191\":1}}],[\"要么成功\",{\"1\":{\"191\":1}}],[\"要么好到可以忽略不计\",{\"1\":{\"48\":1}}],[\"要么稍微好一点\",{\"1\":{\"48\":1}}],[\"要么更好\",{\"1\":{\"48\":1}}],[\"要么他们的选择明显更好\",{\"1\":{\"48\":1}}],[\"作为\",{\"1\":{\"236\":1}}],[\"作为模型的输入\",{\"1\":{\"79\":1}}],[\"作为编码器和解码器\",{\"1\":{\"16\":1}}],[\"作者堆叠多尺度缩放retention\",{\"1\":{\"79\":1}}],[\"作者用groupnorm的尺度不变性来提高retention\",{\"1\":{\"78\":1}}],[\"作者引入了倾斜门\",{\"1\":{\"78\":1}}],[\"作者对每个本地块进行并行编码以提高计算速度\",{\"1\":{\"75\":1}}],[\"作者对模型中的k个输出进行采样\",{\"1\":{\"50\":1}}],[\"作者对模型进行了2个epochs的微调\",{\"1\":{\"46\":1}}],[\"作者提出了多尺度保留机制\",{\"1\":{\"75\":1}}],[\"作者提出了ghost\",{\"1\":{\"51\":1}}],[\"作者也会在一半的时间内修改原始指令\",{\"1\":{\"51\":1}}],[\"作者通过随机组合上述约束来构建最终指令\",{\"1\":{\"51\":1}}],[\"作者通过两个主要的算法来探索rlhf微调\",{\"1\":{\"50\":1}}],[\"作者要求人类评估人员对其有用性和安全性进行评分\",{\"1\":{\"54\":1}}],[\"作者要求llama\",{\"1\":{\"51\":1}}],[\"作者要求注释器首先编写一个提示\",{\"1\":{\"48\":1}}],[\"作者创建了一些综合约束条件\",{\"1\":{\"51\":1}}],[\"作者能够通过在生成之前将模型权重合并到每个节点一次\",{\"1\":{\"50\":1}}],[\"作者为所有模型进行了200到400次迭代的训练\",{\"1\":{\"50\":1}}],[\"作者为每个样本打分\",{\"1\":{\"50\":1}}],[\"作者报告了llama\",{\"1\":{\"50\":1}}],[\"作者从最新的模型中对每个prompt采样k个答案\",{\"1\":{\"50\":1}}],[\"作者只对最大的70b\",{\"1\":{\"50\":1}}],[\"作者只对回答令牌进行反向传播\",{\"1\":{\"46\":1}}],[\"作者在等式\",{\"1\":{\"78\":1}}],[\"作者在每个layer中使用h=dmodel​\",{\"1\":{\"78\":1}}],[\"作者在下图中显示了模型的最大注意力激活\",{\"1\":{\"51\":1}}],[\"作者在图7中说明了拒绝采样的好处\",{\"1\":{\"50\":1}}],[\"作者在给定模型的初始策略的情况下对所有输出进行采样\",{\"1\":{\"50\":1}}],[\"作者在损失中进一步添加了一个margin成分\",{\"1\":{\"49\":1}}],[\"作者更进一步\",{\"1\":{\"50\":1}}],[\"作者强调\",{\"1\":{\"49\":1}}],[\"作者进一步修改它\",{\"1\":{\"49\":1}}],[\"作者使用了循环模式\",{\"1\":{\"79\":1}}],[\"作者使用了平行模式\",{\"1\":{\"79\":1}}],[\"作者使用了二元排名损失\",{\"1\":{\"49\":1}}],[\"作者使用chat\",{\"1\":{\"54\":1}}],[\"作者使用gpt\",{\"1\":{\"53\":1,\"54\":1}}],[\"作者使用pytorch\",{\"1\":{\"50\":1}}],[\"作者使用adamw优化器\",{\"1\":{\"50\":1}}],[\"作者使用自回归目标\",{\"1\":{\"46\":1}}],[\"作者将llama\",{\"1\":{\"54\":1}}],[\"作者将相应测试集的所有提示的并集分别称为\",{\"1\":{\"49\":1}}],[\"作者将收集的成对人类偏好数据转换为二元排名标签格式\",{\"1\":{\"49\":1}}],[\"作者将训练集中的所有提示和答案连接起来\",{\"1\":{\"46\":1}}],[\"作者还要求注释者标注他们更喜欢自己选择的回答而不是选择的程度\",{\"1\":{\"48\":1}}],[\"作者的注释过程如下\",{\"1\":{\"48\":1}}],[\"作者选择了二进制比较协议\",{\"1\":{\"48\":1}}],[\"权重衰减=0\",{\"1\":{\"46\":1}}],[\"万级别的好的数据足够了\",{\"1\":{\"46\":1}}],[\"万亿模型都不足为奇\",{\"1\":{\"14\":1}}],[\"公里内的车辆信息\",{\"1\":{\"244\":1}}],[\"公式6\",{\"1\":{\"79\":1}}],[\"公式7\",{\"1\":{\"79\":1}}],[\"公式5\",{\"1\":{\"79\":1}}],[\"公式\",{\"1\":{\"77\":2}}],[\"公式如下\",{\"1\":{\"18\":2,\"27\":1}}],[\"公开的指令微调数据质量参差不齐\",{\"1\":{\"46\":1}}],[\"开源python库\",{\"1\":{\"70\":1}}],[\"开源\",{\"1\":{\"56\":1,\"57\":1,\"58\":1,\"59\":1}}],[\"开始的\",{\"1\":{\"235\":1}}],[\"开始\",{\"1\":{\"46\":1}}],[\"开发者做了以下努力\",{\"1\":{\"3\":1}}],[\"评测结果如下\",{\"1\":{\"44\":1}}],[\"评测\",{\"0\":{\"44\":1}}],[\"算法的主要不同\",{\"1\":{\"50\":1}}],[\"算法\",{\"1\":{\"43\":1}}],[\"余弦学习率\",{\"1\":{\"42\":1}}],[\"β2=0\",{\"1\":{\"42\":1,\"50\":1}}],[\"β1=0\",{\"1\":{\"42\":1}}],[\"额外空间的条件下完成\",{\"1\":{\"249\":1}}],[\"额外空间并\",{\"1\":{\"248\":1}}],[\"额外发现\",{\"1\":{\"41\":1}}],[\"额外的预训练信息之与llama\",{\"0\":{\"41\":1}}],[\"随时可能被回收\",{\"1\":{\"106\":1}}],[\"随着序列长度的增加\",{\"1\":{\"75\":1}}],[\"随着context\",{\"1\":{\"41\":1}}],[\"随机变为其他token\",{\"1\":{\"27\":1}}],[\"随机替换为其他token\",{\"1\":{\"22\":1}}],[\"理解更长的文本\",{\"1\":{\"41\":1}}],[\"旋转位置embedding\",{\"1\":{\"40\":1}}],[\"激活函数\",{\"1\":{\"40\":1}}],[\"激活函数等有变化\",{\"1\":{\"38\":1}}],[\"具体可表示为\",{\"1\":{\"76\":1}}],[\"具体来说\",{\"1\":{\"75\":1,\"77\":1,\"78\":1}}],[\"具体而言\",{\"1\":{\"38\":1}}],[\"具体地\",{\"1\":{\"22\":1}}],[\"归一化\",{\"1\":{\"38\":1,\"40\":1}}],[\"机制\",{\"1\":{\"37\":1}}],[\"qkv\",{\"1\":{\"77\":2}}],[\"qk\",{\"1\":{\"77\":4}}],[\"q\",{\"1\":{\"77\":7}}],[\"qn​einθ\",{\"1\":{\"77\":1}}],[\"qn​\",{\"1\":{\"77\":2}}],[\"q=xwq​\",{\"1\":{\"77\":1}}],[\"query\",{\"1\":{\"37\":1,\"41\":1}}],[\"qiu\",{\"1\":{\"33\":1}}],[\"qian\",{\"1\":{\"33\":2}}],[\"增加了gqa\",{\"1\":{\"40\":1}}],[\"增加了上下文长度\",{\"1\":{\"40\":1}}],[\"增加了一个segment\",{\"1\":{\"21\":1}}],[\"增加一倍\",{\"1\":{\"37\":1}}],[\"上面说的平衡二叉树每次都可能进行多次的左旋或者右旋\",{\"1\":{\"153\":1}}],[\"上面的等式称为\",{\"1\":{\"77\":2}}],[\"上面两个等式称为\",{\"1\":{\"77\":1}}],[\"上\",{\"1\":{\"77\":1}}],[\"上下文长度\",{\"1\":{\"37\":1,\"41\":1}}],[\"上述公式中\",{\"1\":{\"79\":1}}],[\"上述技巧在稳定正向和反向通道的数值流动的同时\",{\"1\":{\"78\":1}}],[\"上述我们介绍\",{\"1\":{\"32\":1}}],[\"上述我们介绍p\",{\"1\":{\"31\":1}}],[\"上述我们也提到\",{\"1\":{\"29\":1}}],[\"上述两个句子序列的词完全相同\",{\"1\":{\"18\":1}}],[\"共同关注\",{\"0\":{\"225\":1}}],[\"共享session\",{\"1\":{\"207\":1}}],[\"共享特性\",{\"1\":{\"7\":1}}],[\"共在2t\",{\"1\":{\"39\":1}}],[\"共有7b\",{\"1\":{\"37\":1}}],[\"介绍下具体如何使用\",{\"1\":{\"244\":1}}],[\"介绍一下如何使用redis实现一个分布式锁\",{\"0\":{\"198\":1}}],[\"介绍\",{\"0\":{\"37\":1,\"75\":1,\"250\":1}}],[\"论文链接\",{\"1\":{\"36\":1,\"50\":1}}],[\"论文笔记\",{\"0\":{\"36\":1,\"74\":1}}],[\"zinterstore\",{\"1\":{\"229\":1}}],[\"zincrby\",{\"1\":{\"229\":1}}],[\"zunionstore\",{\"1\":{\"229\":1}}],[\"zrevrangebylex\",{\"1\":{\"229\":1}}],[\"zrevrange\",{\"1\":{\"229\":1}}],[\"zrem\",{\"1\":{\"229\":1}}],[\"zrangebylex\",{\"1\":{\"229\":1}}],[\"zrangebyscore\",{\"1\":{\"229\":1}}],[\"zrange\",{\"1\":{\"229\":1}}],[\"zcard\",{\"1\":{\"229\":1}}],[\"zscore\",{\"1\":{\"229\":1}}],[\"zset有序集合\",{\"1\":{\"227\":1}}],[\"zset\",{\"0\":{\"227\":1},\"1\":{\"228\":3,\"229\":2}}],[\"zadd\",{\"1\":{\"229\":1}}],[\"zgc\",{\"0\":{\"118\":1},\"1\":{\"112\":1}}],[\"zettlemoyer\",{\"1\":{\"33\":1}}],[\"z\",{\"1\":{\"33\":1}}],[\"zh\",{\"1\":{\"60\":1}}],[\"zhihu\",{\"1\":{\"58\":1,\"59\":1,\"68\":1,\"70\":1,\"133\":1}}],[\"zhilin\",{\"1\":{\"33\":2}}],[\"zhuanlan\",{\"1\":{\"58\":1,\"59\":1,\"68\":1,\"70\":1,\"133\":1}}],[\"zhu\",{\"1\":{\"33\":1}}],[\"zheng\",{\"1\":{\"33\":1}}],[\"zhengxiao\",{\"1\":{\"33\":2}}],[\"zhou\",{\"1\":{\"33\":2}}],[\"zygisk方式进行获取\",{\"1\":{\"3\":1}}],[\"jerry\",{\"1\":{\"226\":4}}],[\"juejin\",{\"1\":{\"171\":1}}],[\"jdk介绍其用于hotspot\",{\"1\":{\"253\":1}}],[\"jdk17\",{\"1\":{\"253\":1}}],[\"jdk\",{\"1\":{\"121\":3}}],[\"jdk8之后方法区的实现方式\",{\"1\":{\"86\":1}}],[\"jvm引入了happens\",{\"1\":{\"129\":1}}],[\"jvm会进行指令重排\",{\"1\":{\"129\":1}}],[\"jvm会进行老年代空间大小确认\",{\"1\":{\"93\":1}}],[\"jvm内存结构包括\",{\"1\":{\"120\":1}}],[\"jvm内存结构\",{\"0\":{\"120\":1}}],[\"jvm内存结构和java内存模型\",{\"0\":{\"119\":1}}],[\"jvm中垃圾收集器数量众多\",{\"1\":{\"112\":1}}],[\"jvm的垃圾回收\",{\"1\":{\"85\":1}}],[\"jvm垃圾回收\",{\"0\":{\"85\":1}}],[\"john\",{\"1\":{\"226\":2}}],[\"joshi\",{\"1\":{\"33\":1}}],[\"jones\",{\"1\":{\"33\":1}}],[\"jianfeng\",{\"1\":{\"33\":1}}],[\"jingfei\",{\"1\":{\"33\":1}}],[\"jie\",{\"1\":{\"33\":2}}],[\"jiezhong\",{\"1\":{\"33\":1}}],[\"jmlr\",{\"1\":{\"33\":1}}],[\"j\",{\"1\":{\"33\":3,\"251\":1}}],[\"jacob\",{\"1\":{\"33\":1}}],[\"jakob\",{\"1\":{\"33\":2}}],[\"jar中提供\",{\"1\":{\"3\":1}}],[\"jar路径\",{\"1\":{\"3\":1}}],[\"java的反射用到了这里\",{\"1\":{\"134\":1}}],[\"java的数据类型分为基本类型和引用类型\",{\"1\":{\"133\":1}}],[\"java类的加载流程\",{\"0\":{\"133\":1}}],[\"java中使用volatile关键字标记的变量读写就不会被其他线程打断\",{\"1\":{\"130\":1}}],[\"java中有四大引用\",{\"1\":{\"101\":1}}],[\"java内存模型是给java多线程通信准备的\",{\"1\":{\"127\":1}}],[\"java内存模型jmm\",{\"0\":{\"127\":1}}],[\"java和rest\",{\"1\":{\"59\":1}}],[\"java实现多态有三个必要条件\",{\"1\":{\"10\":1}}],[\"java\",{\"0\":{\"256\":1},\"1\":{\"3\":2,\"72\":1,\"105\":2}}],[\"util\",{\"1\":{\"105\":2}}],[\"udp\",{\"1\":{\"82\":2,\"83\":1}}],[\"uid\",{\"1\":{\"235\":4}}],[\"ui是一个开源\",{\"1\":{\"64\":1}}],[\"ui或prefect\",{\"1\":{\"64\":1}}],[\"u1\",{\"1\":{\"51\":1}}],[\"up\",{\"1\":{\"49\":1}}],[\"userid\",{\"1\":{\"235\":1,\"236\":3}}],[\"use\",{\"1\":{\"49\":1}}],[\"using\",{\"1\":{\"41\":1}}],[\"uszkoreit\",{\"1\":{\"33\":2}}],[\"unsqueeze\",{\"1\":{\"77\":3}}],[\"un\",{\"1\":{\"51\":1}}],[\"understands\",{\"1\":{\"33\":1}}],[\"understanding\",{\"1\":{\"33\":2}}],[\"unilmv2\",{\"1\":{\"33\":1}}],[\"universally\",{\"1\":{\"33\":1}}],[\"unified\",{\"1\":{\"33\":2}}],[\"untrusted\",{\"1\":{\"3\":1}}],[\"虽然这里被声明为native方法\",{\"1\":{\"253\":1}}],[\"虽然所需设备与p\",{\"1\":{\"32\":1}}],[\"虽然把这个模型放到现在看起来规模不大\",{\"1\":{\"20\":1}}],[\"以字节byte为单位\",{\"1\":{\"233\":1}}],[\"以字节为单位\",{\"1\":{\"233\":1}}],[\"以用户id为key\",{\"1\":{\"219\":1}}],[\"以客户端发起的为例\",{\"1\":{\"187\":1}}],[\"以上三个和cms类似\",{\"1\":{\"117\":1}}],[\"以上的训练\",{\"1\":{\"38\":1}}],[\"以o\",{\"1\":{\"79\":1}}],[\"以增加retention\",{\"1\":{\"78\":1}}],[\"以增强奖励\",{\"1\":{\"50\":1}}],[\"以增强通用性\",{\"1\":{\"32\":1}}],[\"以下是其其他关键功能\",{\"1\":{\"72\":1}}],[\"以及任何规模\",{\"1\":{\"66\":1}}],[\"以及4000多个单回合和多回合提示的闭源模型\",{\"1\":{\"54\":1}}],[\"以促进构建\",{\"1\":{\"60\":1}}],[\"以标准化过程生产高性能模型的持续交付\",{\"1\":{\"55\":1}}],[\"以避免任何偏差\",{\"1\":{\"53\":1}}],[\"以避免教学和模型知识之间的不匹配\",{\"1\":{\"51\":1}}],[\"以评估哪一代是优选的\",{\"1\":{\"53\":1}}],[\"以帮助在多阶段过程中集中注意力\",{\"1\":{\"51\":1}}],[\"以提前停止\",{\"1\":{\"50\":1}}],[\"以收集新的数据集\",{\"1\":{\"50\":1}}],[\"以胜任多种自然语言处理任务\",{\"1\":{\"26\":1}}],[\"舍弃了词汇mapping的verbalizer的使用\",{\"1\":{\"32\":1}}],[\"基数统计就是指统计一个集合中不重复的元素个数\",{\"1\":{\"237\":1}}],[\"基于模型的评估\",{\"0\":{\"53\":1}}],[\"基于消融结果和易于缩放推断\",{\"1\":{\"41\":1}}],[\"基于多任务数据集的prompt进行预训练\",{\"1\":{\"32\":1}}],[\"基本操作\",{\"1\":{\"233\":1}}],[\"基本上不会产生冲突\",{\"1\":{\"150\":1}}],[\"基本上都采用了transformer或者基于transformer修改的模型\",{\"1\":{\"18\":1}}],[\"基本上都与transformer有着千丝万缕的联系\",{\"1\":{\"14\":1}}],[\"基本类型是通过jvm加载的\",{\"1\":{\"133\":1}}],[\"基本架构\",{\"1\":{\"74\":1}}],[\"基本已经可以确定的是\",{\"1\":{\"14\":1}}],[\"每张表上的索引最多不要大于5个\",{\"1\":{\"165\":1}}],[\"每张表上的索引不宜过多\",{\"0\":{\"165\":1}}],[\"每条路径上的黑色节点数量一定是相同的\",{\"1\":{\"153\":1}}],[\"每次使用只使用一块\",{\"1\":{\"109\":1}}],[\"每熬过一次垃圾回收\",{\"1\":{\"91\":1}}],[\"每个\",{\"1\":{\"237\":1}}],[\"每个消息都有一个全局的\",{\"1\":{\"213\":1}}],[\"每个都需要到达叶子节点\",{\"1\":{\"154\":1}}],[\"每个节点不是红色就是黑色的\",{\"1\":{\"153\":1}}],[\"每个节点的右孩子都比自己大\",{\"1\":{\"151\":1}}],[\"每个节点的左孩子都比自己小\",{\"1\":{\"151\":1}}],[\"每个值都需要进行一次查找\",{\"1\":{\"150\":1}}],[\"每个retnet\",{\"1\":{\"76\":1}}],[\"每个chunk都可以并行编码\",{\"1\":{\"74\":1}}],[\"每个kubeflow组件都被包装到一个容器中\",{\"1\":{\"58\":1}}],[\"每个图的左侧对应系统信息\",{\"1\":{\"51\":1}}],[\"每个样本都包含一个提示和一个答案\",{\"1\":{\"46\":1}}],[\"每一层transformer的embedding输入都需要被微调\",{\"1\":{\"32\":1}}],[\"每隔几秒钟就会重启一次\",{\"1\":{\"0\":1}}],[\"yr\",{\"1\":{\"49\":1}}],[\"yc\",{\"1\":{\"49\":1}}],[\"yu\",{\"1\":{\"33\":1}}],[\"yujie\",{\"1\":{\"33\":2}}],[\"yinhan\",{\"1\":{\"33\":1}}],[\"yih\",{\"1\":{\"33\":1}}],[\"yanan\",{\"1\":{\"33\":1}}],[\"yang\",{\"1\":{\"33\":3}}],[\"yanqi\",{\"1\":{\"33\":1}}],[\"young\",{\"1\":{\"92\":1}}],[\"you\",{\"1\":{\"33\":1}}],[\"y为输出\",{\"1\":{\"32\":1}}],[\"y\",{\"1\":{\"32\":1,\"33\":2,\"49\":2}}],[\"均取得和fine\",{\"1\":{\"32\":1}}],[\"固定语言模型\",{\"1\":{\"32\":1}}],[\"参与运算的key\",{\"1\":{\"233\":1}}],[\"参数指定要检测的范围\",{\"1\":{\"235\":1}}],[\"参数和\",{\"1\":{\"235\":1}}],[\"参数\",{\"1\":{\"32\":1,\"67\":1}}],[\"参数量\",{\"1\":{\"32\":1}}],[\"参考参考1参考2\",{\"1\":{\"119\":1}}],[\"参考资料\",{\"0\":{\"35\":1}}],[\"参考文献\",{\"0\":{\"33\":1}}],[\"参考\",{\"1\":{\"4\":1}}],[\"提示\",{\"1\":{\"245\":1,\"247\":1,\"248\":1,\"249\":1,\"251\":1}}],[\"提示微调完全不需要训练\",{\"1\":{\"31\":1}}],[\"提示微调冻结了预训练模型的所有参数\",{\"1\":{\"31\":1}}],[\"提示微调只用一个冻结的语言模型来微调连续的提示\",{\"1\":{\"31\":1}}],[\"提示微调\",{\"0\":{\"31\":1}}],[\"提供的\",{\"1\":{\"241\":1}}],[\"提供不精确的去重计数\",{\"1\":{\"237\":1}}],[\"提供了\",{\"1\":{\"235\":1,\"236\":1}}],[\"提供了一个端到端ml平台\",{\"1\":{\"61\":1}}],[\"提升了7倍的速度\",{\"1\":{\"75\":1}}],[\"提出了对序列模型的记忆力机制\",{\"1\":{\"74\":1}}],[\"提出一种新技术\",{\"1\":{\"45\":1}}],[\"提高了seq2seq的效率\",{\"1\":{\"18\":1}}],[\"受到了大家的关注\",{\"1\":{\"30\":1}}],[\"受到了业内人士和广大群众的注意\",{\"1\":{\"13\":1}}],[\"包括\",{\"1\":{\"155\":1}}],[\"包括类的方法和字段等等\",{\"1\":{\"134\":1}}],[\"包括对话模型\",{\"1\":{\"54\":1}}],[\"包括助手的消息\",{\"1\":{\"51\":1}}],[\"包括gpt\",{\"1\":{\"49\":1}}],[\"包括glm\",{\"1\":{\"40\":1}}],[\"包括指令微调和rlhf\",{\"1\":{\"45\":1}}],[\"包括p\",{\"1\":{\"30\":1}}],[\"包含了masked\",{\"1\":{\"25\":1}}],[\"研究者着手设计部分参数微调的方法\",{\"1\":{\"30\":1}}],[\"需要合并的数组是\",{\"1\":{\"251\":1}}],[\"需要合并\",{\"1\":{\"251\":2}}],[\"需要具备双指针的思想\",{\"1\":{\"249\":1}}],[\"需要在消息中包含这个全局唯一\",{\"1\":{\"213\":1}}],[\"需要\",{\"1\":{\"213\":1}}],[\"需要保证三个需求\",{\"1\":{\"211\":1}}],[\"需要注意什么\",{\"0\":{\"197\":1}}],[\"需要注意的是边界问题处理\",{\"1\":{\"252\":1}}],[\"需要注意的是\",{\"1\":{\"141\":1,\"235\":1}}],[\"需要删除\",{\"1\":{\"170\":1}}],[\"需要存放到磁盘中\",{\"1\":{\"148\":1}}],[\"需要满足三大条件\",{\"1\":{\"100\":1}}],[\"需要10台dgx\",{\"1\":{\"30\":1,\"32\":1}}],[\"需要给surface设置为in\",{\"1\":{\"3\":1}}],[\"都是一样的\",{\"1\":{\"236\":1}}],[\"都是在一个良好的预训练语言模型上进行微调\",{\"1\":{\"30\":1}}],[\"都会记录到aof文件中\",{\"1\":{\"196\":1}}],[\"都使用了b+树\",{\"1\":{\"154\":1}}],[\"都可能会进行多次的左旋或者右旋\",{\"1\":{\"152\":1}}],[\"都需要更变索引的内容\",{\"1\":{\"148\":1}}],[\"都需要大量的时间\",{\"1\":{\"148\":1}}],[\"都拿出1000个例子作为测试集来评估模型\",{\"1\":{\"49\":1}}],[\"都要基于多个句子之间的关系\",{\"1\":{\"22\":1}}],[\"训练\",{\"1\":{\"79\":1}}],[\"训练详情\",{\"0\":{\"40\":1}}],[\"训练数据\",{\"0\":{\"39\":1}}],[\"训练语言模型的成本是巨大的\",{\"1\":{\"30\":1}}],[\"训练和微调均进行了介绍\",{\"1\":{\"13\":1}}],[\"六\",{\"0\":{\"30\":1}}],[\"相对顺序\",{\"1\":{\"247\":1}}],[\"相同元素分值相加\",{\"1\":{\"229\":2}}],[\"相当于在set的基础上多一个score属性\",{\"1\":{\"227\":1}}],[\"相当于看到了未来的信息\",{\"1\":{\"18\":1}}],[\"相反\",{\"1\":{\"77\":1}}],[\"相关代码见https\",{\"1\":{\"74\":1}}],[\"相关链接\",{\"1\":{\"56\":1,\"57\":1,\"58\":4,\"59\":2,\"60\":1,\"62\":1,\"63\":1,\"64\":1,\"66\":1,\"67\":1,\"68\":3,\"69\":1,\"70\":2,\"71\":1,\"72\":1,\"73\":1}}],[\"相比于\",{\"1\":{\"229\":1}}],[\"相比\",{\"1\":{\"51\":1}}],[\"相比llama\",{\"1\":{\"37\":1}}],[\"相信在未来\",{\"1\":{\"29\":1}}],[\"重复消息的处理\",{\"1\":{\"211\":1}}],[\"重新标记\",{\"1\":{\"116\":1}}],[\"重新利用\",{\"1\":{\"32\":1}}],[\"重新调整了ln和残差连接的顺序\",{\"1\":{\"29\":1}}],[\"重写\",{\"1\":{\"10\":2}}],[\"表示这个key多久\",{\"1\":{\"207\":1}}],[\"表示共轭转置\",{\"1\":{\"77\":1}}],[\"表示非masked的词\",{\"1\":{\"28\":1}}],[\"表示词在parta中的位置\",{\"1\":{\"28\":1}}],[\"替代\",{\"1\":{\"27\":1}}],[\"替换\",{\"1\":{\"22\":1}}],[\"被\",{\"1\":{\"27\":1}}],[\"被引用对象的类型而不是引用变量的类型决定了调用谁的成员方法\",{\"1\":{\"12\":1}}],[\"玩笑\",{\"1\":{\"27\":1}}],[\"玩\",{\"1\":{\"27\":1}}],[\"中仅存的\",{\"1\":{\"251\":1}}],[\"中没有元素\",{\"1\":{\"251\":1}}],[\"中的元素\",{\"1\":{\"251\":1}}],[\"中的元素数目\",{\"1\":{\"251\":1}}],[\"中的前五个元素为\",{\"1\":{\"248\":1}}],[\"中的80\",{\"1\":{\"22\":1}}],[\"中出现的顺序排列\",{\"1\":{\"247\":1}}],[\"中唯一元素的个数\",{\"1\":{\"247\":1}}],[\"中元素权重分数的转换\",{\"1\":{\"241\":1}}],[\"中第一个值为\",{\"1\":{\"235\":1}}],[\"中最长的字符串长度相等\",{\"1\":{\"233\":1}}],[\"中实现了三个归一化因子\",{\"1\":{\"78\":1}}],[\"中可以看到\",{\"1\":{\"27\":1}}],[\"中\",{\"1\":{\"27\":1,\"121\":1,\"228\":1,\"236\":2,\"238\":1,\"241\":1,\"242\":1,\"251\":3}}],[\"~\",{\"1\":{\"233\":1}}],[\"~6~\",{\"1\":{\"27\":1}}],[\"~5~\",{\"1\":{\"27\":1}}],[\"~3~\",{\"1\":{\"27\":1}}],[\"~2~\",{\"1\":{\"27\":1}}],[\"~i\",{\"1\":{\"27\":2}}],[\"~都对应x中的连续token\",{\"1\":{\"27\":1}}],[\"~m~\",{\"1\":{\"27\":1}}],[\"~1~\",{\"1\":{\"27\":2}}],[\"~n~\",{\"1\":{\"27\":1}}],[\"xor\",{\"1\":{\"233\":1,\"236\":1}}],[\"x是32字节\",{\"1\":{\"205\":1}}],[\"xw1​\",{\"1\":{\"79\":1}}],[\"xl−1\",{\"1\":{\"76\":1}}],[\"xl=retnetl​\",{\"1\":{\"76\":1}}],[\"xdmodel​\",{\"1\":{\"76\":2}}],[\"x1​\",{\"1\":{\"76\":1,\"79\":1}}],[\"xi​\",{\"1\":{\"76\":1,\"79\":1}}],[\"xiang\",{\"1\":{\"33\":1}}],[\"xiaodong\",{\"1\":{\"33\":1}}],[\"xiao\",{\"1\":{\"33\":2}}],[\"x∣x∣​\",{\"1\":{\"76\":2,\"79\":1}}],[\"xuanjing\",{\"1\":{\"33\":1}}],[\"x为输入\",{\"1\":{\"32\":1}}],[\"x~6~\",{\"1\":{\"27\":1}}],[\"x~1~\",{\"1\":{\"27\":2}}],[\"x\",{\"1\":{\"27\":4,\"32\":1,\"33\":1,\"49\":2,\"78\":1,\"79\":1}}],[\"解释\",{\"1\":{\"247\":2,\"248\":2,\"249\":2,\"251\":3}}],[\"解释和优化机器学习模型和实验的平台\",{\"1\":{\"62\":1}}],[\"解答\",{\"0\":{\"246\":1,\"252\":1}}],[\"解析阶段需要做的就是把符号引用解析为直接引用\",{\"1\":{\"142\":1}}],[\"解析合称为链接部分\",{\"1\":{\"133\":1}}],[\"解析\",{\"0\":{\"142\":1},\"1\":{\"133\":1}}],[\"解码模型\",{\"1\":{\"26\":1}}],[\"解决过程\",{\"0\":{\"1\":1}}],[\"编码方法实现了经纬度到\",{\"1\":{\"241\":1}}],[\"编码\",{\"1\":{\"26\":1}}],[\"编写aidl文件时\",{\"1\":{\"3\":1}}],[\"五种基本类型\",{\"0\":{\"200\":1}}],[\"五\",{\"0\":{\"26\":1}}],[\"更改数组\",{\"1\":{\"247\":1}}],[\"更新消耗小\",{\"1\":{\"158\":1}}],[\"更新代价大\",{\"1\":{\"157\":1}}],[\"更新了数据混合\",{\"1\":{\"38\":1}}],[\"更加关注吞吐量\",{\"1\":{\"115\":1}}],[\"更快\",{\"1\":{\"68\":1}}],[\"更明显的响应的准确性最为重要\",{\"1\":{\"49\":1}}],[\"更多的样本\",{\"1\":{\"50\":1}}],[\"更多的总结任务\",{\"1\":{\"41\":1}}],[\"更多内容可以阅读原论文\",{\"1\":{\"24\":1}}],[\"更长的上下文长度可以让模型处理更多信息\",{\"1\":{\"41\":1}}],[\"更准确地说\",{\"1\":{\"31\":1}}],[\"更是拥有了理解图像的能力\",{\"1\":{\"25\":1}}],[\"能够实现具有o\",{\"1\":{\"75\":1}}],[\"能够大大的提高开发的效率\",{\"1\":{\"7\":1}}],[\"能力进一步增强\",{\"1\":{\"25\":1}}],[\"转而采用transformer结构\",{\"1\":{\"25\":1}}],[\"来判断当前收到的消息有没有经过处理\",{\"1\":{\"213\":1}}],[\"来判断字符串是否结束\",{\"1\":{\"204\":1}}],[\"来源3\",{\"1\":{\"189\":1}}],[\"来源2\",{\"1\":{\"189\":1}}],[\"来说\",{\"1\":{\"161\":1}}],[\"来得到类模板对象\",{\"1\":{\"134\":1}}],[\"来进行计算\",{\"1\":{\"77\":1}}],[\"来实现机器学习从数据到模型的一整套端到端的过程\",{\"1\":{\"58\":1}}],[\"来执行\",{\"1\":{\"57\":1}}],[\"来提高llama\",{\"1\":{\"38\":1}}],[\"来讨论gpt的基本结构\",{\"1\":{\"25\":1}}],[\"来计算概率的\",{\"1\":{\"22\":1}}],[\"本身并没有设计新的底层数据结构\",{\"1\":{\"241\":1}}],[\"本身没有任何功能\",{\"1\":{\"58\":1}}],[\"本地方法栈也是线程私有的\",{\"1\":{\"125\":1}}],[\"本地方法栈\",{\"0\":{\"125\":1},\"1\":{\"120\":1}}],[\"本地方法栈中引用的对象\",{\"1\":{\"97\":1}}],[\"本地托管的编排引擎和api服务器\",{\"1\":{\"64\":1}}],[\"本节报告了使用监督微调以及初始和迭代奖励建模和rlhf进行的实验和发现\",{\"1\":{\"45\":1}}],[\"本文介绍llama\",{\"1\":{\"37\":2}}],[\"本文对t5模型的介绍就到这里\",{\"1\":{\"24\":1}}],[\"本篇笔记对其内容进行简要记录\",{\"1\":{\"36\":1}}],[\"本篇文章将回到最初的gpt\",{\"1\":{\"25\":1}}],[\"本篇文章将顺着现代自然语言处理方法和模型的脉络\",{\"1\":{\"13\":1}}],[\"本篇文章中将简单对其进行介绍\",{\"1\":{\"23\":1}}],[\"本篇文章会对上述模型和技术进行简要介绍\",{\"1\":{\"14\":1}}],[\"本篇文章选择了清华大学的p\",{\"1\":{\"14\":1}}],[\"本篇文章对t5和gpt进行简要介绍\",{\"1\":{\"13\":1}}],[\"本篇文章对现代语言模型的学习的全过程\",{\"1\":{\"13\":1}}],[\"同上\",{\"1\":{\"158\":1}}],[\"同时我们最好给\",{\"1\":{\"236\":1}}],[\"同时\",{\"1\":{\"207\":1}}],[\"同时随机生成一个序列号seq=j\",{\"1\":{\"184\":1}}],[\"同时随机生成一个序列号seq=i\",{\"1\":{\"184\":1}}],[\"同时进行垃圾回收\",{\"1\":{\"116\":1}}],[\"同时进行可达性分析\",{\"1\":{\"116\":1}}],[\"同时递归地编码全局块以节省gpu内存\",{\"1\":{\"75\":1}}],[\"同时实现了平行训练\",{\"1\":{\"74\":1}}],[\"同时在继承关系中\",{\"1\":{\"7\":1}}],[\"同样\",{\"1\":{\"25\":1,\"26\":1}}],[\"同样的\",{\"1\":{\"236\":1}}],[\"同样的解码过程来完成所有的nlp任务\",{\"1\":{\"24\":1}}],[\"同样的训练过程\",{\"1\":{\"24\":1}}],[\"同样的损失函数\",{\"1\":{\"24\":1}}],[\"给你两个按\",{\"1\":{\"251\":1}}],[\"给你一个有序数组\",{\"1\":{\"249\":1}}],[\"给你一个数组\",{\"1\":{\"248\":1}}],[\"给你一个\",{\"1\":{\"247\":1}}],[\"给伊甸园腾出一些空间\",{\"1\":{\"89\":1}}],[\"给定一个大小为\",{\"1\":{\"245\":1}}],[\"给定一个输入序列x=x1​\",{\"1\":{\"76\":1}}],[\"给定x\",{\"1\":{\"78\":1}}],[\"给定输入x∈r∣x∣×dmodel​\",{\"1\":{\"77\":1}}],[\"给出实验时可访问的最佳奖励模型\",{\"1\":{\"50\":1}}],[\"给我从英语翻译成汉语\",{\"1\":{\"24\":1}}],[\"给远程服务设置surface不生效\",{\"1\":{\"3\":1}}],[\"任何语言\",{\"1\":{\"66\":1}}],[\"任何两个时间步之间的距离应该保持一致\",{\"1\":{\"18\":1}}],[\"任务可以根据有向无环图\",{\"1\":{\"57\":1}}],[\"任务上\",{\"1\":{\"32\":1}}],[\"任务\",{\"1\":{\"24\":1}}],[\"任务都转化为文本到文本\",{\"1\":{\"24\":1}}],[\"四种常见类型\",{\"0\":{\"201\":1}}],[\"四次挥手过程\",{\"0\":{\"187\":1}}],[\"四次挥手\",{\"0\":{\"186\":1}}],[\"四个基于transformer的预训练语言模型也已经介绍完毕\",{\"1\":{\"29\":1}}],[\"四\",{\"0\":{\"23\":1}}],[\"可用于数据库\",{\"1\":{\"190\":1}}],[\"可用内存减半\",{\"1\":{\"110\":1}}],[\"可见性\",{\"0\":{\"128\":1},\"1\":{\"127\":1}}],[\"可预测的停顿\",{\"1\":{\"117\":1}}],[\"可达性分析\",{\"0\":{\"96\":1},\"1\":{\"96\":1,\"98\":1}}],[\"可靠的\",{\"1\":{\"83\":1}}],[\"可视化和运行机器学习管道\",{\"1\":{\"67\":1}}],[\"可对数据\",{\"1\":{\"67\":1}}],[\"可让您可视化流程\",{\"1\":{\"64\":1}}],[\"可帮助您管理机器学习生命周期的核心部分\",{\"1\":{\"59\":1}}],[\"可移植和可扩展\",{\"1\":{\"58\":1}}],[\"可微调的参数多了\",{\"1\":{\"32\":1}}],[\"可能觉得稍微困难\",{\"1\":{\"249\":1}}],[\"可能会造成redis的短暂卡顿甚至缓存雪崩\",{\"1\":{\"197\":1}}],[\"可能会有一个问题\",{\"1\":{\"22\":1}}],[\"可能效果会更好一些\",{\"1\":{\"27\":1}}],[\"可能差别很大\",{\"1\":{\"27\":1}}],[\"可能是因为在中文中\",{\"1\":{\"27\":1}}],[\"可以将一个或多个经度\",{\"1\":{\"242\":1}}],[\"可以是\",{\"1\":{\"236\":1}}],[\"可以有多个\",{\"1\":{\"233\":1}}],[\"可以通过偏移量来定位元素\",{\"1\":{\"231\":1}}],[\"可以通过multi和exec指令包起来\",{\"1\":{\"191\":1}}],[\"可以用0表示未签到\",{\"1\":{\"234\":1}}],[\"可以用来表示某个元素的值或者状态\",{\"1\":{\"231\":1}}],[\"可以用来做高速缓存\",{\"1\":{\"103\":1}}],[\"可以用set的交集来做共同关注功能\",{\"1\":{\"225\":1}}],[\"可以参考这篇文章\",{\"1\":{\"209\":1}}],[\"可以参考新华字典里的目录\",{\"1\":{\"147\":1}}],[\"可以进行头插和尾插\",{\"1\":{\"208\":1}}],[\"可以防止上锁后忘记解锁\",{\"1\":{\"207\":1}}],[\"可以实现\",{\"1\":{\"207\":1}}],[\"可以更好地利用cpu缓存提升性能\",{\"1\":{\"206\":1}}],[\"可以删除一些误操作的命令\",{\"1\":{\"196\":1}}],[\"可以表示客户端到服务端已经不再进行通信了\",{\"1\":{\"187\":1}}],[\"可以确认客户端到服务端通信是正常的\",{\"1\":{\"185\":1}}],[\"可以使用\",{\"1\":{\"226\":2}}],[\"可以使用set\",{\"1\":{\"198\":1}}],[\"可以使用setnx\",{\"1\":{\"198\":1}}],[\"可以使用索引\",{\"1\":{\"173\":1}}],[\"可以使用具有单个kv投影的原始多查询格式\",{\"1\":{\"41\":1}}],[\"可以显著降低部署成本和延迟\",{\"1\":{\"75\":1}}],[\"可以在list中没有数据时进行阻塞\",{\"1\":{\"212\":1}}],[\"可以在该数据集上微调llama\",{\"1\":{\"51\":1}}],[\"可以在除第一个回合外的所有回合中放弃它\",{\"1\":{\"51\":1}}],[\"可以在类似于拒绝采样的过程中对模型进行微调\",{\"1\":{\"51\":1}}],[\"可以在多个头之间共享key和value预测\",{\"1\":{\"41\":1}}],[\"可以观察到\",{\"1\":{\"50\":1}}],[\"可以适配到序列标注任务\",{\"1\":{\"32\":1}}],[\"可以让行唯一\",{\"1\":{\"148\":1}}],[\"可以让查找效率变得很高\",{\"1\":{\"148\":1}}],[\"可以让微调后的预训练模型在处理下游任务时得到良好的效果\",{\"1\":{\"30\":1}}],[\"可以让模型去关注不同方面的信息\",{\"1\":{\"18\":1}}],[\"可以体现出bert是双向模型\",{\"1\":{\"22\":1}}],[\"可以看出\",{\"1\":{\"20\":1}}],[\"可以看到该方法有一个注解\",{\"1\":{\"253\":1}}],[\"可以看到这是一个native方法\",{\"1\":{\"253\":1}}],[\"可以看到我们给定的文本序列为\",{\"1\":{\"27\":1}}],[\"可以看到\",{\"1\":{\"18\":1}}],[\"可以对成员进行更精准的控制\",{\"1\":{\"6\":1}}],[\"全称retentive\",{\"1\":{\"74\":1}}],[\"全面\",{\"1\":{\"58\":1}}],[\"全面工具集\",{\"0\":{\"58\":1}}],[\"全参数微调效果相对较好\",{\"1\":{\"30\":1}}],[\"全部把输入的token替换成\",{\"1\":{\"22\":1}}],[\"全能模型\",{\"1\":{\"14\":1}}],[\"证明了15\",{\"1\":{\"22\":1}}],[\"既包括token左边的\",{\"1\":{\"22\":1}}],[\"坏\",{\"1\":{\"22\":1,\"31\":1}}],[\"还得回到队列中等待执行\",{\"1\":{\"214\":1}}],[\"还可以用来统计多个集合的交集\",{\"1\":{\"223\":1}}],[\"还可以存储图片\",{\"1\":{\"204\":1}}],[\"还可以保存二进制数据\",{\"1\":{\"204\":1}}],[\"还有新的一些数据类型\",{\"1\":{\"193\":1}}],[\"还注重了高吞吐量\",{\"1\":{\"117\":1}}],[\"还要进行一次筛选\",{\"1\":{\"98\":1}}],[\"还提到了一个工具\",{\"1\":{\"71\":1}}],[\"还是\",{\"1\":{\"22\":1}}],[\"还拥有自己独有得特性\",{\"1\":{\"7\":1}}],[\"好\",{\"1\":{\"22\":1,\"31\":1}}],[\"遮住之后\",{\"1\":{\"22\":1}}],[\"呀\",{\"1\":{\"22\":1}}],[\"今天天气真\",{\"1\":{\"22\":1}}],[\"今天天气真好呀\",{\"1\":{\"22\":1}}],[\"预训练tokenizer\",{\"0\":{\"43\":1}}],[\"预训练超参设置\",{\"0\":{\"42\":1}}],[\"预训练的设置和模型架构和llama\",{\"1\":{\"40\":1}}],[\"预训练方法\",{\"0\":{\"38\":1}}],[\"预训练模型llama\",{\"1\":{\"37\":1}}],[\"预训练模型基本上属于这个范畴\",{\"1\":{\"24\":1}}],[\"预训练语言模型的方式只有全参数微调\",{\"1\":{\"30\":1}}],[\"预训练语言模型可以分为三种\",{\"1\":{\"26\":1}}],[\"预训练bert\",{\"0\":{\"22\":1}}],[\"预测\",{\"1\":{\"18\":1}}],[\"图像\",{\"1\":{\"66\":1}}],[\"图11报告了作者针对安全和帮助轴的不同sft和rlhf版本的进展\",{\"1\":{\"53\":1}}],[\"图6\",{\"1\":{\"32\":2}}],[\"图中含有色块部分\",{\"1\":{\"27\":1}}],[\"图5\",{\"1\":{\"27\":2,\"28\":1}}],[\"图4\",{\"1\":{\"24\":1,\"25\":1}}],[\"图3\",{\"1\":{\"20\":1,\"21\":1}}],[\"图2\",{\"1\":{\"16\":1,\"17\":2,\"18\":3}}],[\"就用区间的编码值来表示\",{\"1\":{\"241\":1}}],[\"就非常适合应用在\",{\"1\":{\"240\":1}}],[\"就非常节省空间\",{\"1\":{\"237\":1}}],[\"就说明该用户\",{\"1\":{\"236\":1}}],[\"就从list中消失了\",{\"1\":{\"215\":1}}],[\"就把一条全局\",{\"1\":{\"213\":1}}],[\"就只有quicklist了\",{\"1\":{\"209\":1}}],[\"就没有session信息了\",{\"1\":{\"207\":1}}],[\"就会重发这个报文\",{\"1\":{\"188\":1}}],[\"就会一直处于连接状态\",{\"1\":{\"188\":1}}],[\"就会将它回收掉\",{\"1\":{\"104\":1}}],[\"就不走索引了\",{\"1\":{\"177\":1}}],[\"就不需要进行回表了\",{\"1\":{\"159\":1}}],[\"就要进行多次左旋或者右旋\",{\"1\":{\"152\":1}}],[\"就像git一样\",{\"1\":{\"66\":1}}],[\"就是一个双指针问题\",{\"1\":{\"249\":1}}],[\"就是一个消息执行了\",{\"1\":{\"214\":1}}],[\"就是一个线程对某个变量进行操作\",{\"1\":{\"128\":1}}],[\"就是如果list中没有数据\",{\"1\":{\"212\":1}}],[\"就是不走索引的\",{\"1\":{\"161\":1}}],[\"就是在使用联合索引时\",{\"1\":{\"161\":1}}],[\"就是在进行minor\",{\"1\":{\"93\":1}}],[\"就是把多列合在一起共同作为一个索引\",{\"1\":{\"160\":1}}],[\"就是把class字节码中的内容加载到内存当中去\",{\"1\":{\"133\":1}}],[\"就是准备查找的列恰好都是有索引的列\",{\"1\":{\"160\":1}}],[\"就是指的索引和数据是在一起的\",{\"1\":{\"157\":1}}],[\"就是两种索引\",{\"1\":{\"147\":1}}],[\"就是为类中的静态变量分配内存空间\",{\"1\":{\"141\":1}}],[\"就是serial的多线程版本\",{\"1\":{\"114\":1}}],[\"就是从一系列被称之为\",{\"1\":{\"96\":1}}],[\"就是没用办法解决循环引用\",{\"1\":{\"95\":1}}],[\"就是给个输入\",{\"1\":{\"48\":1}}],[\"就是transformer的encoder部分\",{\"1\":{\"20\":1}}],[\"就可以把\",{\"1\":{\"244\":1}}],[\"就可以计算接近2^64个不同元素的基数\",{\"1\":{\"237\":1}}],[\"就可以按照下面的步骤进行操作\",{\"1\":{\"235\":1}}],[\"就可以在rlhf中优化模型了\",{\"1\":{\"49\":1}}],[\"就可以用同样的模型\",{\"1\":{\"24\":1}}],[\"就可以将nlp任务都转成text\",{\"1\":{\"24\":1}}],[\"就可以调用子类中新添加而超类没有的方法了\",{\"1\":{\"12\":1}}],[\"就可以让引用变量绑定到各种不同的类实现上\",{\"1\":{\"9\":1}}],[\"自动化机器学习\",{\"0\":{\"73\":1},\"1\":{\"73\":1}}],[\"自然也没有原生的表示位置信息的能力\",{\"1\":{\"28\":1}}],[\"自然语言处理中常见的翻译任务\",{\"1\":{\"24\":1}}],[\"自然语言推理等\",{\"1\":{\"22\":1}}],[\"自编码模型\",{\"1\":{\"26\":1}}],[\"自回归解码的标准做法是缓存序列中先前token的key\",{\"1\":{\"41\":1}}],[\"自回归填空\",{\"0\":{\"27\":1},\"1\":{\"27\":1}}],[\"自回归模型\",{\"1\":{\"26\":1}}],[\"自回归模型对自然语言生成有着天然的优势\",{\"1\":{\"25\":1}}],[\"自回归语言模型更加适合自然语言生成任务\",{\"1\":{\"19\":1}}],[\"自注意力机制示例\",{\"1\":{\"17\":1}}],[\"自注意力机制正符合这一点\",{\"1\":{\"17\":1}}],[\"自注意力机制得到了应用\",{\"1\":{\"17\":1}}],[\"自注意力机制是注意力机制的一种\",{\"1\":{\"17\":1}}],[\"或运算\",{\"1\":{\"233\":1}}],[\"或者把数据返回给客户端\",{\"1\":{\"223\":1}}],[\"或者说单向模型和双向模型谁好谁坏\",{\"1\":{\"19\":1}}],[\"或公众人物\",{\"1\":{\"51\":1}}],[\"或\",{\"1\":{\"51\":1,\"248\":1}}],[\"或具有8kv投影的分组查询注意力\",{\"1\":{\"41\":1}}],[\"或卷积神经网络\",{\"1\":{\"16\":1}}],[\"组织模型看到将要预测的信息\",{\"1\":{\"18\":1}}],[\"单向\",{\"1\":{\"18\":1}}],[\"与操作\",{\"1\":{\"236\":1}}],[\"与运算\",{\"1\":{\"233\":1}}],[\"与自注意力类似\",{\"1\":{\"77\":1}}],[\"与transformer相似\",{\"1\":{\"76\":1}}],[\"与transformer和bert的一维位置编码不同\",{\"1\":{\"28\":1}}],[\"与wandb类似\",{\"1\":{\"71\":1}}],[\"与没有gatt的模型\",{\"1\":{\"51\":1}}],[\"与样本在训练时间不匹配\",{\"1\":{\"51\":1}}],[\"与discriminative\",{\"1\":{\"50\":1}}],[\"与相似对相比\",{\"1\":{\"49\":1}}],[\"与其他方案相比\",{\"1\":{\"48\":1}}],[\"与多头注意力\",{\"1\":{\"41\":1}}],[\"与llama\",{\"1\":{\"40\":1,\"43\":1}}],[\"与glm一样\",{\"1\":{\"32\":1}}],[\"与\",{\"1\":{\"31\":1,\"236\":1}}],[\"与bert不同的是\",{\"1\":{\"25\":1}}],[\"与encoder一致\",{\"1\":{\"18\":1}}],[\"与传统注意力机制完全相同\",{\"1\":{\"17\":1}}],[\"第四次挥手\",{\"1\":{\"187\":1}}],[\"第二步\",{\"1\":{\"235\":1}}],[\"第二次挥手\",{\"1\":{\"187\":1}}],[\"第二次握手\",{\"1\":{\"184\":1,\"185\":1}}],[\"第二\",{\"1\":{\"78\":1}}],[\"第二层为一个线性变换\",{\"1\":{\"18\":1}}],[\"第一步\",{\"1\":{\"235\":1}}],[\"第一次挥手\",{\"1\":{\"187\":1}}],[\"第一次握手\",{\"1\":{\"184\":1,\"185\":1}}],[\"第一次判断不可达后\",{\"1\":{\"98\":1}}],[\"第一个步骤就是加载\",{\"1\":{\"134\":1}}],[\"第一\",{\"1\":{\"78\":1}}],[\"第一层有一个relu激活函数\",{\"1\":{\"18\":1}}],[\"第三步\",{\"1\":{\"235\":1}}],[\"第三次挥手\",{\"1\":{\"187\":1}}],[\"第三次握手\",{\"1\":{\"184\":1,\"185\":1}}],[\"第三\",{\"1\":{\"75\":1,\"78\":1}}],[\"形成多个子空间\",{\"1\":{\"18\":1}}],[\"多数元素是指在数组中出现次数\",{\"1\":{\"245\":1}}],[\"多数元素\",{\"0\":{\"245\":1}}],[\"多个操作也支持原子性\",{\"1\":{\"191\":1}}],[\"多尺度保留\",{\"1\":{\"78\":1}}],[\"多尺度保留模块\",{\"1\":{\"76\":1}}],[\"多框架的模型服务和监控工具\",{\"1\":{\"69\":1}}],[\"多轮一致性的指令\",{\"0\":{\"51\":1}}],[\"多了一个segment\",{\"1\":{\"20\":1}}],[\"多头注意力机制\",{\"1\":{\"18\":1}}],[\"多头注意力机制的公式如下\",{\"1\":{\"18\":1}}],[\"多头注意力机制就是将缩放点积注意力机制的过程做h次\",{\"1\":{\"18\":1}}],[\"多头注意力机制如图2\",{\"1\":{\"18\":1}}],[\"多态的实现条件\",{\"0\":{\"10\":1}}],[\"多态分为编译时多态和运行时多态\",{\"1\":{\"9\":1}}],[\"多态\",{\"0\":{\"4\":1,\"9\":1}}],[\"缩放点击注意力机制会进行一个缩放\",{\"1\":{\"18\":1}}],[\"缩放点积注意力机制在做完query和key的点积之后\",{\"1\":{\"18\":1}}],[\"会根据输入的用户的经纬度信息\",{\"1\":{\"244\":1}}],[\"会存储在该key中\",{\"1\":{\"233\":1}}],[\"会使用跳表作为\",{\"1\":{\"228\":1}}],[\"会使用压缩列表作为\",{\"1\":{\"228\":1}}],[\"会使用哈希表\",{\"1\":{\"217\":1}}],[\"会导致\",{\"1\":{\"223\":1}}],[\"会导致query和key的点积非常大\",{\"1\":{\"18\":1}}],[\"会在某个时间点将数据库中的所有键值对写到一个文件中\",{\"1\":{\"196\":1}}],[\"会发送一个报文\",{\"1\":{\"188\":1}}],[\"会将ack设置为i+1\",{\"1\":{\"187\":1}}],[\"会将syn标志位设为1\",{\"1\":{\"184\":1}}],[\"会话层\",{\"1\":{\"181\":1}}],[\"会退化成o\",{\"1\":{\"151\":1}}],[\"会把仍然需要存活的对象复制到另一块内存区域\",{\"1\":{\"109\":1}}],[\"会对没有引用的对象进行标记\",{\"1\":{\"109\":1}}],[\"会直接分配到老年代\",{\"1\":{\"90\":1}}],[\"会考虑进行一次垃圾回收\",{\"1\":{\"89\":1}}],[\"会有更多更好的语言模型诞生\",{\"1\":{\"29\":1}}],[\"会进行一个缩放\",{\"1\":{\"18\":1}}],[\"会计算残差和\",{\"1\":{\"18\":1}}],[\"构成\",{\"1\":{\"18\":2}}],[\"由客户端来完成聚合统计\",{\"1\":{\"223\":1}}],[\"由于直接调用jvm内部实现\",{\"1\":{\"253\":1}}],[\"由于尺度不变的性质\",{\"1\":{\"78\":1}}],[\"由于作者应用了迭代模型更新\",{\"1\":{\"50\":1}}],[\"由于注释者的主观性和他们对可能区分反应的细微细节的依赖\",{\"1\":{\"49\":1}}],[\"由于bert的主要目的是构建一个通用的预训练模型\",{\"1\":{\"21\":1}}],[\"由\",{\"1\":{\"32\":1}}],[\"由一个多头注意力机制\",{\"1\":{\"18\":1}}],[\"由query和key做向量比对\",{\"1\":{\"17\":1}}],[\"最多可以存储2^32\",{\"1\":{\"220\":1}}],[\"最多支持4k\",{\"1\":{\"37\":1}}],[\"最左前缀匹配\",{\"0\":{\"172\":1}}],[\"最左前缀匹配原则\",{\"0\":{\"161\":1},\"1\":{\"161\":1}}],[\"最好的部分是它可以与各种机器学习框架一起使用\",{\"1\":{\"68\":1}}],[\"最终\",{\"1\":{\"251\":1}}],[\"最终标记\",{\"1\":{\"117\":1}}],[\"最终希望能够实现一套完整可用的流水线\",{\"1\":{\"58\":1}}],[\"最终的input\",{\"1\":{\"21\":1}}],[\"最佳温度为t∈\",{\"1\":{\"50\":1}}],[\"最佳温度不是恒定的\",{\"1\":{\"50\":1}}],[\"最大的llama\",{\"1\":{\"54\":1}}],[\"最大的glm参数量已经达到了130b\",{\"1\":{\"14\":1}}],[\"最大曲线和中值曲线之间的增量可以解释为对最佳输出进行微调的潜在增益\",{\"1\":{\"50\":1}}],[\"最少5\",{\"1\":{\"49\":1}}],[\"最后\",{\"1\":{\"18\":1,\"46\":1}}],[\"序列号\",{\"1\":{\"183\":1}}],[\"序列的顺序中往往蕴含着一些重要信息\",{\"1\":{\"18\":1}}],[\"序列转换模型一般由循环神经网络\",{\"1\":{\"16\":1}}],[\"序列转换模型并不是transformer首次提出的\",{\"1\":{\"16\":1}}],[\"序列转换模型\",{\"0\":{\"16\":1},\"1\":{\"16\":1}}],[\"拥有更加优秀的能力\",{\"1\":{\"18\":1}}],[\"采用了多任务学习优化\",{\"1\":{\"32\":1}}],[\"采用了注意力机制\",{\"1\":{\"18\":1}}],[\"采用三角函数来作为位置编码公式\",{\"1\":{\"18\":1}}],[\"采用的小窗方式是使用decorcaptionview\",{\"1\":{\"3\":1}}],[\"下面之所以和c语言来比\",{\"1\":{\"204\":1}}],[\"下面记一下\",{\"1\":{\"157\":1}}],[\"下面挑选介绍\",{\"1\":{\"61\":1}}],[\"下面挑选开源的和azure商用的进行总结\",{\"1\":{\"55\":1}}],[\"下面有介绍\",{\"1\":{\"38\":1}}],[\"下面一节\",{\"1\":{\"29\":1}}],[\"下面\",{\"1\":{\"26\":1,\"77\":1}}],[\"下面我们来介绍一下bert\",{\"1\":{\"19\":1}}],[\"下面我们来介绍自注意力机制\",{\"1\":{\"17\":1}}],[\"下面我们讨论位置编码\",{\"1\":{\"18\":1}}],[\"下面需要加载mifreeformdisplayadapter\",{\"1\":{\"3\":1}}],[\"得到锁后可以使用expire\",{\"1\":{\"198\":1}}],[\"得到attention\",{\"1\":{\"17\":1}}],[\"得到query和key的相似度\",{\"1\":{\"17\":1}}],[\"km\",{\"1\":{\"244\":1}}],[\"km​eimθ\",{\"1\":{\"77\":1}}],[\"km​\",{\"1\":{\"77\":2}}],[\"kb\",{\"1\":{\"237\":1}}],[\"kindbrave\",{\"1\":{\"105\":1}}],[\"kv\",{\"1\":{\"77\":10}}],[\"k=xwk​\",{\"1\":{\"77\":1}}],[\"kn​进行内容感知\",{\"1\":{\"77\":1}}],[\"kn​∈r1×d\",{\"1\":{\"77\":1}}],[\"kt表示k的转置\",{\"1\":{\"77\":1}}],[\"kubernetes\",{\"1\":{\"69\":1}}],[\"kubeflow项目致力于在kubernetes上部署机器学习\",{\"1\":{\"58\":1}}],[\"kubeflow是一个基于kubernetes的端到端ml平台\",{\"1\":{\"58\":1}}],[\"kubeflow是一个基于kubernetes的机器学习工具集\",{\"1\":{\"58\":1}}],[\"kubeflow主要是为了简化在kubernetes上面运行机器学习任务的流程\",{\"1\":{\"58\":1}}],[\"kubeflow\",{\"0\":{\"58\":1},\"1\":{\"58\":2}}],[\"kl惩罚\",{\"1\":{\"50\":1}}],[\"karthik\",{\"1\":{\"33\":1}}],[\"katherine\",{\"1\":{\"33\":1}}],[\"kaiser\",{\"1\":{\"33\":1}}],[\"kristina\",{\"1\":{\"33\":1}}],[\"kenton\",{\"1\":{\"33\":1}}],[\"keyn\",{\"1\":{\"233\":2}}],[\"key1\",{\"1\":{\"233\":2}}],[\"key是唯一表示\",{\"1\":{\"202\":1}}],[\"key=b\",{\"1\":{\"105\":1}}],[\"key=a\",{\"1\":{\"105\":2}}],[\"key\",{\"1\":{\"17\":1,\"103\":3,\"105\":2,\"198\":4,\"210\":9,\"216\":2,\"218\":8,\"222\":19,\"229\":14,\"233\":5,\"235\":1,\"236\":6,\"238\":3,\"242\":8,\"244\":1}}],[\"k\",{\"1\":{\"17\":1,\"41\":1,\"77\":8,\"247\":6}}],[\"用途\",{\"1\":{\"58\":1,\"63\":1,\"64\":1,\"65\":1,\"66\":1,\"67\":1,\"68\":1,\"69\":1,\"70\":1,\"72\":1,\"73\":1}}],[\"用来表示一个元素的二值状态\",{\"1\":{\"232\":1}}],[\"用来判断引用是否存在\",{\"1\":{\"140\":1}}],[\"用来分析字节码跳转位置是否正确\",{\"1\":{\"139\":1}}],[\"用来分析字节码是否合规\",{\"1\":{\"139\":1}}],[\"用来集成定义\",{\"1\":{\"56\":1}}],[\"用来自动学习和计算输入数据对输出数据的贡献大小\",{\"1\":{\"17\":1}}],[\"用例如法语说话\",{\"1\":{\"51\":1}}],[\"用这个分数\",{\"1\":{\"49\":1}}],[\"用到的一些人类偏好开源数据集\",{\"1\":{\"48\":1}}],[\"用公开可获得的指令微调数据\",{\"1\":{\"46\":1}}],[\"用于执行结构化数据和模型质量检查\",{\"1\":{\"70\":1}}],[\"用于在开发\",{\"1\":{\"70\":1}}],[\"用于在生产中部署和维护api\",{\"1\":{\"68\":1}}],[\"用于数据科学和机器学习项目\",{\"1\":{\"65\":1}}],[\"用于监控\",{\"1\":{\"64\":1}}],[\"用于部署生产环境机器学习流水线\",{\"1\":{\"56\":1}}],[\"用于帮助控制多轮对话流\",{\"1\":{\"45\":1}}],[\"用于将所有的自然语言处理\",{\"1\":{\"24\":1}}],[\"用cpu可以\",{\"1\":{\"35\":1}}],[\"用\",{\"1\":{\"22\":1}}],[\"用户可以通过可选的\",{\"1\":{\"235\":1}}],[\"用户可以在开源hydrosphere\",{\"1\":{\"72\":1}}],[\"用户数据包协议\",{\"1\":{\"83\":1}}],[\"用户和助手\",{\"1\":{\"51\":1}}],[\"用户程序可以通过以下方式获取到mi\",{\"1\":{\"3\":1}}],[\"用户态程序获取系统服务的方式通常是走binder\",{\"1\":{\"3\":1}}],[\"彻底抛弃了复杂的rnn和cnn\",{\"1\":{\"16\":1}}],[\"让\",{\"1\":{\"236\":1}}],[\"让它不那么集中\",{\"1\":{\"197\":1}}],[\"让r表示retention\",{\"1\":{\"78\":1}}],[\"让微调的资源消耗降下来了\",{\"1\":{\"30\":1}}],[\"让序列转换模型的效率得以一定的提升\",{\"1\":{\"16\":1}}],[\"让程序可以选择多个运行状态\",{\"1\":{\"9\":1}}],[\"有关quicklist\",{\"1\":{\"209\":1}}],[\"有序性\",{\"0\":{\"129\":1},\"1\":{\"127\":1}}],[\"有了标记\",{\"1\":{\"109\":1}}],[\"有两种常见的死亡对象判断方法\",{\"1\":{\"94\":1}}],[\"有两种方法可以实现多态\",{\"1\":{\"11\":1}}],[\"有数据验证\",{\"1\":{\"56\":1}}],[\"有必要逐步重新调整温度\",{\"1\":{\"50\":1}}],[\"有效batch大小固定为512对\",{\"1\":{\"49\":1}}],[\"有用性\",{\"1\":{\"49\":1}}],[\"有的学者尝试将注意力机制\",{\"1\":{\"16\":1}}],[\"后\",{\"1\":{\"251\":1}}],[\"后会过期\",{\"1\":{\"207\":1}}],[\"后面也会空出大量的连续空间\",{\"1\":{\"110\":1}}],[\"后续更新支持的\",{\"1\":{\"199\":1}}],[\"后续有详细介绍\",{\"1\":{\"37\":1}}],[\"后续有介绍\",{\"1\":{\"37\":1}}],[\"后续的语言模型\",{\"1\":{\"14\":1,\"18\":1}}],[\"后来\",{\"1\":{\"16\":1}}],[\"导致序列转换模型的效率不高\",{\"1\":{\"16\":1}}],[\"导致无法持续连接android设备\",{\"1\":{\"0\":1}}],[\"和元素越多就越耗费内存的\",{\"1\":{\"237\":1}}],[\"和输入\",{\"1\":{\"233\":1}}],[\"和记录的已处理过的消息\",{\"1\":{\"213\":1}}],[\"和块循环模式\",{\"1\":{\"79\":1}}],[\"和前馈网络\",{\"1\":{\"76\":1,\"79\":1}}],[\"和transformer相似的性能和并行训练\",{\"1\":{\"75\":1}}],[\"和10e−6的恒定学习率\",{\"1\":{\"50\":1}}],[\"和llama\",{\"1\":{\"46\":1,\"50\":1}}],[\"和lora\",{\"1\":{\"14\":1}}],[\"和value\",{\"1\":{\"41\":1}}],[\"和字符标签\",{\"1\":{\"32\":1}}],[\"和\",{\"1\":{\"27\":1,\"49\":1,\"55\":1,\"233\":2,\"237\":1,\"241\":1,\"244\":1,\"251\":6}}],[\"和超多的预训练语料\",{\"1\":{\"25\":1}}],[\"和一个值\",{\"1\":{\"248\":1}}],[\"和一个前馈网络\",{\"1\":{\"18\":1}}],[\"和一个解码器\",{\"1\":{\"16\":1}}],[\"和p\",{\"1\":{\"13\":1,\"30\":1}}],[\"国内外研究者先后提出了p\",{\"1\":{\"14\":1}}],[\"因此他们可以专注于构建模型\",{\"1\":{\"65\":1}}],[\"因此segment\",{\"1\":{\"21\":1}}],[\"因此难免需要兼顾到各种nlp任务场景下的输入\",{\"1\":{\"21\":1}}],[\"因此\",{\"1\":{\"14\":1,\"18\":1,\"19\":1,\"46\":1,\"50\":1,\"53\":1,\"75\":1,\"223\":1}}],[\"因为bit占用空间非常小\",{\"1\":{\"231\":1}}],[\"因为一条数据一旦被处理\",{\"1\":{\"215\":1}}],[\"因为重复的消息代表已经被处理过了\",{\"1\":{\"213\":1}}],[\"因为下一次分配到的服务器可能不是上一次的\",{\"1\":{\"207\":1}}],[\"因为set命令有个参数nx\",{\"1\":{\"207\":1}}],[\"因为sds会在拼接之前检查空间是否满足要求\",{\"1\":{\"204\":1}}],[\"因为sds使用参数len来保存字符串长度\",{\"1\":{\"204\":1}}],[\"因为embstr编码的字符串对象的所有数据都保存在一块连续的内存里面\",{\"1\":{\"206\":1}}],[\"因为rdb是隔一段时间进行备份的\",{\"1\":{\"196\":1}}],[\"因为\",{\"1\":{\"173\":1,\"235\":1,\"251\":1}}],[\"因为前缀索引占用空间更小\",{\"1\":{\"168\":1}}],[\"因为索引也是要存储的\",{\"1\":{\"167\":1}}],[\"因为索引很难优化null值\",{\"1\":{\"164\":1}}],[\"因为更新也需要更新索引\",{\"1\":{\"164\":1}}],[\"因为更高的温度使作者能够对更多样的输出进行采样\",{\"1\":{\"50\":1}}],[\"因为非聚簇索引中没有真正的值\",{\"1\":{\"158\":1}}],[\"因为聚簇索引数据是放到data中的\",{\"1\":{\"157\":1}}],[\"因为聚簇查找不需要进行回表操作\",{\"1\":{\"157\":1}}],[\"因为data中就是这一行的信息了\",{\"1\":{\"157\":1}}],[\"因为节点中既有key又有value\",{\"1\":{\"154\":1}}],[\"因为每次对数据内容进行更变\",{\"1\":{\"148\":1}}],[\"因为每个step的复杂度都是o\",{\"1\":{\"75\":1}}],[\"因为除了serial\",{\"1\":{\"114\":1}}],[\"因为被清除的内存位置都是不连续的\",{\"1\":{\"108\":1}}],[\"因为最大值增加\",{\"1\":{\"50\":1}}],[\"因为gelus效果更好\",{\"1\":{\"29\":1}}],[\"因为在微调的过程中\",{\"1\":{\"22\":1}}],[\"因为在程序运行时才确定具体的类\",{\"1\":{\"9\":1}}],[\"因为这里的上下文\",{\"1\":{\"22\":1}}],[\"因为三角函数具有周期性\",{\"1\":{\"18\":1}}],[\"因为transformer抛弃了rnn和cnn\",{\"1\":{\"18\":1}}],[\"因为transformer需要判断序列中词与词之间的关系强度\",{\"1\":{\"17\":1}}],[\"因为没有注册服务\",{\"1\":{\"3\":1}}],[\"因为获取dms后执行的操作均在java层完成\",{\"1\":{\"3\":1}}],[\"因为我们的自定义系统服务不在scache中\",{\"1\":{\"3\":1}}],[\"因为它处于无限重启状态\",{\"1\":{\"0\":1}}],[\"大于\",{\"1\":{\"245\":1}}],[\"大约占\",{\"1\":{\"236\":1}}],[\"大内存对象会被分配到老年代\",{\"0\":{\"90\":1}}],[\"大大减少了训练时的存储和内存使用\",{\"1\":{\"31\":1}}],[\"大家首先想起的一定是chatgpt\",{\"1\":{\"25\":1}}],[\"大家的参数量还没有那么夸张\",{\"1\":{\"20\":1}}],[\"大部分个人和团队都已经没有能力去做模型的全参数微调了\",{\"1\":{\"14\":1}}],[\"大模型\",{\"1\":{\"14\":1}}],[\"抛开预训练不谈\",{\"1\":{\"14\":1}}],[\"然而无论是rnn也好\",{\"1\":{\"16\":1}}],[\"然而\",{\"1\":{\"14\":1,\"41\":1,\"50\":1,\"51\":1,\"75\":1}}],[\"然后返回\",{\"1\":{\"247\":1}}],[\"然后取length\",{\"1\":{\"246\":1}}],[\"然后取消勾选enable\",{\"1\":{\"1\":1}}],[\"然后也有set的不可重复\",{\"1\":{\"227\":1}}],[\"然后通过中序遍历\",{\"1\":{\"154\":1}}],[\"然后把后面的内存都释放掉\",{\"1\":{\"110\":1}}],[\"然后清除\",{\"1\":{\"108\":1}}],[\"然后用这个向量x0=\",{\"1\":{\"79\":1}}],[\"然后用这些类继承该父类\",{\"1\":{\"7\":1}}],[\"然后可以将此指令综合连接到会话的所有用户消息\",{\"1\":{\"51\":1}}],[\"然后定义了一个指令\",{\"1\":{\"51\":1}}],[\"然后在生成之后释放内存\",{\"1\":{\"50\":1}}],[\"然后为给定提示选择最佳答案\",{\"1\":{\"50\":1}}],[\"然后作者在新的一组排序样本上微调模型\",{\"1\":{\"50\":1}}],[\"然后输出一个分数来表明这个结果的质量\",{\"1\":{\"49\":1}}],[\"然后有两种输出\",{\"1\":{\"48\":1}}],[\"然后根据提供的标准在两个采样的模型响应之间进行选择\",{\"1\":{\"48\":1}}],[\"然后再进行修改\",{\"1\":{\"206\":1}}],[\"然后再写入主内存\",{\"1\":{\"131\":1}}],[\"然后再适配下游任务\",{\"1\":{\"32\":1}}],[\"然后再加上要翻译的内容即可\",{\"1\":{\"24\":1}}],[\"然后\",{\"1\":{\"31\":1,\"76\":1,\"131\":1}}],[\"然后模型来预测b是否为a的下一句话\",{\"1\":{\"22\":1}}],[\"然后归一化相似度\",{\"1\":{\"17\":1}}],[\"千亿\",{\"1\":{\"14\":1}}],[\"小模型\",{\"1\":{\"14\":1}}],[\"lbs\",{\"1\":{\"240\":3,\"241\":1,\"244\":3}}],[\"lpop\",{\"1\":{\"210\":1}}],[\"lpush\",{\"1\":{\"210\":1,\"213\":2}}],[\"ln表示layernorm\",{\"1\":{\"79\":1}}],[\"l∈\",{\"1\":{\"76\":1}}],[\"lrange\",{\"1\":{\"210\":1}}],[\"lr\",{\"1\":{\"50\":1}}],[\"lr=2e\",{\"1\":{\"46\":1}}],[\"llm\",{\"0\":{\"257\":1},\"1\":{\"74\":1}}],[\"llama\",{\"0\":{\"36\":1},\"1\":{\"37\":4,\"38\":4,\"39\":1,\"41\":2,\"45\":1,\"49\":3,\"50\":1,\"51\":1,\"54\":5}}],[\"llion\",{\"1\":{\"33\":1}}],[\"l\",{\"1\":{\"33\":1,\"76\":1}}],[\"lucky\",{\"1\":{\"226\":7}}],[\"lucia\",{\"1\":{\"33\":1}}],[\"luke\",{\"1\":{\"33\":1}}],[\"lukasz\",{\"1\":{\"33\":1}}],[\"length\",{\"1\":{\"174\":1,\"245\":1,\"247\":2,\"248\":1,\"249\":3,\"251\":2,\"253\":3}}],[\"lenght\",{\"1\":{\"46\":1}}],[\"len\",{\"1\":{\"77\":6,\"248\":2,\"249\":2}}],[\"lewis\",{\"1\":{\"33\":1}}],[\"levy\",{\"1\":{\"33\":1}}],[\"lester\",{\"1\":{\"33\":1}}],[\"learn\",{\"1\":{\"33\":1,\"60\":1,\"62\":1,\"68\":1}}],[\"learning\",{\"1\":{\"24\":1,\"33\":1,\"55\":1}}],[\"leetcode\",{\"0\":{\"261\":1,\"262\":1}}],[\"lee\",{\"1\":{\"33\":2}}],[\"lm\",{\"1\":{\"32\":1}}],[\"limit\",{\"1\":{\"229\":3}}],[\"limits\",{\"1\":{\"33\":1}}],[\"listpack\",{\"1\":{\"228\":1}}],[\"list有序\",{\"1\":{\"220\":1}}],[\"list可以存储重复数据\",{\"1\":{\"220\":1}}],[\"list不支持多个消费者消费同一条数据\",{\"1\":{\"215\":1}}],[\"list实现消息队列有什么问题\",{\"0\":{\"215\":1}}],[\"list通过brpoplpush把当前读到的数据存到另一个list中进行备份\",{\"1\":{\"214\":1}}],[\"list如何处理重复消息\",{\"0\":{\"213\":1}}],[\"list如何保证消息的有序性\",{\"0\":{\"212\":1}}],[\"list本身就是一个先进先出的数据类型\",{\"1\":{\"212\":1}}],[\"list底层数据结构为双向链表或压缩列表\",{\"1\":{\"209\":1}}],[\"list是字符串列表\",{\"1\":{\"208\":1}}],[\"list\",{\"0\":{\"208\":1},\"1\":{\"193\":1,\"200\":1,\"213\":1}}],[\"lisa\",{\"1\":{\"33\":1}}],[\"lightgbm\",{\"1\":{\"68\":1}}],[\"lindy\",{\"1\":{\"226\":2}}],[\"line\",{\"1\":{\"49\":1}}],[\"linguistics\",{\"1\":{\"33\":2}}],[\"liang\",{\"1\":{\"33\":1}}],[\"liu\",{\"1\":{\"33\":6}}],[\"li\",{\"1\":{\"33\":4}}],[\"li~\",{\"1\":{\"27\":1}}],[\"like不会\",{\"1\":{\"161\":1}}],[\"like\",{\"0\":{\"176\":1},\"1\":{\"18\":4}}],[\"latitude\",{\"1\":{\"242\":4}}],[\"layers的数值精度\",{\"1\":{\"78\":1}}],[\"layers的非线性\",{\"1\":{\"78\":1}}],[\"layer的定义如下\",{\"1\":{\"77\":1}}],[\"layer结构图如下\",{\"1\":{\"77\":1}}],[\"layernorm\",{\"1\":{\"76\":1}}],[\"lang包下的system类中\",{\"1\":{\"253\":1}}],[\"language\",{\"1\":{\"14\":1,\"22\":1,\"26\":1,\"27\":1,\"33\":7,\"41\":1,\"46\":1}}],[\"landscape非常丰富地介绍了mlops\",{\"1\":{\"72\":1}}],[\"large的参数规模是340m\",{\"1\":{\"20\":1}}],[\"large\",{\"1\":{\"20\":3,\"33\":1,\"49\":1}}],[\"lookup\",{\"1\":{\"253\":1}}],[\"locations\",{\"1\":{\"244\":3}}],[\"location\",{\"1\":{\"240\":1}}],[\"loading\",{\"0\":{\"134\":1}}],[\"loadclass\",{\"1\":{\"3\":2}}],[\"low\",{\"1\":{\"33\":1}}],[\"lora在文生图领域应用广泛\",{\"1\":{\"30\":1}}],[\"lora同样也是一种部分参数微调的方法\",{\"1\":{\"30\":1}}],[\"lora\",{\"1\":{\"30\":1,\"33\":1}}],[\"longitude\",{\"1\":{\"242\":4}}],[\"long\",{\"1\":{\"3\":1}}],[\"logn\",{\"1\":{\"151\":1}}],[\"log文件并打开\",{\"1\":{\"1\":1}}],[\"log\",{\"1\":{\"1\":1,\"3\":3}}],[\"意为通用语言模型\",{\"1\":{\"14\":1}}],[\"放眼国内\",{\"1\":{\"14\":1}}],[\"了\",{\"1\":{\"14\":1}}],[\"的介绍\",{\"0\":{\"253\":1}}],[\"的长度为\",{\"1\":{\"251\":1}}],[\"的初始长度为\",{\"1\":{\"251\":1}}],[\"的所有元素\",{\"1\":{\"248\":1,\"249\":1}}],[\"的大小不重要\",{\"1\":{\"247\":1}}],[\"的其余元素与\",{\"1\":{\"247\":1}}],[\"的前五个元素被修改为\",{\"1\":{\"247\":1}}],[\"的前两个元素被修改为\",{\"1\":{\"247\":1}}],[\"的前\",{\"1\":{\"247\":1}}],[\"的唯一元素的数量为\",{\"1\":{\"247\":1}}],[\"的算法解决此问题吗\",{\"1\":{\"251\":1}}],[\"的算法解决此问题\",{\"1\":{\"245\":1}}],[\"的元素\",{\"1\":{\"245\":1,\"248\":1}}],[\"的数组\",{\"1\":{\"245\":1,\"247\":1}}],[\"的数据集合类型\",{\"1\":{\"237\":1}}],[\"的车辆的当前经纬度位置存入\",{\"1\":{\"244\":1}}],[\"的需求\",{\"1\":{\"241\":1}}],[\"的特性\",{\"1\":{\"241\":1}}],[\"的特点\",{\"1\":{\"227\":1}}],[\"的应用\",{\"1\":{\"240\":1}}],[\"的基数估算值\",{\"1\":{\"238\":1}}],[\"的优点是\",{\"1\":{\"237\":1}}],[\"的字符串序列\",{\"1\":{\"236\":1}}],[\"的字符串的长度\",{\"1\":{\"233\":1}}],[\"的对应的\",{\"1\":{\"236\":1}}],[\"的对象开始\",{\"1\":{\"96\":1}}],[\"的用户在\",{\"1\":{\"235\":1}}],[\"的位置\",{\"1\":{\"233\":1,\"242\":1}}],[\"的个数便得到了连续打卡\",{\"1\":{\"236\":1}}],[\"的个数\",{\"1\":{\"233\":1,\"236\":1}}],[\"的差集\",{\"1\":{\"223\":1}}],[\"的消息插入了消息队列\",{\"1\":{\"213\":1}}],[\"的时间就可以完成\",{\"1\":{\"151\":1}}],[\"的内存开销约为\",{\"1\":{\"236\":1}}],[\"的内存\",{\"1\":{\"75\":2,\"236\":1}}],[\"的开源库\",{\"1\":{\"73\":1}}],[\"的提示上优于mpt\",{\"1\":{\"54\":1}}],[\"的黄金标准\",{\"1\":{\"54\":1}}],[\"的胜率\",{\"1\":{\"53\":1}}],[\"的loss设置为0\",{\"1\":{\"51\":1}}],[\"的测试集\",{\"1\":{\"49\":1}}],[\"的warm\",{\"1\":{\"49\":1}}],[\"的输入\",{\"1\":{\"27\":1}}],[\"的思想\",{\"1\":{\"27\":1}}],[\"的token来进行训练\",{\"1\":{\"22\":1}}],[\"的\",{\"1\":{\"18\":1,\"56\":1,\"235\":1,\"236\":2}}],[\"的能力不是很强\",{\"1\":{\"14\":1}}],[\"的任务\",{\"1\":{\"14\":1}}],[\"的关系\",{\"1\":{\"7\":1}}],[\"轰动一时\",{\"1\":{\"14\":1}}],[\"早在2017年\",{\"1\":{\"14\":1}}],[\"讯飞的星火等\",{\"1\":{\"14\":1}}],[\"各种语言模型层出不穷\",{\"1\":{\"14\":1}}],[\"几个方面来介绍\",{\"1\":{\"13\":1}}],[\"g1的回收不像cms一样是并发的\",{\"1\":{\"117\":1}}],[\"g1的流程\",{\"1\":{\"117\":1}}],[\"g1的特点\",{\"1\":{\"117\":1}}],[\"g1整体上看是标记\",{\"1\":{\"117\":1}}],[\"g1收集器在尽可能较短时间完成回收的基础上\",{\"1\":{\"117\":1}}],[\"g1\",{\"0\":{\"117\":1},\"1\":{\"112\":1}}],[\"gc之前\",{\"1\":{\"93\":1}}],[\"gc分类\",{\"0\":{\"92\":1}}],[\"gc\",{\"1\":{\"89\":1,\"92\":4,\"93\":2,\"96\":1,\"105\":1}}],[\"gpu的内存急剧增加\",{\"1\":{\"75\":1}}],[\"gpt似乎有能力处理自然语言理解和自然语言生成任务\",{\"1\":{\"26\":1}}],[\"gpt已经迎来了第四个大版本\",{\"1\":{\"25\":1}}],[\"gpt也并没有将transformer的decoder拿过来直接用\",{\"1\":{\"25\":1}}],[\"gpt的核心部分是n=12的transformer\",{\"1\":{\"25\":1}}],[\"gpt采用的不是transformer的encoder部分\",{\"1\":{\"25\":1}}],[\"gpt同样也抛弃了传统的rnn和cnn\",{\"1\":{\"25\":1}}],[\"gpt模型结构\",{\"1\":{\"25\":1}}],[\"gpt模型与bert模型不同\",{\"1\":{\"25\":1}}],[\"gpt模型\",{\"0\":{\"25\":1}}],[\"gpt是一个自回归\",{\"1\":{\"19\":1}}],[\"gpt和glm均为预训练语言模型\",{\"1\":{\"13\":1}}],[\"gpt\",{\"1\":{\"13\":1,\"14\":3,\"25\":1,\"26\":1,\"33\":1}}],[\"group\",{\"1\":{\"77\":3}}],[\"grouped\",{\"1\":{\"37\":1,\"41\":1}}],[\"gradient\",{\"1\":{\"50\":1}}],[\"gate\",{\"1\":{\"78\":1}}],[\"gated\",{\"0\":{\"78\":1}}],[\"gatt方法\",{\"1\":{\"51\":1}}],[\"gatt允许对多轮进行对话控制\",{\"1\":{\"51\":1}}],[\"gatt\",{\"1\":{\"45\":1,\"51\":1}}],[\"gao\",{\"1\":{\"33\":1}}],[\"ghost\",{\"1\":{\"45\":1}}],[\"gqa\",{\"1\":{\"38\":1,\"41\":1}}],[\"github\",{\"1\":{\"35\":1,\"68\":1}}],[\"goyal\",{\"1\":{\"33\":1}}],[\"gomez\",{\"1\":{\"33\":1}}],[\"google公开了t5模型\",{\"1\":{\"14\":1}}],[\"google公开了以transformer作为基础的语言模型bert\",{\"1\":{\"14\":1}}],[\"google\",{\"1\":{\"14\":1,\"56\":2}}],[\"g\",{\"1\":{\"26\":3}}],[\"georadius\",{\"1\":{\"242\":1,\"244\":3}}],[\"geodist\",{\"1\":{\"242\":1}}],[\"geopos\",{\"1\":{\"242\":1}}],[\"geoadd\",{\"1\":{\"242\":1,\"244\":2}}],[\"geohash\",{\"1\":{\"241\":1}}],[\"geo\",{\"0\":{\"240\":1},\"1\":{\"201\":1,\"240\":2,\"241\":2,\"244\":3}}],[\"geo地理位置\",{\"1\":{\"193\":1}}],[\"generation也提出了同样的llm重新排序策略\",{\"1\":{\"50\":1}}],[\"generation\",{\"1\":{\"33\":2}}],[\"generative\",{\"1\":{\"25\":1,\"33\":1}}],[\"general\",{\"1\":{\"14\":1,\"26\":1,\"33\":1}}],[\"getbit\",{\"1\":{\"233\":1,\"235\":1}}],[\"getobject\",{\"1\":{\"103\":2}}],[\"getoutercontext\",{\"1\":{\"3\":1}}],[\"getconstructors\",{\"1\":{\"3\":1}}],[\"getclassloader\",{\"1\":{\"3\":1}}],[\"getclass\",{\"1\":{\"3\":4}}],[\"getmethod\",{\"1\":{\"3\":1}}],[\"getdeclaredfield\",{\"1\":{\"3\":2}}],[\"get\",{\"1\":{\"3\":5,\"103\":1}}],[\"getservice\",{\"1\":{\"3\":5}}],[\"glm的基本结构已经介绍完毕\",{\"1\":{\"29\":1}}],[\"glm同样只使用了transformer的encoder部分\",{\"1\":{\"29\":1}}],[\"glm同样对transformer的结构进行了修改\",{\"1\":{\"29\":1}}],[\"glm同样是基于transformer的结构\",{\"1\":{\"29\":1}}],[\"glm与transformer\",{\"0\":{\"29\":1}}],[\"glm二维编码\",{\"1\":{\"28\":1}}],[\"glm采用了二维位置编码\",{\"1\":{\"28\":1}}],[\"glm采用自回归的方式尝试还原它们\",{\"1\":{\"27\":1}}],[\"glm仍然是以transformer为基础的结构\",{\"1\":{\"28\":1}}],[\"glm自回归填空示意图二\",{\"1\":{\"27\":1}}],[\"glm自回归填空示意图之一\",{\"1\":{\"27\":1}}],[\"glm把输入的文本分为两个部分\",{\"1\":{\"28\":1}}],[\"glm把masked掉的信息全部保留在了partb\",{\"1\":{\"27\":1}}],[\"glm把s~1~和s~2~对应的x部分替换为一个\",{\"1\":{\"27\":1}}],[\"glm预测的条件比bert多了一个partb\",{\"1\":{\"27\":1}}],[\"glm随机masked掉的比例为15\",{\"1\":{\"27\":1}}],[\"glm随机masked掉一些文本\",{\"1\":{\"27\":1}}],[\"glm没有选择直接丢失这些x\",{\"1\":{\"27\":1}}],[\"glm模型应用了名为自回归填空\",{\"1\":{\"27\":1}}],[\"glm创新地应用了自回归填空思想\",{\"1\":{\"26\":1}}],[\"glm也尝试能够同时处理自然语言理解和自然语言生成等多种nlp任务\",{\"1\":{\"26\":1}}],[\"glm意为通用语言模型\",{\"1\":{\"26\":1}}],[\"glm\",{\"0\":{\"26\":1},\"1\":{\"13\":1}}],[\"反过来不一定\",{\"1\":{\"153\":1}}],[\"反复打磨多年才得以形成的\",{\"1\":{\"13\":1}}],[\"反之则不可以\",{\"1\":{\"7\":1}}],[\"是以\",{\"1\":{\"248\":1,\"249\":1}}],[\"是统计规则是基于概率完成的\",{\"1\":{\"237\":1}}],[\"是因为redis的底层实现是c\",{\"1\":{\"204\":1}}],[\"是因为对于输入的d大值\",{\"1\":{\"18\":1}}],[\"是key\",{\"1\":{\"190\":1}}],[\"是否继承了final类等等~~\",{\"1\":{\"137\":1}}],[\"是jvm虚拟出来的一块内存\",{\"1\":{\"131\":1}}],[\"是java调用native原生方法执行c\",{\"1\":{\"125\":1}}],[\"是引用类型的存放位置\",{\"1\":{\"122\":1}}],[\"是微软研究院和清华大学推出的大语言模型\",{\"1\":{\"74\":1}}],[\"是一种用于\",{\"1\":{\"237\":1}}],[\"是一种基于\",{\"1\":{\"56\":1}}],[\"是一串连续的二进制数组\",{\"1\":{\"231\":1}}],[\"是一个配置框架\",{\"1\":{\"56\":1}}],[\"是一个端到端平台\",{\"1\":{\"56\":1}}],[\"是llama\",{\"1\":{\"37\":1}}],[\"是被masked标注的预测概率来预测样本的标签\",{\"1\":{\"31\":1}}],[\"是transfer\",{\"1\":{\"24\":1}}],[\"是效果最好的\",{\"1\":{\"22\":1}}],[\"是\",{\"1\":{\"22\":1,\"237\":1,\"240\":1,\"244\":2}}],[\"是这三个嵌入式张量的和\",{\"1\":{\"21\":1}}],[\"是训练出来的\",{\"1\":{\"21\":1}}],[\"是由三角函数计算出来的\",{\"1\":{\"21\":1}}],[\"是decoder单元中多了一层masked\",{\"1\":{\"18\":1}}],[\"是人们在机器学习模型中嵌入的一种特殊结构\",{\"1\":{\"17\":1}}],[\"是bert\",{\"1\":{\"14\":1}}],[\"是其总结前人经验\",{\"1\":{\"13\":1}}],[\"是指利用抽象数据类型将数据和基于数据的操作封装在一起\",{\"1\":{\"5\":1}}],[\"从要复制的数组哪里开始复制\",{\"1\":{\"253\":1}}],[\"从源数组的哪个位置开始复制\",{\"1\":{\"253\":1}}],[\"从给定的\",{\"1\":{\"242\":1}}],[\"从\",{\"1\":{\"235\":1}}],[\"从集合key中随机选出count个元素\",{\"1\":{\"222\":2}}],[\"从集合key中删除元素\",{\"1\":{\"222\":1}}],[\"从key列表表尾弹出一个元素\",{\"1\":{\"210\":1}}],[\"从key列表表头弹出一个元素\",{\"1\":{\"210\":1}}],[\"从上次备份后的数据就丢失了\",{\"1\":{\"196\":1}}],[\"从根节点到叶子节点\",{\"1\":{\"153\":1}}],[\"从论文题目可以看出\",{\"1\":{\"74\":1}}],[\"从现在起始终充当拿破仑\",{\"1\":{\"51\":1}}],[\"从两个不同的模型变量中对给定提示的两个响应进行采样\",{\"1\":{\"48\":1}}],[\"从前\",{\"1\":{\"30\":1}}],[\"从全称中可以看出\",{\"1\":{\"19\":1}}],[\"从bert到glm\",{\"0\":{\"13\":1}}],[\"从而得知变化\",{\"1\":{\"131\":1}}],[\"从而得到attention\",{\"1\":{\"17\":1}}],[\"从而将较大模型的能力提取到较小的模型中\",{\"1\":{\"50\":1}}],[\"从而加快注意力计算\",{\"1\":{\"41\":1}}],[\"从而导致该引用调用的具体方法随之改变\",{\"1\":{\"9\":1}}],[\"从而实现小窗功能\",{\"1\":{\"3\":1}}],[\"当nums1处理完之后\",{\"1\":{\"252\":1}}],[\"当用户想要寻找自己附近的网约车时\",{\"1\":{\"244\":1}}],[\"当\",{\"1\":{\"233\":1,\"236\":1}}],[\"当我们存储的数据是无序并且需要去重的情况下\",{\"1\":{\"223\":1}}],[\"当我们对embstr编码的字符串进行append操作的时候\",{\"1\":{\"206\":1}}],[\"当收到一条消息后\",{\"1\":{\"213\":1}}],[\"当一个\",{\"1\":{\"236\":1}}],[\"当一个类的使命完成后\",{\"1\":{\"145\":1}}],[\"当一个常量没有任何引用指向它\",{\"1\":{\"99\":1}}],[\"当时方法区的实现方法是永久代\",{\"1\":{\"121\":1}}],[\"当时提出transformer架构是为了克服基于rnn的模型的无法并行训练的问题\",{\"1\":{\"75\":1}}],[\"当伊甸园区不够之后\",{\"1\":{\"89\":1}}],[\"当您想要跟踪机器学习模型的性能时\",{\"1\":{\"59\":1}}],[\"当在10到100个输出之间采样时\",{\"1\":{\"50\":1}}],[\"当在两个相似的模型反应之间做出决定时\",{\"1\":{\"49\":1}}],[\"当作者在表8中按偏好评级对分数进行分组时\",{\"1\":{\"49\":1}}],[\"当然\",{\"1\":{\"19\":1,\"26\":1,\"206\":1}}],[\"当前的token只能看到它和它之前的token\",{\"1\":{\"19\":1}}],[\"当超类对象引用变量引用子类对象时\",{\"1\":{\"12\":1}}],[\"当米窗3想要实例化上述字段来获取dms实例时\",{\"1\":{\"3\":1}}],[\"99\",{\"1\":{\"213\":2}}],[\"95\",{\"1\":{\"42\":1,\"50\":1}}],[\"977\",{\"1\":{\"35\":1}}],[\"9\",{\"0\":{\"70\":1},\"1\":{\"12\":1,\"14\":1,\"30\":1,\"33\":1,\"42\":1,\"50\":1,\"237\":1}}],[\"81\",{\"1\":{\"237\":1}}],[\"84\",{\"1\":{\"236\":1}}],[\"8支持\",{\"1\":{\"201\":1}}],[\"8字符\",{\"1\":{\"43\":1}}],[\"8\",{\"0\":{\"69\":1,\"163\":1,\"164\":1,\"165\":1,\"166\":1,\"167\":1,\"168\":1,\"169\":1,\"170\":1,\"179\":1,\"240\":1,\"241\":1,\"242\":1,\"243\":1,\"244\":1},\"1\":{\"12\":1,\"14\":1,\"20\":1,\"30\":1,\"33\":1,\"121\":1,\"236\":1,\"237\":1}}],[\"7161964571853815822\",{\"1\":{\"171\":1}}],[\"7之前\",{\"1\":{\"121\":1}}],[\"784044\",{\"1\":{\"58\":1}}],[\"7b\",{\"1\":{\"54\":1}}],[\"7b模型在60\",{\"1\":{\"54\":1}}],[\"70b模型在很大程度上优于palm\",{\"1\":{\"54\":1}}],[\"70b模型相对于chatgpt的胜率为36\",{\"1\":{\"54\":1}}],[\"70b模型上的ppo每次迭代平均耗时≈330秒\",{\"1\":{\"50\":1}}],[\"70b参数llama\",{\"1\":{\"49\":1}}],[\"70b四种参数规模\",{\"1\":{\"37\":1}}],[\"703\",{\"1\":{\"1\":1}}],[\"768\",{\"1\":{\"20\":1}}],[\"7\",{\"0\":{\"68\":1,\"145\":1,\"162\":1,\"170\":1,\"178\":1,\"237\":1,\"238\":1,\"239\":1},\"1\":{\"12\":1,\"14\":1,\"30\":1,\"33\":1,\"121\":1,\"228\":1,\"236\":7,\"249\":2}}],[\"64的小批量大小\",{\"1\":{\"50\":1}}],[\"64\",{\"1\":{\"46\":1,\"228\":1}}],[\"642\",{\"1\":{\"33\":1}}],[\"6b\",{\"1\":{\"35\":1}}],[\"652\",{\"1\":{\"33\":1}}],[\"68\",{\"1\":{\"33\":1}}],[\"61\",{\"1\":{\"33\":1}}],[\"67\",{\"1\":{\"33\":1}}],[\"60th\",{\"1\":{\"33\":1}}],[\"6010\",{\"1\":{\"33\":1}}],[\"6000\",{\"1\":{\"33\":1}}],[\"6\",{\"0\":{\"31\":1,\"32\":1,\"61\":1,\"67\":1,\"85\":1,\"118\":1,\"144\":1,\"160\":1,\"161\":1,\"169\":1,\"177\":1,\"231\":1,\"232\":1,\"233\":1,\"234\":1,\"235\":1,\"236\":1},\"1\":{\"12\":1,\"13\":1,\"14\":1,\"18\":2,\"30\":1,\"33\":1,\"77\":1,\"235\":6,\"251\":4}}],[\"5种\",{\"1\":{\"191\":1}}],[\"50\",{\"1\":{\"75\":1,\"248\":1}}],[\"574提到了大部分上述已有的工具\",{\"1\":{\"71\":1}}],[\"57399\",{\"1\":{\"1\":1}}],[\"55840115\",{\"1\":{\"58\":1}}],[\"5的对话聊天机器人\",{\"1\":{\"25\":1}}],[\"512m\",{\"1\":{\"195\":1}}],[\"512\",{\"1\":{\"20\":1}}],[\"5b\",{\"1\":{\"14\":1}}],[\"5\",{\"0\":{\"27\":1,\"28\":1,\"29\":1,\"60\":1,\"66\":1,\"73\":1,\"112\":1,\"113\":1,\"114\":1,\"115\":1,\"116\":1,\"117\":2,\"118\":1,\"126\":1,\"143\":1,\"154\":1,\"156\":1,\"157\":1,\"158\":1,\"168\":1,\"171\":1,\"176\":1,\"227\":1,\"228\":1,\"229\":1,\"230\":1,\"245\":1},\"1\":{\"12\":1,\"13\":1,\"18\":2,\"33\":1,\"42\":1,\"46\":1,\"54\":2,\"77\":1,\"78\":1,\"226\":1,\"244\":2,\"245\":1,\"247\":2,\"248\":2,\"249\":2,\"251\":4}}],[\"4倍\",{\"1\":{\"75\":1}}],[\"495814838\",{\"1\":{\"68\":1}}],[\"40b型号相比\",{\"1\":{\"54\":1}}],[\"4096\",{\"1\":{\"46\":1}}],[\"4提示中出现的顺序是随机交换的\",{\"1\":{\"53\":1}}],[\"4额外计算最终结果\",{\"1\":{\"53\":1}}],[\"46\",{\"0\":{\"41\":1}}],[\"4186\",{\"1\":{\"33\":1}}],[\"4171\",{\"1\":{\"33\":1}}],[\"4\",{\"0\":{\"24\":1,\"25\":1,\"52\":1,\"53\":1,\"54\":1,\"59\":1,\"65\":1,\"72\":1,\"100\":1,\"106\":1,\"107\":1,\"108\":1,\"109\":1,\"110\":1,\"111\":2,\"116\":1,\"125\":1,\"131\":1,\"140\":1,\"142\":1,\"146\":1,\"153\":1,\"155\":1,\"167\":1,\"175\":1,\"215\":1,\"220\":1,\"221\":1,\"222\":1,\"223\":1,\"224\":1,\"225\":1,\"226\":1,\"249\":1},\"1\":{\"12\":1,\"13\":1,\"18\":3,\"20\":1,\"25\":1,\"26\":1,\"33\":1,\"49\":1,\"175\":1,\"247\":3,\"248\":3,\"253\":1}}],[\"43\",{\"1\":{\"1\":1}}],[\"只出现一次\",{\"1\":{\"247\":1}}],[\"只要弱引用所在区域进行垃圾回收了\",{\"1\":{\"104\":1}}],[\"只对新时代和部分老年代进行回收\",{\"1\":{\"92\":1}}],[\"只进行老年代回收\",{\"1\":{\"92\":1}}],[\"只进行新时代回收\",{\"1\":{\"92\":1}}],[\"只需对其排序\",{\"1\":{\"246\":1}}],[\"只需将前几轮中的所有token\",{\"1\":{\"51\":1}}],[\"只需要调用\",{\"1\":{\"253\":1}}],[\"只需要将nums2复制到nums1即可\",{\"1\":{\"252\":1}}],[\"只需要考虑到即可\",{\"1\":{\"246\":1}}],[\"只需要通过lpush和rpop的组合就可以保证消息的有序性了\",{\"1\":{\"212\":1}}],[\"只需要存储一份模型参数\",{\"1\":{\"31\":1}}],[\"只需要在给模型输入的部分加上前缀\",{\"1\":{\"24\":1}}],[\"只有不存在才插入\",{\"1\":{\"207\":1}}],[\"只有一个文件dump\",{\"1\":{\"196\":1}}],[\"只有or左右都有索引才会走索引\",{\"1\":{\"179\":1}}],[\"只有格式验证成功了\",{\"1\":{\"137\":1}}],[\"只有parnew可以和cms配合\",{\"1\":{\"114\":1}}],[\"只有满足了以下条件\",{\"1\":{\"93\":1}}],[\"只有第一层embedding才需要被微调\",{\"1\":{\"32\":1}}],[\"只有这样该引用才能够具备技能调用父类的方法和子类的方法\",{\"1\":{\"10\":1}}],[\"只不过\",{\"1\":{\"27\":1}}],[\"只不过是某些词的顺序不同\",{\"1\":{\"18\":1}}],[\"只保留了masked\",{\"1\":{\"25\":1}}],[\"只保留一些对外接口使之与外部发生联系\",{\"1\":{\"5\":1}}],[\"只依赖于注意力机制\",{\"1\":{\"16\":1}}],[\"通过\",{\"1\":{\"247\":1}}],[\"通过append模式写入文件\",{\"1\":{\"196\":1}}],[\"通过建立唯一索引\",{\"1\":{\"148\":1}}],[\"通过吸收λ到wq​\",{\"1\":{\"77\":1}}],[\"通过状态sn​来把v\",{\"1\":{\"77\":1}}],[\"通过meta内部的安全和帮助奖励模型进行测量\",{\"1\":{\"53\":1}}],[\"通过上一节的介绍我们知道\",{\"1\":{\"28\":1}}],[\"通过这样的方式\",{\"1\":{\"24\":1}}],[\"通过以上几个部分\",{\"1\":{\"13\":1}}],[\"通过编辑之后会变成两个不同的函数\",{\"1\":{\"9\":1}}],[\"通过使用继承我们能够非常方便地复用以前的代码\",{\"1\":{\"7\":1}}],[\"动态调用\",{\"1\":{\"9\":1}}],[\"动态连接\",{\"1\":{\"9\":1}}],[\"必定是使用子类中定义的这些方法\",{\"1\":{\"9\":1}}],[\"必须在由程序运行期间才能决定\",{\"1\":{\"9\":1}}],[\"若是打卡则将\",{\"1\":{\"236\":1}}],[\"若key不存在则新建\",{\"1\":{\"222\":1}}],[\"若子类重写了父类中的某些方法\",{\"1\":{\"9\":1}}],[\"若可以描述为\",{\"1\":{\"7\":1}}],[\"尽可能地使用联合索引\",{\"0\":{\"167\":1}}],[\"尽可能地隐藏内部的细节\",{\"1\":{\"5\":1}}],[\"尽管我们最新的llama\",{\"1\":{\"53\":1}}],[\"尽管前面提到了使用meta的奖励作为逐点衡量标准的相关性\",{\"1\":{\"53\":1}}],[\"尽管是重载该方法\",{\"1\":{\"9\":1}}],[\"不走常规的jni\",{\"1\":{\"253\":1}}],[\"不对实参做任何拷贝\",{\"1\":{\"249\":1}}],[\"不对实参作任何拷贝\",{\"1\":{\"248\":1}}],[\"不要使用额外的数组空间\",{\"1\":{\"248\":1,\"249\":1}}],[\"不需要考虑数组中超出新长度后面的元素\",{\"1\":{\"247\":2,\"249\":2}}],[\"不需要再处理了\",{\"1\":{\"213\":1}}],[\"不存在的返回\",{\"1\":{\"242\":1}}],[\"不能就不执行了\",{\"1\":{\"214\":1}}],[\"不能接收\",{\"1\":{\"82\":1}}],[\"不知道服务端那边要断开了\",{\"1\":{\"188\":1}}],[\"不是非常准确\",{\"1\":{\"237\":1}}],[\"不是索引中的内容\",{\"1\":{\"174\":1}}],[\"不是上面提到的对象的回收\",{\"1\":{\"100\":1}}],[\"不应该把可空的列作为索引\",{\"1\":{\"164\":1}}],[\"不会按照插入的先后顺序进行存储\",{\"1\":{\"220\":1}}],[\"不会被其他线程打断\",{\"1\":{\"130\":1}}],[\"不会有反射可以反射到这个类\",{\"1\":{\"100\":1}}],[\"不一定\",{\"1\":{\"98\":1,\"159\":1}}],[\"不可重复\",{\"1\":{\"223\":1}}],[\"不可达的对象一定会被垃圾回收吗\",{\"0\":{\"98\":1}}],[\"不可能三角\",{\"1\":{\"75\":1}}],[\"不保证可靠\",{\"1\":{\"83\":1}}],[\"不确定\",{\"1\":{\"48\":1}}],[\"不同的头使用不同的参数矩阵wq​\",{\"1\":{\"78\":1}}],[\"不同于之前研究的简单结合\",{\"1\":{\"26\":1}}],[\"不同长度的句子之间\",{\"1\":{\"18\":1}}],[\"不变\",{\"1\":{\"22\":1,\"27\":1}}],[\"不用修改源程序代码\",{\"1\":{\"9\":1}}],[\"不过我愿意分类为哈希表题\",{\"1\":{\"245\":1}}],[\"不过parta是看不到partb的\",{\"1\":{\"27\":1}}],[\"不过考虑到span之间可能也有关系\",{\"1\":{\"27\":1}}],[\"不过看到它只用了z<i的部分\",{\"1\":{\"27\":1}}],[\"不过在当时的条件下并没有很出色的能力\",{\"1\":{\"26\":1}}],[\"不过因为自回归和自编码在模型结构上相差太多\",{\"1\":{\"26\":1}}],[\"不过因为未知原因\",{\"1\":{\"3\":1}}],[\"不过与bert\",{\"1\":{\"25\":1,\"29\":1}}],[\"不过它们仍然没有脱离rnn或者cnn\",{\"1\":{\"16\":1}}],[\"不过注意到\",{\"1\":{\"3\":1}}],[\"不过\",{\"1\":{\"3\":1,\"22\":1}}],[\"不过这个native方法有些特殊\",{\"1\":{\"253\":1}}],[\"不过这会增加用户成本\",{\"1\":{\"3\":1}}],[\"不过这种办法很不方便\",{\"1\":{\"0\":1}}],[\"不过需要修改较多的源码\",{\"1\":{\"3\":1}}],[\"不过其自带的adb一直处于无法使用的状态\",{\"1\":{\"0\":1}}],[\"即可\",{\"1\":{\"253\":1}}],[\"即可接受的输入长度\",{\"1\":{\"37\":1}}],[\"即类模型对象\",{\"1\":{\"123\":1}}],[\"即groupnorm\",{\"1\":{\"78\":1}}],[\"即q\",{\"1\":{\"77\":1}}],[\"即并行\",{\"1\":{\"75\":1}}],[\"即所谓\",{\"1\":{\"75\":1}}],[\"即所有的自然语言理解任务都可划分为\",{\"1\":{\"14\":1}}],[\"即推理时比较低效\",{\"1\":{\"75\":1}}],[\"即最后一个回合之前的所有中间辅助消息\",{\"1\":{\"51\":1}}],[\"即使一天产生一个亿的数据\",{\"1\":{\"236\":1}}],[\"即使内存不足\",{\"1\":{\"102\":1}}],[\"即使它们和外界没用任何联系了\",{\"1\":{\"95\":1}}],[\"即使在使用大批量和kv缓存时也是如此\",{\"1\":{\"50\":1}}],[\"即使是少量的\",{\"1\":{\"46\":1}}],[\"即每batch\",{\"1\":{\"49\":1}}],[\"即选择和拒绝\",{\"1\":{\"49\":1}}],[\"即对parta部分和partb部分都进行编码\",{\"1\":{\"28\":1}}],[\"即自回归的\",{\"1\":{\"27\":1}}],[\"即带mask部分的句子\",{\"1\":{\"27\":1}}],[\"即masked\",{\"1\":{\"22\":1}}],[\"即此时segment词表的长度为2\",{\"1\":{\"21\":1}}],[\"即8个缩放点积注意力叠加而成\",{\"1\":{\"20\":1}}],[\"即\",{\"1\":{\"19\":1,\"27\":1,\"50\":1,\"74\":1}}],[\"即除以d的开方\",{\"1\":{\"18\":1}}],[\"即通过注入词的顺序信息来增强模型的输入\",{\"1\":{\"18\":1}}],[\"即结构\",{\"1\":{\"13\":1}}],[\"即transformer\",{\"1\":{\"13\":1}}],[\"即不修改程序代码就可以改变程序运行时所绑定的具体代码\",{\"1\":{\"9\":1}}],[\"即一个引用变量倒底会指向哪个类的实例对象\",{\"1\":{\"9\":1}}],[\"即子类可以对父类进行扩展\",{\"1\":{\"7\":1}}],[\"所有值小于64字节\",{\"1\":{\"217\":1}}],[\"所有的命令行记录都会以redis命令请求协议的格式完全持久化保存为aof文件\",{\"1\":{\"196\":1}}],[\"所有被同步锁持有的对象\",{\"1\":{\"97\":1}}],[\"所有较小的模型都根据较大模型的拒绝采样数据进行微调\",{\"1\":{\"50\":1}}],[\"所提出的机制也可以写成rnn\",{\"1\":{\"77\":1}}],[\"所示\",{\"1\":{\"51\":1}}],[\"所谓的可靠性\",{\"1\":{\"214\":1}}],[\"所谓联合索引\",{\"1\":{\"160\":1}}],[\"所谓覆盖索引\",{\"1\":{\"160\":1}}],[\"所谓聚簇索引\",{\"1\":{\"157\":1}}],[\"所谓准备阶段\",{\"1\":{\"141\":1}}],[\"所谓可见性\",{\"1\":{\"128\":1}}],[\"所谓二维编码\",{\"1\":{\"28\":1}}],[\"所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定\",{\"1\":{\"9\":1}}],[\"所替代\",{\"1\":{\"27\":1}}],[\"所以也省了一些开销\",{\"1\":{\"253\":1}}],[\"所以我们需要将返回的\",{\"1\":{\"235\":1}}],[\"所以我们需要自行为每个消息生成一个全局唯一id\",{\"1\":{\"213\":1}}],[\"所以我们可以通过执行这条命令来获取\",{\"1\":{\"235\":1}}],[\"所以非常适合一些数据量大并且只需要二值统计的场景\",{\"1\":{\"231\":1}}],[\"所以redis提供了一个方法brpop\",{\"1\":{\"212\":1}}],[\"所以可以把session存到redis中\",{\"1\":{\"207\":1}}],[\"所以可以给过期时间设置一个随机数\",{\"1\":{\"197\":1}}],[\"所以时间复杂度为o\",{\"1\":{\"204\":1}}],[\"所以引入了缓存中间件\",{\"1\":{\"191\":1}}],[\"所以需要进行判断\",{\"1\":{\"213\":1}}],[\"所以需要第三次握手\",{\"1\":{\"185\":1}}],[\"所以需要走else\",{\"1\":{\"3\":1}}],[\"所以比较耗时\",{\"1\":{\"164\":1}}],[\"所以对于联合索引\",{\"1\":{\"161\":1}}],[\"所以对于多态我们可以总结如下\",{\"1\":{\"9\":1}}],[\"所以在进行联合索引设计时\",{\"1\":{\"161\":1}}],[\"所以插入和删除的效率比平衡二叉树来说是有所提升的\",{\"1\":{\"153\":1}}],[\"所以会占用一定的空间\",{\"1\":{\"148\":1}}],[\"所以放到堆中\",{\"1\":{\"135\":1}}],[\"所以字符串常量池在堆中\",{\"1\":{\"121\":1}}],[\"所以这个步骤就是记录哪些引用发生了变化\",{\"1\":{\"116\":1}}],[\"所以这里就用了masked自注意力机制\",{\"1\":{\"18\":1}}],[\"所以选择标记\",{\"1\":{\"111\":2}}],[\"所以根据不同代的特点来选择不同策略是比较好的\",{\"1\":{\"111\":1}}],[\"所以s~z~的顺序是随机打乱的\",{\"1\":{\"27\":1}}],[\"所以masked掉多个字\",{\"1\":{\"27\":1}}],[\"所以效果不是很好\",{\"1\":{\"26\":1}}],[\"所以\",{\"1\":{\"18\":1,\"28\":1,\"154\":1,\"204\":1,\"206\":1,\"232\":1,\"237\":1,\"251\":1,\"253\":1}}],[\"所以语言模型方向模型参数规模越来越大\",{\"1\":{\"14\":1}}],[\"所以米窗3选择了zygoteloader\",{\"1\":{\"3\":1}}],[\"所以米窗3采用了将自定义系统服务添加到scache中的做法\",{\"1\":{\"3\":1}}],[\"三次握手\",{\"0\":{\"182\":1}}],[\"三大特性展开\",{\"1\":{\"127\":1}}],[\"三部分构成\",{\"1\":{\"32\":1}}],[\"三\",{\"0\":{\"9\":1,\"19\":1}}],[\"它会打印出数组中\",{\"1\":{\"248\":1,\"249\":1}}],[\"它会去寻找唯一非空索引\",{\"1\":{\"155\":1}}],[\"它会存放一些类的静态变量和类的信息\",{\"1\":{\"123\":1}}],[\"它之中保存着主内存中变量的副本\",{\"1\":{\"131\":1}}],[\"它记录了虚拟机执行到了哪行字节码\",{\"1\":{\"126\":1}}],[\"它同样采用了分代回收\",{\"1\":{\"115\":1}}],[\"它必须和referencequeue来做配合\",{\"1\":{\"106\":1}}],[\"它必须是确定性的\",{\"1\":{\"18\":1}}],[\"它不和生命周期相关联\",{\"1\":{\"106\":1}}],[\"它不仅只在embedding层进行微调\",{\"1\":{\"32\":1}}],[\"它使用softreference类来声明\",{\"1\":{\"103\":1}}],[\"它只会在内存不足的时候才会被回收\",{\"1\":{\"103\":1}}],[\"它只能访问父类中拥有的方法和属性\",{\"1\":{\"9\":1}}],[\"它就会被移除队列\",{\"1\":{\"98\":1}}],[\"它就像一个仪表盘\",{\"1\":{\"59\":1}}],[\"它有三种计算范式\",{\"1\":{\"75\":1}}],[\"它与语言和框架无关\",{\"1\":{\"72\":1}}],[\"它与git无缝协作\",{\"1\":{\"67\":1}}],[\"它检查数据和模型质量\",{\"1\":{\"70\":1}}],[\"它通过提供可扩展的端点来管理负载\",{\"1\":{\"69\":1}}],[\"它通过运行并行推理和自适应批处理来扩展强大的优化功能\",{\"1\":{\"68\":1}}],[\"它通常用于实验跟踪\",{\"1\":{\"59\":1}}],[\"它为您提供了对本地prefect\",{\"1\":{\"64\":1}}],[\"它还允许人们调度和管理一个ml管道的不同阶段\",{\"1\":{\"60\":1}}],[\"它还可用于编排数据工程作业\",{\"1\":{\"57\":1}}],[\"它提供了一个高级的实验跟踪功能\",{\"1\":{\"59\":1}}],[\"它允许端到端管理ml生命周期\",{\"1\":{\"59\":1}}],[\"它可能偏向于llama\",{\"1\":{\"53\":1}}],[\"它可以破解微调数据\",{\"1\":{\"51\":1}}],[\"它采用了字节对编码\",{\"1\":{\"43\":1}}],[\"它的value是\",{\"1\":{\"216\":1}}],[\"它的左孩子和右孩子的高度可能差值会大一些\",{\"1\":{\"153\":1}}],[\"它的默认值会在编译阶段进行赋予\",{\"1\":{\"141\":1}}],[\"它的执行过程如下\",{\"1\":{\"116\":1}}],[\"它的思想就是对没有引用的对象进行标记\",{\"1\":{\"108\":1}}],[\"它的class对象的引用都已经被回收\",{\"1\":{\"100\":1}}],[\"它的类加载器classloader已经被回收\",{\"1\":{\"100\":1}}],[\"它的所有实例对象都已经被回收\",{\"1\":{\"100\":1}}],[\"它的功能之一\",{\"1\":{\"72\":1}}],[\"它的值应该是有界的\",{\"1\":{\"18\":1}}],[\"它的实现对子类是完全透明的\",{\"1\":{\"8\":1}}],[\"它能为每个时间步输出一个独一无二的编码\",{\"1\":{\"18\":1}}],[\"它们是\",{\"1\":{\"199\":1}}],[\"它们的强度依次下降\",{\"1\":{\"101\":1}}],[\"它们的引用次数也都不是0\",{\"1\":{\"95\":1}}],[\"它们均基于prefix\",{\"1\":{\"30\":1}}],[\"它们整体相似\",{\"1\":{\"29\":1}}],[\"它们各有各的擅长之处\",{\"1\":{\"26\":1}}],[\"它们各有优缺点\",{\"1\":{\"19\":1}}],[\"它们分别代表query\",{\"1\":{\"17\":1}}],[\"它们都足够复杂\",{\"1\":{\"16\":1}}],[\"它是server虚拟机的首选\",{\"1\":{\"114\":1}}],[\"它是串行单线程的\",{\"1\":{\"113\":1}}],[\"它是一个mlops工具\",{\"1\":{\"69\":1}}],[\"它是一个开源\",{\"1\":{\"69\":1}}],[\"它是一个开源的轻量级工具\",{\"1\":{\"64\":1}}],[\"它是一个python优先的工具\",{\"1\":{\"68\":1}}],[\"它是为数据科学家构建的\",{\"1\":{\"65\":1}}],[\"它是t5提出的一个统一框架\",{\"1\":{\"24\":1}}],[\"它是注意力机制的一种\",{\"1\":{\"18\":1}}],[\"它是通过动态绑定来实现的\",{\"1\":{\"9\":1}}],[\"它是根据参数列表的不同来区分不同的函数\",{\"1\":{\"9\":1}}],[\"它除了拥有被继承者的特性外\",{\"1\":{\"7\":1}}],[\"对区间进行编码\",{\"1\":{\"241\":1}}],[\"对二维地图做区间划分\",{\"1\":{\"241\":1}}],[\"对应对应的\",{\"1\":{\"236\":1}}],[\"对应的集合的每个\",{\"1\":{\"236\":1}}],[\"对上个步骤中发生变化的内容进行重新标记\",{\"1\":{\"116\":1}}],[\"对堆中的所有位置都进行回收\",{\"1\":{\"92\":1}}],[\"对不同的头指定了不同的γ\",{\"1\":{\"78\":1}}],[\"对象的最高级别是repository\",{\"1\":{\"66\":1}}],[\"对\",{\"1\":{\"41\":1}}],[\"对总token进行了40\",{\"1\":{\"38\":1}}],[\"对glm\",{\"1\":{\"32\":1}}],[\"对input\",{\"1\":{\"21\":1}}],[\"对输入\",{\"1\":{\"18\":1}}],[\"对于字符串索引\",{\"0\":{\"168\":1}}],[\"对于排序查找和范围查找很方便\",{\"1\":{\"157\":1}}],[\"对于二级索引来说\",{\"1\":{\"155\":1}}],[\"对于myisam来说\",{\"1\":{\"155\":1,\"159\":1}}],[\"对于masked掉的词\",{\"1\":{\"27\":1}}],[\"对于innodb来说\",{\"1\":{\"155\":2}}],[\"对于范围查找来说\",{\"1\":{\"154\":1}}],[\"对于哈希表结构来说\",{\"1\":{\"150\":1}}],[\"对于散列函数比较优秀的哈希表来说\",{\"1\":{\"150\":1}}],[\"对于加载流程\",{\"1\":{\"133\":1}}],[\"对于回收过程中产生的引用变化无法回收\",{\"1\":{\"116\":1}}],[\"对于老年代\",{\"1\":{\"113\":1}}],[\"对于新生代\",{\"1\":{\"113\":1}}],[\"对于弱引用\",{\"1\":{\"104\":1}}],[\"对于大内存对象\",{\"1\":{\"90\":1}}],[\"对于jvm堆来说\",{\"1\":{\"86\":1}}],[\"对于l层的retention网络\",{\"1\":{\"79\":1}}],[\"对于llama\",{\"1\":{\"50\":1}}],[\"对于第n个时间步长\",{\"1\":{\"77\":1}}],[\"对于7b型号和8k序列长度\",{\"1\":{\"75\":1}}],[\"对于7b和13b模型\",{\"1\":{\"50\":1}}],[\"对于其他方面\",{\"1\":{\"58\":1}}],[\"对于palm\",{\"1\":{\"54\":1}}],[\"对于position\",{\"1\":{\"28\":1}}],[\"对于chatgpt\",{\"1\":{\"54\":1}}],[\"对于训练指令\",{\"1\":{\"51\":1}}],[\"对于所有模型\",{\"1\":{\"50\":1}}],[\"对于每个节点来说\",{\"1\":{\"152\":1}}],[\"对于每个ppo迭代\",{\"1\":{\"50\":1}}],[\"对于每个prompt\",{\"1\":{\"50\":1}}],[\"对于每个模型\",{\"1\":{\"50\":1}}],[\"对于每个样本来说都是由a和b两句话构成\",{\"1\":{\"22\":1}}],[\"对于提高llama\",{\"1\":{\"49\":1}}],[\"对于微调过程\",{\"1\":{\"46\":1}}],[\"对于监督微调\",{\"1\":{\"46\":1}}],[\"对于34b和70b模型\",{\"1\":{\"50\":1}}],[\"对于34b和70b\",{\"1\":{\"41\":1}}],[\"对于kv缓存大小成为瓶颈的大型模型\",{\"1\":{\"41\":1}}],[\"对于情感分析问题\",{\"1\":{\"31\":1}}],[\"对于glm\",{\"1\":{\"30\":1}}],[\"对于预训练好的语言模型\",{\"1\":{\"29\":1}}],[\"对于token的预测输出用的是单个的线形层\",{\"1\":{\"29\":1}}],[\"对于相对位置的计算更加方便\",{\"1\":{\"18\":1}}],[\"对于面向对象而言\",{\"1\":{\"9\":1}}],[\"对于若干个相同或者相识的类\",{\"1\":{\"7\":1}}],[\"对父类而言\",{\"1\":{\"8\":1}}],[\"父类变\",{\"1\":{\"8\":1}}],[\"谨慎继承\",{\"0\":{\"8\":1}}],[\"子类对父类中某些方法进行重新定义\",{\"1\":{\"10\":1}}],[\"子类就必须变\",{\"1\":{\"8\":1}}],[\"子类可以用自己的方式实现父类的方法\",{\"1\":{\"7\":1}}],[\"子类可以拥有自己的属性和方法\",{\"1\":{\"7\":1}}],[\"子类拥有父类非private的属性和方法\",{\"1\":{\"7\":1}}],[\"他们不仅可以拥有父类的属性\",{\"1\":{\"7\":1}}],[\"诚然\",{\"1\":{\"7\":1}}],[\"向上转型\",{\"1\":{\"7\":1,\"10\":2}}],[\"其data的值不是数据\",{\"1\":{\"155\":1}}],[\"其他消费者读不到了\",{\"1\":{\"215\":1}}],[\"其他位置只存放key\",{\"1\":{\"154\":1}}],[\"其他三种验证就是在方法区中进行的了\",{\"1\":{\"137\":1}}],[\"其他线程是可以看见的\",{\"1\":{\"128\":1}}],[\"其他常量在元空间中\",{\"1\":{\"121\":1}}],[\"其他常量还在方法区中\",{\"1\":{\"121\":1}}],[\"其次\",{\"1\":{\"75\":1,\"108\":1}}],[\"其组件hydrosphere\",{\"1\":{\"72\":1}}],[\"其余参数为1×10−5\",{\"1\":{\"49\":1}}],[\"其余基本没有变化\",{\"1\":{\"25\":1}}],[\"其主要结果如下\",{\"1\":{\"32\":1}}],[\"其主要的修改如下\",{\"1\":{\"20\":1}}],[\"其事实上是将文本生成的prefix\",{\"1\":{\"32\":1}}],[\"其在挖掘语言模型的潜在能力上有着不错的成绩\",{\"1\":{\"30\":1}}],[\"其通用性体现在哪里\",{\"1\":{\"26\":1}}],[\"其优良的效果源于其创新的思想和持续的研究\",{\"1\":{\"26\":1}}],[\"其参数规模进一步增大\",{\"1\":{\"25\":1}}],[\"其参数量已经达到了175b\",{\"1\":{\"14\":1}}],[\"其参数量有1\",{\"1\":{\"14\":1}}],[\"其参数量有117m\",{\"1\":{\"14\":1}}],[\"其成功的关键在于超大的参数规模\",{\"1\":{\"25\":1}}],[\"其能力大家有目共睹\",{\"1\":{\"23\":1,\"25\":1}}],[\"其本质就是通过一个普通的词嵌入来区分每一个序列所处的位置\",{\"1\":{\"21\":1}}],[\"其实上面的三种垃圾收集策略各有各的特点\",{\"1\":{\"111\":1}}],[\"其实在bert中也是这样做的\",{\"1\":{\"27\":1}}],[\"其实multi\",{\"1\":{\"18\":1}}],[\"其实对于这个我们将其称之为\",{\"1\":{\"7\":1}}],[\"其计算公式如下\",{\"1\":{\"18\":1}}],[\"其计算过程与注意力机制一致\",{\"1\":{\"18\":1}}],[\"其比独热编码\",{\"1\":{\"18\":1}}],[\"其结构可以分为输入输出嵌入向量\",{\"1\":{\"18\":1}}],[\"其抛弃了rnn和cnn作为encoder和decoder\",{\"1\":{\"18\":1}}],[\"其中斜体加粗标注的为\",{\"1\":{\"251\":1}}],[\"其中前\",{\"1\":{\"251\":1}}],[\"其中value只能是\",{\"1\":{\"233\":1}}],[\"其中int对应数据结构int\",{\"1\":{\"205\":1}}],[\"其中验证\",{\"1\":{\"133\":1}}],[\"其中保存着一些变量\",{\"1\":{\"131\":1}}],[\"其中新生代中包括伊甸园和两个存活区\",{\"1\":{\"86\":1}}],[\"其中d表示头维度\",{\"1\":{\"78\":1}}],[\"其中\",{\"1\":{\"77\":2}}],[\"其中qn​∈r1×d\",{\"1\":{\"77\":1}}],[\"其中a∈rd×d\",{\"1\":{\"77\":1}}],[\"其中un和an分别对应于回合n的用户和助手消息\",{\"1\":{\"51\":1}}],[\"其中β1=0\",{\"1\":{\"50\":1}}],[\"其中n∈\",{\"1\":{\"50\":1}}],[\"其中奖励被视为能量函数\",{\"1\":{\"50\":1}}],[\"其中只有80\",{\"1\":{\"27\":1}}],[\"其中x~corrupt~就是parta\",{\"1\":{\"27\":1}}],[\"其中s~1~对应着\",{\"1\":{\"27\":1}}],[\"其中每个文本域s~i\",{\"1\":{\"27\":1}}],[\"其中的情况b确实为a的下一句话\",{\"1\":{\"22\":1}}],[\"其中蓝色阴影部分\",{\"1\":{\"20\":1}}],[\"其中因为内容相对重复\",{\"1\":{\"13\":1}}],[\"其中transformer是一种全新的序列转换模型\",{\"1\":{\"13\":1}}],[\"其中编辑时多态是静态的\",{\"1\":{\"9\":1}}],[\"其中b是被继承者称之为父类或者超类\",{\"1\":{\"7\":1}}],[\"例如keras\",{\"1\":{\"68\":1}}],[\"例如scikit\",{\"1\":{\"62\":1}}],[\"例如\",{\"1\":{\"49\":1,\"51\":5,\"56\":1,\"213\":1,\"244\":1,\"248\":1}}],[\"例如在nsp任务中\",{\"1\":{\"21\":1}}],[\"例如我们可以说猫是动物\",{\"1\":{\"7\":1}}],[\"例如猫有抓老鼠\",{\"1\":{\"7\":1}}],[\"爬树等其他动物没有的特性\",{\"1\":{\"7\":1}}],[\"也被看作是包含\",{\"1\":{\"236\":1}}],[\"也会被视作正确答案\",{\"1\":{\"248\":1}}],[\"也会进入established状态\",{\"1\":{\"184\":1}}],[\"也会在这里分配空间\",{\"1\":{\"141\":1}}],[\"也确实\",{\"1\":{\"134\":1}}],[\"也是垃圾回收的主要区域\",{\"1\":{\"122\":1}}],[\"也是bert的核心\",{\"1\":{\"20\":1}}],[\"也叫jvm内存模型\",{\"0\":{\"120\":1}}],[\"也各有各的缺点\",{\"1\":{\"111\":1}}],[\"也就没有办法被回收\",{\"1\":{\"95\":1}}],[\"也就是对于一个节点来说\",{\"1\":{\"153\":1}}],[\"也就是才会在元空间有这个类模板对象\",{\"1\":{\"137\":1}}],[\"也就是尽可能让回收快速进行或者和工作线程并行执行\",{\"1\":{\"116\":1}}],[\"也就是单向的\",{\"1\":{\"27\":1}}],[\"也就是在这里\",{\"1\":{\"22\":1}}],[\"也就是某个词既可以看到它之后的词\",{\"1\":{\"18\":1}}],[\"也就是说\",{\"1\":{\"113\":1,\"248\":1,\"249\":1}}],[\"也就是说被子类覆盖的方法\",{\"1\":{\"12\":1}}],[\"也就是说用户是无需知道对象内部的细节\",{\"1\":{\"5\":1}}],[\"也就是我们所说的多态性\",{\"1\":{\"9\":1}}],[\"也有很多人在努力\",{\"1\":{\"75\":1}}],[\"也有研究人员尝试将上述三种模型结合\",{\"1\":{\"26\":1}}],[\"也只能够通过位置编码的形式来获取位置信息\",{\"1\":{\"28\":1}}],[\"也许通过文字描述比较难以理解\",{\"1\":{\"27\":1}}],[\"也包括token右边的\",{\"1\":{\"22\":1}}],[\"也可以是数字\",{\"1\":{\"202\":1}}],[\"也可以是服务端发起的\",{\"1\":{\"187\":1}}],[\"也可以通过redis\",{\"1\":{\"196\":1}}],[\"也可以通过公式计算得到\",{\"1\":{\"18\":1}}],[\"也可以用于ml工作流程编排\",{\"1\":{\"57\":1}}],[\"也可以用父类的功能\",{\"1\":{\"7\":1}}],[\"也可以让结果很好\",{\"1\":{\"46\":1}}],[\"也可以看到它之前的词\",{\"1\":{\"18\":1}}],[\"二叉查找树是一个二叉树\",{\"1\":{\"151\":1}}],[\"二叉查找树\",{\"0\":{\"151\":1}}],[\"二维位置编码\",{\"0\":{\"28\":1}}],[\"二\",{\"0\":{\"7\":1,\"15\":1}}],[\"实时传输协议\",{\"1\":{\"82\":1}}],[\"实时监控\",{\"1\":{\"70\":1}}],[\"实验\",{\"0\":{\"80\":1}}],[\"实验跟踪\",{\"1\":{\"67\":1}}],[\"实际上是需要回表的\",{\"1\":{\"155\":1}}],[\"实际上这里的拼音和部首\",{\"1\":{\"147\":1}}],[\"实际上\",{\"1\":{\"17\":1}}],[\"实际上继承者是被继承者的特殊化\",{\"1\":{\"7\":1}}],[\"实例阻塞\",{\"1\":{\"223\":1}}],[\"实例分析\",{\"0\":{\"12\":1}}],[\"实例化mifreeformdisplayadapter需要displaymanagerservice的一些字段\",{\"1\":{\"3\":1}}],[\"实现\",{\"1\":{\"241\":1}}],[\"实现多态的方法\",{\"0\":{\"11\":1}}],[\"实现细节\",{\"1\":{\"6\":1}}],[\"隐藏信息\",{\"1\":{\"6\":1}}],[\"良好的封装能够减少耦合\",{\"1\":{\"6\":1}}],[\"使合并后的数组同样按\",{\"1\":{\"251\":1}}],[\"使得出现次数超过两次的元素只出现两次\",{\"1\":{\"249\":1}}],[\"使\",{\"1\":{\"247\":1}}],[\"使每个元素\",{\"1\":{\"247\":1}}],[\"使其不那么冗长\",{\"1\":{\"51\":1}}],[\"使其构成一个不可分割的独立实体\",{\"1\":{\"5\":1}}],[\"使用方法比较简单\",{\"1\":{\"253\":1}}],[\"使用哈希表来存储\",{\"1\":{\"221\":1}}],[\"使用listpack数据结构了\",{\"1\":{\"217\":1}}],[\"使用双向链表\",{\"1\":{\"209\":1}}],[\"使用数据集快照的方式半持久化模式记录redis数据库的所有键值对\",{\"1\":{\"196\":1}}],[\"使用\",{\"0\":{\"144\":1}}],[\"使用automl框架\",{\"1\":{\"73\":1}}],[\"使用cml持续集成和部署机器学习\",{\"1\":{\"67\":1}}],[\"使用选定的输出进行梯度更新\",{\"1\":{\"50\":1}}],[\"使用一个特殊的令牌来分隔提示段和应答段\",{\"1\":{\"46\":1}}],[\"使用了余弦学习率\",{\"1\":{\"46\":1}}],[\"使用了\",{\"1\":{\"46\":1}}],[\"使用了来自sentencepiece的实现\",{\"1\":{\"43\":1}}],[\"使用与llama\",{\"1\":{\"43\":1}}],[\"使用0\",{\"1\":{\"42\":1,\"50\":1}}],[\"使用封装的优点\",{\"0\":{\"6\":1}}],[\"但输出的答案是数组呢\",{\"1\":{\"248\":1,\"249\":1}}],[\"但要注意\",{\"1\":{\"237\":1}}],[\"但您也可以将其用于再现性\",{\"1\":{\"59\":1}}],[\"但可以说\",{\"1\":{\"53\":1}}],[\"但可以通过该对象对外的提供的接口来访问该对象\",{\"1\":{\"5\":1}}],[\"但这会导致系统消息\",{\"1\":{\"51\":1}}],[\"但在生成过程中会导致很大的减慢\",{\"1\":{\"50\":1}}],[\"但并不是一个实际的语言模型\",{\"1\":{\"19\":1}}],[\"但是它跟jdk中其他的native方法实现地方不同\",{\"1\":{\"253\":1}}],[\"但是它节点有一些特点\",{\"1\":{\"151\":1}}],[\"但是只需要进行二值存储的内容\",{\"1\":{\"234\":1}}],[\"但是要提醒你一下\",{\"1\":{\"223\":1}}],[\"但是失败了\",{\"1\":{\"214\":1}}],[\"但是list\",{\"1\":{\"213\":1}}],[\"但是这样可能会有一个问题\",{\"1\":{\"212\":1}}],[\"但是这个被调用的方法必须是在超类中定义过的\",{\"1\":{\"12\":1}}],[\"但是分布式部署的就不行了\",{\"1\":{\"207\":1}}],[\"但是对象编码是raw\",{\"1\":{\"205\":1}}],[\"但是对于多线程来说\",{\"1\":{\"129\":1}}],[\"但是不一定\",{\"1\":{\"253\":1}}],[\"但是不推荐使用select\",{\"1\":{\"173\":1}}],[\"但是不像对象那样\",{\"1\":{\"100\":1}}],[\"但是也需要消耗时间来维护\",{\"1\":{\"165\":1}}],[\"但是也不是非死不可\",{\"1\":{\"98\":1}}],[\"但是查询\",{\"1\":{\"161\":1}}],[\"但是红黑树并不是完全保证平衡的\",{\"1\":{\"153\":1}}],[\"但是因为每次插入或者删除\",{\"1\":{\"152\":1}}],[\"但是因为工作线程执行过程对象的引用可能会发生变化\",{\"1\":{\"116\":1}}],[\"但是二叉查找树可能会退化到一条链\",{\"1\":{\"151\":1}}],[\"但是进行范围查找\",{\"1\":{\"150\":1}}],[\"但是如果从头开始看\",{\"1\":{\"249\":1}}],[\"但是如果尝试使用o\",{\"1\":{\"246\":1}}],[\"但是如果改变了查询的值就会了\",{\"1\":{\"174\":1}}],[\"但是如果通过前面的目录进行拼音查找或者部首查找\",{\"1\":{\"147\":1}}],[\"但是如果强制把超类转换成子类的话\",{\"1\":{\"12\":1}}],[\"但是会牺牲一些吞吐量\",{\"1\":{\"118\":1}}],[\"但是引用计数器有个问题\",{\"1\":{\"95\":1}}],[\"但是用mps会失败\",{\"1\":{\"35\":1}}],[\"但是最终似乎并没有成功\",{\"1\":{\"35\":1}}],[\"但是最大的t5模型达到了11b\",{\"1\":{\"14\":1}}],[\"但是其性能并没有p\",{\"1\":{\"32\":1}}],[\"但是其性能仍然不如全参数微调\",{\"1\":{\"30\":1}}],[\"但是其自然语言理解能力非常强\",{\"1\":{\"14\":1}}],[\"但是全参数微调的设备需求仍然很大\",{\"1\":{\"30\":1}}],[\"但是都有自己的创新点\",{\"1\":{\"29\":1}}],[\"但是glm没有这样做\",{\"1\":{\"27\":1}}],[\"但是在当时\",{\"1\":{\"20\":1}}],[\"但是在解码阶段\",{\"1\":{\"18\":1}}],[\"但是单纯的multi\",{\"1\":{\"18\":1}}],[\"但是\",{\"1\":{\"18\":1,\"29\":1,\"30\":1,\"145\":1}}],[\"但是微调参数量仅为全参数微调的3\",{\"1\":{\"14\":1}}],[\"但是语言模型的研究并不是最近才兴起的\",{\"1\":{\"14\":1}}],[\"但是部分内容被隐藏的类时\",{\"1\":{\"3\":1}}],[\"但不能说动物是猫就是这个道理\",{\"1\":{\"7\":1}}],[\"但不能选择性地继承父类\",{\"1\":{\"7\":1}}],[\"系统会用下面的代码来测试你的题解\",{\"1\":{\"247\":1}}],[\"系统部署\",{\"1\":{\"55\":1}}],[\"系统开发\",{\"1\":{\"55\":1}}],[\"系统的其他对象只能通过包裹在数据外面的已经授权的操作来与这个封装的对象进行交流和交互\",{\"1\":{\"5\":1}}],[\"系统还引入了selinux\",{\"1\":{\"3\":1}}],[\"一致\",{\"1\":{\"247\":1}}],[\"一组经纬度落在某个区间后\",{\"1\":{\"241\":1}}],[\"一共有\",{\"1\":{\"236\":1}}],[\"一年也就365bit\",{\"1\":{\"234\":1}}],[\"一切完成之后就可以进行使用了\",{\"1\":{\"144\":1}}],[\"一些临时随写内容\",{\"0\":{\"81\":1}}],[\"一些指示应适用于所有的对话轮数中\",{\"1\":{\"51\":1}}],[\"一种并行表示和循环表示的混合形式可用于加速训练\",{\"1\":{\"77\":1}}],[\"一层一层地计算\",{\"1\":{\"76\":1}}],[\"一般masked掉的是一个词\",{\"1\":{\"27\":1}}],[\"一般会mask掉句子的15\",{\"1\":{\"22\":1}}],[\"一个文件可以非常方便地存储到安全的磁盘中\",{\"1\":{\"196\":1}}],[\"一个开源的\",{\"1\":{\"190\":1}}],[\"一个节点是黑色的话\",{\"1\":{\"153\":1}}],[\"一个红黑树有以下5个特点\",{\"1\":{\"153\":1}}],[\"一个类被卸载的概率很小\",{\"1\":{\"145\":1}}],[\"一个类要进行回收\",{\"1\":{\"100\":1}}],[\"一个线程对主内存的变量进行写入\",{\"1\":{\"131\":1}}],[\"一个对象被垃圾回收\",{\"1\":{\"98\":1}}],[\"一个对象出生在伊甸园\",{\"1\":{\"91\":1}}],[\"一个模型注册表和模型服务组件\",{\"1\":{\"59\":1}}],[\"一个多卡并行训练的论文\",{\"1\":{\"41\":1}}],[\"一个字可能意义不如多个字组成的意义大\",{\"1\":{\"27\":1}}],[\"一个双向编码语言模型\",{\"1\":{\"19\":1}}],[\"一个encoder单元\",{\"1\":{\"18\":1}}],[\"一个好的位置编码方案需要满足以下几个条件\",{\"1\":{\"18\":1}}],[\"一个序列转换\",{\"1\":{\"16\":1}}],[\"一\",{\"0\":{\"5\":1,\"14\":1}}],[\"封装从字面上来理解就是包装的意思\",{\"1\":{\"5\":1}}],[\"封装\",{\"0\":{\"4\":1,\"5\":1}}],[\"vm会对其进行一些优化\",{\"1\":{\"253\":1}}],[\"vm内部的方法\",{\"1\":{\"253\":1}}],[\"vm内部\",{\"1\":{\"253\":1}}],[\"vm\",{\"1\":{\"253\":1}}],[\"vm​\",{\"1\":{\"77\":1}}],[\"v7\",{\"1\":{\"217\":1}}],[\"v存储的数据格式\",{\"1\":{\"216\":1}}],[\"v5\",{\"1\":{\"201\":1}}],[\"v3\",{\"1\":{\"201\":1,\"209\":1}}],[\"v3之后的两个轴上都优于chatgpt\",{\"1\":{\"53\":1}}],[\"v中的\",{\"1\":{\"77\":1}}],[\"version\",{\"0\":{\"67\":1},\"1\":{\"67\":1}}],[\"veselin\",{\"1\":{\"33\":1}}],[\"view=azure\",{\"1\":{\"60\":1}}],[\"video\",{\"1\":{\"58\":1}}],[\"vicuna\",{\"1\":{\"54\":1}}],[\"volume\",{\"1\":{\"33\":1}}],[\"void\",{\"1\":{\"3\":2,\"12\":1,\"105\":1,\"253\":2}}],[\"vaswani\",{\"1\":{\"33\":1}}],[\"value最长可容纳的数据长度是512m\",{\"1\":{\"202\":1}}],[\"value不只是字符串\",{\"1\":{\"202\":1}}],[\"value结构\",{\"1\":{\"202\":1}}],[\"value或者set\",{\"1\":{\"198\":1}}],[\"value数据库\",{\"1\":{\"190\":1}}],[\"value\",{\"1\":{\"17\":1,\"75\":1,\"198\":2,\"210\":2,\"216\":2,\"218\":3,\"233\":2,\"235\":1}}],[\"val\",{\"1\":{\"3\":3,\"248\":6}}],[\"v是encoder的输出计算的\",{\"1\":{\"18\":1}}],[\"v\",{\"1\":{\"17\":1,\"41\":1,\"77\":10}}],[\"v21\",{\"1\":{\"33\":1}}],[\"v2好\",{\"1\":{\"32\":1}}],[\"v2几乎一致\",{\"1\":{\"32\":1}}],[\"v2进行微调\",{\"1\":{\"32\":1}}],[\"v2进行介绍\",{\"1\":{\"30\":1}}],[\"v2对glm进行微调\",{\"1\":{\"32\":1}}],[\"v2也是清华大学发布的\",{\"1\":{\"32\":1}}],[\"v2还包括以下改进\",{\"1\":{\"32\":1}}],[\"v2共有50\",{\"1\":{\"32\":1}}],[\"v2可以微调的参数变多了\",{\"1\":{\"32\":1}}],[\"v2实际上就是prefix\",{\"1\":{\"32\":1}}],[\"v2的关键所在就是引入了prefix\",{\"1\":{\"32\":1}}],[\"v2并不是一个全新的方法\",{\"1\":{\"32\":1}}],[\"v2应该属于提示微调\",{\"1\":{\"31\":1}}],[\"v2属于部分参数微调\",{\"1\":{\"31\":1}}],[\"v2基本上是同一时期发布的\",{\"1\":{\"30\":1}}],[\"v2是一种对预训练语言模型进行高效微调的方法\",{\"1\":{\"13\":1}}],[\"v2\",{\"0\":{\"30\":1,\"32\":1},\"1\":{\"13\":1,\"14\":2,\"30\":1,\"32\":2,\"201\":2,\"205\":1}}],[\"+\",{\"1\":{\"12\":9,\"77\":3,\"175\":2,\"235\":1,\"251\":4,\"253\":1}}],[\"+service\",{\"1\":{\"3\":1}}],[\"+b+\",{\"1\":{\"3\":1}}],[\"bsd\",{\"1\":{\"190\":1}}],[\"bsz\",{\"1\":{\"77\":7}}],[\"b=2是可以走索引的\",{\"1\":{\"161\":1}}],[\"b+树有很多优势\",{\"1\":{\"154\":1}}],[\"b+树的叶子节点之间是通过一个链连在一起的\",{\"1\":{\"154\":1}}],[\"b+树只有叶子节点存放key和data\",{\"1\":{\"154\":1}}],[\"b+树是b树的一个改进\",{\"1\":{\"154\":1}}],[\"b+树\",{\"0\":{\"154\":1}}],[\"b树需要查找到左范围\",{\"1\":{\"154\":1}}],[\"b树进行查找只需要查到key就可以了\",{\"1\":{\"154\":1}}],[\"b树的叶子节点是相互独立的\",{\"1\":{\"154\":1}}],[\"b树所有位置都可以存放key和data\",{\"1\":{\"154\":1}}],[\"b树和b+树的区别如下\",{\"1\":{\"154\":1}}],[\"b树叫做多路平衡查找树\",{\"1\":{\"154\":1}}],[\"b树\",{\"0\":{\"154\":1}}],[\"blpop\",{\"1\":{\"210\":1,\"212\":1}}],[\"block包括两个模块\",{\"1\":{\"76\":1}}],[\"blog\",{\"1\":{\"61\":1,\"72\":2}}],[\"blank\",{\"1\":{\"27\":1}}],[\"bv1g14y1972c\",{\"1\":{\"58\":1}}],[\"bpe\",{\"1\":{\"43\":1}}],[\"batch\",{\"1\":{\"46\":1,\"50\":2}}],[\"bao\",{\"1\":{\"33\":1}}],[\"based\",{\"1\":{\"50\":1,\"240\":1}}],[\"base的参数规模是110m\",{\"1\":{\"20\":1}}],[\"base的100倍大小\",{\"1\":{\"14\":1}}],[\"base\",{\"1\":{\"3\":1,\"20\":3}}],[\"brpop\",{\"1\":{\"210\":1}}],[\"branches\",{\"1\":{\"66\":1}}],[\"brain就发布了一个全新的序列转换模型\",{\"1\":{\"14\":1}}],[\"brian\",{\"1\":{\"33\":1}}],[\"between\",{\"1\":{\"161\":1}}],[\"before原则\",{\"1\":{\"129\":1}}],[\"best\",{\"1\":{\"71\":1,\"72\":1}}],[\"bentoml为模型部署\",{\"1\":{\"68\":1}}],[\"bentoml的交互式集中式仪表板可以在部署机器学习模型时轻松组织和监控\",{\"1\":{\"68\":1}}],[\"bentoml使机器学习应用程序的发布变得更简单\",{\"1\":{\"68\":1}}],[\"bentoml\",{\"0\":{\"68\":1},\"1\":{\"68\":3}}],[\"be\",{\"1\":{\"33\":1,\"49\":1}}],[\"bert通过这种方式来保留一些被masked的原始信息\",{\"1\":{\"27\":1}}],[\"bert模型是自编码模型\",{\"1\":{\"25\":1}}],[\"bert模型结构图\",{\"1\":{\"20\":1}}],[\"bert在预训练中加入了nsp\",{\"1\":{\"22\":1}}],[\"bert在当时的条件下认为\",{\"1\":{\"19\":1}}],[\"bert把这15\",{\"1\":{\"22\":1}}],[\"bert的效果遥遥领先于同期其他语言模型\",{\"1\":{\"22\":1}}],[\"bert的论文中没有提为什么是15\",{\"1\":{\"22\":1}}],[\"bert的预训练没有采用传统的自左向右或者自右向左语言模型来训练bert\",{\"1\":{\"22\":1}}],[\"bert的基础模型有110m参数\",{\"1\":{\"14\":1}}],[\"bert对positional\",{\"1\":{\"21\":1}}],[\"bert对encoder进行了一些修改\",{\"1\":{\"20\":1}}],[\"bert相对transformer来说\",{\"1\":{\"21\":1}}],[\"bert只用了transformer的encoder部分\",{\"1\":{\"20\":1}}],[\"bert并没有把transformer拿来直接用\",{\"1\":{\"20\":1}}],[\"bert与transformer\",{\"0\":{\"20\":1}}],[\"bert诞生了\",{\"1\":{\"19\":1}}],[\"bert是一个双向编码模型\",{\"1\":{\"19\":1}}],[\"bert全称bidirectional\",{\"1\":{\"19\":1}}],[\"bert和t5都是google的精彩操作\",{\"1\":{\"14\":1}}],[\"bert\",{\"0\":{\"19\":1},\"1\":{\"13\":2,\"20\":8,\"21\":1,\"26\":1,\"33\":2}}],[\"by\",{\"0\":{\"178\":1},\"1\":{\"33\":1,\"178\":1}}],[\"bit\",{\"1\":{\"236\":6}}],[\"bitvalue\",{\"1\":{\"235\":2}}],[\"bitpos\",{\"1\":{\"233\":1,\"235\":2}}],[\"bitop\",{\"1\":{\"233\":2,\"236\":3}}],[\"bitcount\",{\"1\":{\"233\":1,\"235\":1,\"236\":3}}],[\"bitmap适合存储一些数据量很大\",{\"1\":{\"234\":1}}],[\"bitmap间的运算\",{\"1\":{\"233\":1}}],[\"bitmap可以看作是一个bit数组\",{\"1\":{\"232\":1}}],[\"bitmap底层通过string来实现\",{\"1\":{\"232\":1}}],[\"bitmap通过计算机中的最小单位bit来进行0\",{\"1\":{\"231\":1}}],[\"bitmap\",{\"0\":{\"231\":1},\"1\":{\"201\":1,\"231\":1,\"233\":2,\"235\":1,\"236\":13}}],[\"bitmap位图\",{\"1\":{\"193\":1}}],[\"biases是一个用于实验跟踪\",{\"1\":{\"63\":1}}],[\"biases\",{\"0\":{\"63\":1}}],[\"bilibili\",{\"1\":{\"58\":1}}],[\"billion\",{\"1\":{\"41\":1}}],[\"bison聊天模型\",{\"1\":{\"54\":1}}],[\"bison\",{\"1\":{\"54\":1}}],[\"bidirectional\",{\"1\":{\"33\":1}}],[\"binderservice\",{\"1\":{\"3\":1}}],[\"binder\",{\"1\":{\"3\":4}}],[\"b部分\",{\"1\":{\"27\":1}}],[\"but\",{\"1\":{\"18\":2}}],[\"boolean\",{\"1\":{\"3\":3}}],[\"b\",{\"1\":{\"3\":2,\"12\":17,\"27\":1,\"32\":1,\"105\":1,\"161\":5}}],[\"此后\",{\"1\":{\"91\":1}}],[\"此外\",{\"1\":{\"18\":1,\"21\":1,\"32\":1,\"58\":1,\"63\":1,\"64\":1,\"69\":1,\"71\":1,\"72\":2,\"75\":3,\"78\":1,\"193\":1,\"252\":1}}],[\"此处使用了androidhiddenapibypass\",{\"1\":{\"3\":1}}],[\"此时可以看的ptr是指向sds的地址的\",{\"1\":{\"205\":1}}],[\"此时没有设置expire怎么办\",{\"1\":{\"198\":1}}],[\"此时服务端也可以确认自己的发送是正常的\",{\"1\":{\"185\":1}}],[\"此时服务端进到syn\",{\"1\":{\"184\":1}}],[\"此时客户端进入到established状态\",{\"1\":{\"184\":1}}],[\"此时客户端进入到syn\",{\"1\":{\"184\":1}}],[\"此时\",{\"1\":{\"3\":1,\"185\":1}}],[\"$binder\",{\"1\":{\"3\":1}}],[\"hlen\",{\"1\":{\"218\":1}}],[\"hl=zh\",{\"1\":{\"56\":1}}],[\"hdel\",{\"1\":{\"218\":1}}],[\"hmget\",{\"1\":{\"218\":1}}],[\"hmset\",{\"1\":{\"218\":1}}],[\"hgetall\",{\"1\":{\"218\":1}}],[\"hget\",{\"1\":{\"218\":1}}],[\"hset\",{\"1\":{\"218\":1}}],[\"hsiao\",{\"1\":{\"33\":1}}],[\"hyperloglog\",{\"0\":{\"237\":1},\"1\":{\"193\":1,\"201\":1,\"237\":6,\"238\":4}}],[\"hydrosphere还提供与当前机器学习流程的快速合并\",{\"1\":{\"72\":1}}],[\"hydrosphere允许用户提供在任何框架中开发的模型\",{\"1\":{\"72\":1}}],[\"hydrosphere对贡献发生变化的时间给出了明确的解释\",{\"1\":{\"72\":1}}],[\"hydrosphere提供了模型预测的简单摘要\",{\"1\":{\"72\":1}}],[\"hydrosphere是一个用于在生产环境中部署\",{\"1\":{\"72\":1}}],[\"hydrosphere\",{\"0\":{\"72\":1},\"1\":{\"72\":3}}],[\"hincrby\",{\"1\":{\"218\":1}}],[\"history和provenance来跟踪和版本化数据集\",{\"1\":{\"66\":1}}],[\"hiddenapi冲突\",{\"1\":{\"3\":1}}],[\"hiddenapibypass\",{\"1\":{\"3\":1}}],[\"hash类型的底层数据结构是压缩列表或者哈希表\",{\"1\":{\"217\":1}}],[\"hash特别适用于存储对象\",{\"1\":{\"216\":1}}],[\"hash是k\",{\"1\":{\"216\":1}}],[\"hash\",{\"0\":{\"150\":1,\"216\":1},\"1\":{\"193\":1,\"200\":1,\"237\":1}}],[\"hashmap<>\",{\"1\":{\"105\":1}}],[\"hashmap<string\",{\"1\":{\"105\":1}}],[\"hashmap\",{\"1\":{\"105\":1}}],[\"hardlinkstoavoidgarbagecollection\",{\"1\":{\"103\":4}}],[\"harmlessness\",{\"1\":{\"50\":1,\"51\":1}}],[\"hangbo\",{\"1\":{\"33\":1}}],[\"hu\",{\"1\":{\"33\":1}}],[\"huang\",{\"1\":{\"33\":1}}],[\"html\",{\"1\":{\"33\":1}}],[\"https\",{\"1\":{\"35\":1,\"56\":1,\"57\":1,\"58\":4,\"59\":2,\"60\":1,\"62\":1,\"63\":1,\"64\":1,\"66\":1,\"67\":1,\"68\":3,\"69\":1,\"70\":2,\"71\":1,\"72\":2}}],[\"http\",{\"1\":{\"33\":1,\"73\":1,\"82\":1}}],[\"h\",{\"1\":{\"20\":2}}],[\"helpfulness\",{\"1\":{\"49\":2}}],[\"headi​\",{\"1\":{\"78\":1}}],[\"head\",{\"1\":{\"18\":6,\"75\":1,\"77\":12,\"78\":1}}],[\"height\",{\"1\":{\"3\":2}}],[\"hon\",{\"1\":{\"33\":1}}],[\"hotspot\",{\"1\":{\"253\":1}}],[\"hot\",{\"1\":{\"18\":1}}],[\"hook\",{\"1\":{\"3\":1}}],[\"pfmerge\",{\"1\":{\"238\":1}}],[\"pfcount\",{\"1\":{\"238\":1}}],[\"pfadd\",{\"1\":{\"238\":1}}],[\"px\",{\"1\":{\"198\":1}}],[\"platforms\",{\"1\":{\"72\":1}}],[\"pb数据\",{\"1\":{\"66\":1}}],[\"pytorch等\",{\"1\":{\"72\":1}}],[\"pytorch和scikit\",{\"1\":{\"68\":1}}],[\"pytorch\",{\"1\":{\"62\":1}}],[\"python\",{\"1\":{\"59\":1,\"66\":1,\"72\":1}}],[\"pipelines\",{\"0\":{\"60\":1},\"1\":{\"60\":1}}],[\"piao\",{\"1\":{\"33\":1}}],[\"ppo介绍\",{\"1\":{\"50\":1}}],[\"ppo\",{\"1\":{\"50\":1}}],[\"pseudo\",{\"1\":{\"33\":1}}],[\"percy\",{\"1\":{\"33\":1}}],[\"peter\",{\"1\":{\"33\":1}}],[\"post\",{\"1\":{\"171\":1}}],[\"position\",{\"1\":{\"28\":1}}],[\"positional\",{\"1\":{\"20\":1,\"40\":1}}],[\"pop3\",{\"1\":{\"82\":1}}],[\"policy\",{\"1\":{\"50\":1}}],[\"polosukhin\",{\"1\":{\"33\":1}}],[\"power\",{\"1\":{\"33\":1}}],[\"package\",{\"1\":{\"105\":1}}],[\"pachyderm通过kubernetes上的数据版本化\",{\"1\":{\"66\":1}}],[\"pachyderm\",{\"0\":{\"66\":1},\"1\":{\"66\":1}}],[\"past\",{\"1\":{\"77\":5}}],[\"pairs\",{\"1\":{\"49\":1}}],[\"papers\",{\"1\":{\"33\":1}}],[\"pages\",{\"1\":{\"33\":3}}],[\"parnew和serial类似\",{\"1\":{\"114\":1}}],[\"parnew\",{\"0\":{\"114\":1},\"1\":{\"112\":1}}],[\"parallelretention\",{\"1\":{\"77\":1}}],[\"parallel\",{\"0\":{\"115\":1},\"1\":{\"50\":1,\"112\":1,\"115\":1}}],[\"parallelism\",{\"1\":{\"41\":1}}],[\"parameter\",{\"1\":{\"33\":1,\"41\":1}}],[\"parikh\",{\"1\":{\"33\":1}}],[\"parmar\",{\"1\":{\"33\":1}}],[\"parta和partb\",{\"1\":{\"28\":1}}],[\"partb可以自回归地看到已经走过的partb和全部的parta\",{\"1\":{\"27\":1}}],[\"path\",{\"1\":{\"3\":1}}],[\"p\",{\"0\":{\"30\":1,\"32\":1,\"41\":1},\"1\":{\"13\":1,\"14\":1,\"30\":1,\"31\":1,\"32\":7,\"33\":2,\"58\":1,\"59\":1,\"68\":1,\"70\":1,\"133\":1}}],[\"print\",{\"1\":{\"248\":1,\"249\":1}}],[\"prints\",{\"1\":{\"33\":1}}],[\"println\",{\"1\":{\"12\":9,\"105\":4}}],[\"projectpro\",{\"1\":{\"71\":1}}],[\"proximal\",{\"1\":{\"50\":1}}],[\"proceedings\",{\"1\":{\"33\":1}}],[\"processing\",{\"1\":{\"33\":3}}],[\"proc\",{\"1\":{\"33\":1}}],[\"product\",{\"1\":{\"18\":1}}],[\"prompts\",{\"1\":{\"33\":1}}],[\"prompt\",{\"1\":{\"14\":1,\"30\":2,\"31\":1,\"33\":2,\"49\":1}}],[\"prefect是一个现代化的数据堆栈\",{\"1\":{\"64\":1}}],[\"prefect\",{\"0\":{\"64\":1},\"1\":{\"64\":3}}],[\"preference\",{\"1\":{\"49\":1}}],[\"preferred\",{\"1\":{\"49\":1}}],[\"prefixtuning\",{\"1\":{\"33\":1}}],[\"prefix参数进行微调\",{\"1\":{\"32\":1}}],[\"prefix为前缀\",{\"1\":{\"32\":1}}],[\"prefix\",{\"1\":{\"14\":1,\"30\":2,\"32\":4}}],[\"preprint\",{\"1\":{\"33\":3}}],[\"pretraining\",{\"1\":{\"33\":2}}],[\"pre\",{\"1\":{\"25\":1,\"33\":3,\"76\":1}}],[\"prediction\",{\"1\":{\"22\":1}}],[\"presentationdeadlinenanos\",{\"1\":{\"3\":1}}],[\"put\",{\"1\":{\"3\":1,\"105\":2}}],[\"public\",{\"1\":{\"3\":2,\"12\":10,\"103\":1,\"105\":2,\"253\":2}}],[\"而其他的会在jdk库中实现\",{\"1\":{\"253\":1}}],[\"而属于hotspot\",{\"1\":{\"253\":1}}],[\"而无需逐一比较\",{\"1\":{\"252\":1}}],[\"而\",{\"1\":{\"248\":1}}],[\"而raw会通过两次内存分配来分别分配两块内存空间来保存redisobject和sds\",{\"1\":{\"206\":1}}],[\"而如果有了第四次挥手\",{\"1\":{\"188\":1}}],[\"而如果改为使用p\",{\"1\":{\"32\":1}}],[\"而这个报文可能在传输过程中丢失了\",{\"1\":{\"188\":1}}],[\"而myisam主键索引也是非聚簇索引\",{\"1\":{\"157\":1}}],[\"而b+树通过叶子节点的链表即可\",{\"1\":{\"154\":1}}],[\"而b+树必须要查到叶子节点\",{\"1\":{\"154\":1}}],[\"而bert的位置编码\",{\"1\":{\"21\":1}}],[\"而红黑树每次都只进行一次左旋或者右旋\",{\"1\":{\"153\":1}}],[\"而引用类型是通过类加载器加载的\",{\"1\":{\"133\":1}}],[\"而weakhashmap的键是弱引用的\",{\"1\":{\"105\":1}}],[\"而且要能查询相邻的经纬度范围\",{\"1\":{\"240\":1}}],[\"而且没有内存分配担保\",{\"1\":{\"111\":1}}],[\"而且\",{\"1\":{\"78\":1,\"204\":1}}],[\"而中值保持不变\",{\"1\":{\"50\":1}}],[\"而不是通过终止符\",{\"1\":{\"204\":1}}],[\"而不是三次挥手\",{\"0\":{\"188\":1}}],[\"而不是两次或者四次\",{\"0\":{\"185\":1}}],[\"而不是担心mlops工程\",{\"1\":{\"65\":1}}],[\"而不是用指令来增加所有上下文对话回合\",{\"1\":{\"51\":1}}],[\"而不会导致性能大幅下降\",{\"1\":{\"41\":1}}],[\"而不能看到它之后的token\",{\"1\":{\"19\":1}}],[\"而对ppo只进行一次生成\",{\"1\":{\"50\":1}}],[\"而对glm进行微调同样还可以使用lora\",{\"1\":{\"32\":1}}],[\"而对于子类中存在而父类中不存在的方法\",{\"1\":{\"9\":1}}],[\"而另外10\",{\"1\":{\"27\":1}}],[\"而另一边的openai也不甘落后\",{\"1\":{\"14\":1}}],[\"而glm中可能会masked掉连续的多个字\",{\"1\":{\"27\":1}}],[\"而gpt模型是自回归模型\",{\"1\":{\"25\":1}}],[\"而与bert等masked不同\",{\"1\":{\"27\":1}}],[\"而每一个文本域都会被一个单独的token\",{\"1\":{\"27\":1}}],[\"而自编码模型擅长自然语言理解任务\",{\"1\":{\"26\":1}}],[\"而自编码模型更加适合自然语言理解任务\",{\"1\":{\"19\":1}}],[\"而首个将transformer应用到实际语言模型中的\",{\"1\":{\"19\":1}}],[\"而多头注意力机制是由缩放点积注意力机制\",{\"1\":{\"18\":1}}],[\"而是存储在数组\",{\"1\":{\"251\":1}}],[\"而是直接使用了\",{\"1\":{\"241\":1}}],[\"而是数据的指针或者主键的值\",{\"1\":{\"158\":1}}],[\"而是指向了数据的位置\",{\"1\":{\"157\":1}}],[\"而是主键的值\",{\"1\":{\"155\":1}}],[\"而是暂停工作线程的\",{\"1\":{\"117\":1}}],[\"而是统一使用标记\",{\"1\":{\"116\":1}}],[\"而是把仍然需要存活的对象给整理到一块\",{\"1\":{\"110\":1}}],[\"而是要用\",{\"1\":{\"103\":1}}],[\"而是提供一种直接的方法\",{\"1\":{\"58\":1}}],[\"而是在每一层都进行微调\",{\"1\":{\"32\":1}}],[\"而是在程序运行期间才确定\",{\"1\":{\"9\":1}}],[\"而是将其放到了part\",{\"1\":{\"27\":1}}],[\"而是做了一些修改\",{\"1\":{\"25\":1}}],[\"而是其decoder部分\",{\"1\":{\"25\":1}}],[\"而是采用的mlm\",{\"1\":{\"22\":1}}],[\"而是只用到了encoder部分\",{\"1\":{\"20\":1}}],[\"而是用这个向量让每个词具有它在句子序列中的位置信息\",{\"1\":{\"18\":1}}],[\"而是包含句子中特定位置信息的d维向量\",{\"1\":{\"18\":1}}],[\"而在计算方式上\",{\"1\":{\"17\":1}}],[\"而在日常使用时\",{\"1\":{\"3\":1}}],[\"而transformer\",{\"1\":{\"16\":1}}],[\"而2020年\",{\"1\":{\"14\":1}}],[\"而运行时多态是动态的\",{\"1\":{\"9\":1}}],[\"而非inout\",{\"1\":{\"3\":1}}],[\"而dms类在该jar包中\",{\"1\":{\"3\":1}}],[\"发现训练时间过长会导致过度拟合\",{\"1\":{\"49\":1}}],[\"发现会抛出noclassdeffounderror异常\",{\"1\":{\"3\":1}}],[\"发现日志中有关adb的日志如下\",{\"1\":{\"1\":1}}],[\"old\",{\"1\":{\"92\":1}}],[\"or\",{\"0\":{\"179\":1},\"1\":{\"233\":1,\"236\":1}}],[\"order\",{\"0\":{\"178\":1},\"1\":{\"178\":1}}],[\"orion实例和工作流的深入了解\",{\"1\":{\"64\":1}}],[\"orion\",{\"1\":{\"64\":2}}],[\"org\",{\"1\":{\"33\":1,\"57\":1,\"58\":1,\"59\":1,\"67\":1}}],[\"operation\",{\"1\":{\"236\":2}}],[\"operations\",{\"1\":{\"55\":1,\"233\":2}}],[\"open\",{\"1\":{\"72\":1}}],[\"ops\",{\"1\":{\"55\":1}}],[\"optimization\",{\"1\":{\"50\":1}}],[\"optimized\",{\"1\":{\"33\":1}}],[\"optimizing\",{\"1\":{\"33\":1}}],[\"omer\",{\"1\":{\"33\":1}}],[\"ott\",{\"1\":{\"33\":1}}],[\"only\",{\"1\":{\"196\":1}}],[\"on​=m=1∑n​γn−m\",{\"1\":{\"77\":1}}],[\"on​=m=1∑n​qn​\",{\"1\":{\"77\":1}}],[\"on​=qn​sn​=m=1∑n​qn​an−mkmt​vm​\",{\"1\":{\"77\":1}}],[\"on​=o\",{\"1\":{\"77\":1}}],[\"onnx\",{\"1\":{\"68\":1}}],[\"on\",{\"1\":{\"33\":1,\"49\":1,\"50\":1}}],[\"one\",{\"1\":{\"18\":1,\"49\":1}}],[\"offer\",{\"0\":{\"259\":1}}],[\"offset\",{\"1\":{\"229\":3,\"233\":2,\"235\":2,\"236\":4}}],[\"of\",{\"1\":{\"18\":2,\"33\":6,\"49\":1}}],[\"o\",{\"1\":{\"12\":4,\"245\":2,\"246\":1,\"248\":1,\"249\":1,\"251\":1}}],[\"obj\",{\"1\":{\"12\":4}}],[\"object\",{\"1\":{\"3\":2,\"103\":3,\"253\":2}}],[\"owncontentonly\",{\"1\":{\"3\":1}}],[\"overview\",{\"1\":{\"55\":1}}],[\"override\",{\"1\":{\"3\":1}}],[\"overlaydisplayadapter平行的自定义适配器\",{\"1\":{\"3\":1}}],[\"oscar\",{\"1\":{\"33\":1,\"51\":1}}],[\"os\",{\"1\":{\"3\":1}}],[\"output\",{\"1\":{\"49\":1,\"77\":10}}],[\"out\",{\"1\":{\"3\":2,\"12\":9,\"105\":4}}],[\"并在使用\",{\"1\":{\"249\":1}}],[\"并在数据问题发生时向用户发送通知\",{\"1\":{\"72\":1}}],[\"并返回移除后数组的新长度\",{\"1\":{\"248\":1}}],[\"并返回给\",{\"1\":{\"244\":1}}],[\"并按照它们最初在\",{\"1\":{\"247\":1}}],[\"并把编码值作为\",{\"1\":{\"241\":1}}],[\"并对存储的信息进行操作\",{\"1\":{\"240\":1}}],[\"并对延迟的提示进行了评估\",{\"1\":{\"50\":1}}],[\"并将结果保存到\",{\"1\":{\"236\":1}}],[\"并将字符串对象的编码设置为int\",{\"1\":{\"205\":1}}],[\"并集计算\",{\"1\":{\"229\":1}}],[\"并集\",{\"1\":{\"223\":1}}],[\"并集和交集的计算复杂度较高\",{\"1\":{\"223\":1}}],[\"并集运算\",{\"1\":{\"222\":1}}],[\"并发\",{\"1\":{\"117\":1}}],[\"并发标记\",{\"1\":{\"116\":1,\"117\":1}}],[\"并保持它们不变\",{\"1\":{\"78\":1}}],[\"并行表示使能够有效地用gpu训练模型\",{\"1\":{\"77\":1}}],[\"并行表示使训练并行性能够充分利用gpu设备\",{\"1\":{\"75\":1}}],[\"并优化学习算法的超参数\",{\"1\":{\"73\":1}}],[\"并通过grpc\",{\"1\":{\"72\":1}}],[\"并提供硬件加速\",{\"1\":{\"68\":1}}],[\"并可视化数据集\",{\"1\":{\"63\":1}}],[\"并由kubernetes协调\",{\"1\":{\"58\":1}}],[\"并用我们的奖励选择最佳候选者\",{\"1\":{\"50\":1}}],[\"并用相似度与key所对应的value做矩阵运算并求和\",{\"1\":{\"17\":1}}],[\"并随着比较对变得更加相似而逐渐退化\",{\"1\":{\"49\":1}}],[\"并强制选择的响应比对应的响应具有更高的分数\",{\"1\":{\"49\":1}}],[\"并改变温度超参数\",{\"1\":{\"48\":1}}],[\"并从用户提示中消除令牌的损失\",{\"1\":{\"46\":1}}],[\"并使用户能够通过监控和可解释性分析进行检查\",{\"1\":{\"72\":1}}],[\"并使用字节分解未知的utf\",{\"1\":{\"43\":1}}],[\"并使用分组查询注意力\",{\"1\":{\"38\":1}}],[\"并使用自然语言提示来查询语言模型\",{\"1\":{\"31\":1}}],[\"并不会为每个消息生成\",{\"1\":{\"213\":1}}],[\"并不会出现\",{\"1\":{\"22\":1}}],[\"并不影响最终的结果\",{\"1\":{\"78\":1}}],[\"并不是dms\",{\"1\":{\"3\":1}}],[\"并且\",{\"1\":{\"248\":2}}],[\"并且原数组的前五个元素被修改为\",{\"1\":{\"249\":2}}],[\"并且原数组\",{\"1\":{\"247\":2}}],[\"并且给定的数组总是存在多数元素\",{\"1\":{\"245\":1}}],[\"并且是很小的\",{\"1\":{\"237\":1}}],[\"并且每个元素的值小于\",{\"1\":{\"228\":1}}],[\"并且每个小批量采取一个梯度步骤\",{\"1\":{\"50\":1}}],[\"并且将对象编码设置为embstr\",{\"1\":{\"205\":1}}],[\"并且这个值可以用long来表示\",{\"1\":{\"205\":1}}],[\"并且这可以在获得相同结果的同时\",{\"1\":{\"79\":1}}],[\"并且ack是j+1\",{\"1\":{\"185\":1}}],[\"并且会发送一个ack标志位为1\",{\"1\":{\"184\":1}}],[\"并且把应答号ack设为i+1\",{\"1\":{\"184\":1}}],[\"并且也需要占用一定的存储空间\",{\"1\":{\"165\":1}}],[\"并且赋予默认值\",{\"1\":{\"141\":1}}],[\"并且得到class对象\",{\"1\":{\"134\":1}}],[\"并且得到类的原型\",{\"1\":{\"134\":1}}],[\"并且不受堆大小影响\",{\"1\":{\"118\":1}}],[\"并且不需要用户查看模型结构\",{\"1\":{\"72\":1}}],[\"并且通过下列公式计算模型的输出\",{\"1\":{\"79\":1}}],[\"并且要在内存中缓存key\",{\"1\":{\"75\":1}}],[\"并且它支持用于保护api的自动扩展功能\",{\"1\":{\"69\":1}}],[\"并且总是从每个新rlhf版本的基本模型开始\",{\"1\":{\"50\":1}}],[\"并且采用了分组查询注意力\",{\"1\":{\"37\":1}}],[\"并且更推荐使用p\",{\"1\":{\"32\":1}}],[\"并且其资源消耗极低\",{\"1\":{\"30\":1}}],[\"并且做了以下修改\",{\"1\":{\"29\":1}}],[\"并且相对transformer来说\",{\"1\":{\"20\":1}}],[\"并且和transformer有关\",{\"1\":{\"19\":1}}],[\"并且经过证明\",{\"1\":{\"3\":1}}],[\"并正规化\",{\"1\":{\"18\":1}}],[\"element\",{\"1\":{\"238\":2}}],[\"else\",{\"1\":{\"3\":1,\"103\":1,\"253\":1}}],[\"evidentlyai\",{\"1\":{\"70\":1}}],[\"evidently有三个主要组成部分\",{\"1\":{\"70\":1}}],[\"evidently\",{\"0\":{\"70\":1}}],[\"eps=10−5\",{\"1\":{\"50\":1}}],[\"eps=10e\",{\"1\":{\"42\":1}}],[\"especially\",{\"1\":{\"49\":1}}],[\"eds\",{\"1\":{\"33\":1}}],[\"embstr编码的字符串实际上是只读的\",{\"1\":{\"206\":1}}],[\"embstr将创建字符串对象所需的内存分配次数从两次降低到了一次\",{\"1\":{\"206\":1}}],[\"embstr会通过一次内存分配函数来分配一块连续的内存空间来保存redisobject和sds\",{\"1\":{\"206\":1}}],[\"embstr和raw有什么区别\",{\"0\":{\"206\":1}}],[\"embstr是专门用于存储短字符串的一种优化编码\",{\"1\":{\"205\":1}}],[\"embstr\",{\"1\":{\"205\":1}}],[\"embedding层转为向量\",{\"1\":{\"79\":1}}],[\"embeddings\",{\"1\":{\"40\":1}}],[\"embedding的作用便是用来区分输入序列中的不同序列\",{\"1\":{\"21\":1}}],[\"embedding部分做了一些修改\",{\"1\":{\"21\":1}}],[\"embedding部分有了一些调整\",{\"1\":{\"20\":1}}],[\"embedding也有调整\",{\"1\":{\"20\":1,\"21\":1}}],[\"embedding\",{\"0\":{\"21\":1},\"1\":{\"18\":1,\"20\":1,\"21\":3}}],[\"emnlp\",{\"1\":{\"33\":1}}],[\"empirical\",{\"1\":{\"33\":2}}],[\"efficient\",{\"1\":{\"33\":1}}],[\"et\",{\"1\":{\"33\":1}}],[\"energy\",{\"1\":{\"50\":1}}],[\"encoder单元中的另一个部分是一个前馈网络\",{\"1\":{\"18\":1}}],[\"encoder结构图\",{\"1\":{\"18\":1}}],[\"encoder部分由n个encoder单元构成\",{\"1\":{\"18\":1}}],[\"encoder模块\",{\"1\":{\"18\":1}}],[\"encoder\",{\"1\":{\"16\":1,\"18\":1,\"19\":1}}],[\"end\",{\"1\":{\"3\":1,\"233\":2,\"235\":2}}],[\"error\",{\"1\":{\"3\":1}}],[\"e\",{\"1\":{\"3\":4,\"26\":3,\"33\":2}}],[\"extended\",{\"0\":{\"56\":1},\"1\":{\"56\":1}}],[\"extends\",{\"1\":{\"3\":2,\"12\":3}}],[\"expectednums\",{\"1\":{\"247\":3}}],[\"experiences\",{\"1\":{\"50\":1}}],[\"exploring\",{\"1\":{\"33\":1}}],[\"explorer选项打开android\",{\"1\":{\"1\":1}}],[\"execution\",{\"1\":{\"1\":1}}],[\"nil\",{\"1\":{\"242\":1}}],[\"niki\",{\"1\":{\"33\":1}}],[\"nx\",{\"1\":{\"198\":1}}],[\"nx来争夺锁\",{\"1\":{\"198\":1}}],[\"nums2\",{\"1\":{\"251\":8,\"253\":7}}],[\"nums1的实际数组长度为0时\",{\"1\":{\"252\":1}}],[\"nums1\",{\"1\":{\"251\":14,\"253\":10}}],[\"nums中的前两个元素均为\",{\"1\":{\"248\":1}}],[\"nums\",{\"1\":{\"245\":5,\"247\":20,\"248\":13,\"249\":11}}],[\"numberkeys\",{\"1\":{\"229\":2}}],[\"numberkeys一共多少个key\",{\"1\":{\"229\":2}}],[\"numberofhardlinks\",{\"1\":{\"103\":1}}],[\"num\",{\"1\":{\"77\":12}}],[\"null\",{\"1\":{\"3\":4,\"103\":3}}],[\"n−mkmt​vm​\",{\"1\":{\"77\":1}}],[\"n−mλ−1\",{\"1\":{\"77\":1}}],[\"n个样本\",{\"1\":{\"50\":1}}],[\"native\",{\"1\":{\"253\":1}}],[\"naturally\",{\"1\":{\"49\":1}}],[\"natural\",{\"1\":{\"33\":2}}],[\"nan\",{\"1\":{\"33\":1}}],[\"naman\",{\"1\":{\"33\":1}}],[\"name\",{\"1\":{\"3\":4}}],[\"narasimhan\",{\"1\":{\"33\":1}}],[\"narang\",{\"1\":{\"33\":1}}],[\"naacl\",{\"1\":{\"33\":1}}],[\"nlg\",{\"1\":{\"32\":1}}],[\"nlu\",{\"1\":{\"32\":1}}],[\"nlp\",{\"1\":{\"24\":1,\"33\":1}}],[\"nlp经历了什么\",{\"0\":{\"13\":1}}],[\"nsp的位置在bert模型图中有所体现\",{\"1\":{\"22\":1}}],[\"nsp是一个二分类下句预测任务\",{\"1\":{\"22\":1}}],[\"n\",{\"1\":{\"20\":2,\"33\":1,\"75\":1,\"77\":7,\"151\":1,\"204\":1,\"218\":1,\"245\":5,\"246\":1,\"251\":12,\"253\":9}}],[\"noah\",{\"1\":{\"33\":1}}],[\"noam\",{\"1\":{\"33\":2}}],[\"norm\",{\"1\":{\"18\":1,\"77\":3}}],[\"not运算只能一个key\",{\"1\":{\"233\":1}}],[\"not\",{\"1\":{\"18\":2,\"233\":1,\"236\":1}}],[\"n=6\",{\"1\":{\"18\":1}}],[\"networks\",{\"0\":{\"76\":1}}],[\"network\",{\"0\":{\"258\":1},\"1\":{\"74\":1,\"76\":1}}],[\"neptune\",{\"1\":{\"72\":2}}],[\"neural\",{\"1\":{\"33\":1}}],[\"next\",{\"1\":{\"22\":1}}],[\"need\",{\"1\":{\"3\":1,\"33\":1}}],[\"newinstance\",{\"1\":{\"3\":1}}],[\"new\",{\"1\":{\"3\":3,\"12\":5,\"103\":1,\"105\":4}}],[\"=4\",{\"1\":{\"174\":1}}],[\"==\",{\"1\":{\"103\":1,\"245\":1,\"247\":2,\"251\":2,\"253\":1}}],[\"=gelu\",{\"1\":{\"79\":1}}],[\"=groupnorm\",{\"1\":{\"78\":1}}],[\"=r~v\",{\"1\":{\"78\":1}}],[\"=m=1∑n​\",{\"1\":{\"77\":1}}],[\"=xn​⋅wv​上\",{\"1\":{\"77\":1}}],[\"=\",{\"1\":{\"3\":18,\"12\":5,\"20\":8,\"28\":1,\"38\":2,\"46\":2,\"77\":14,\"103\":6,\"105\":2,\"161\":1,\"175\":1,\"235\":1,\"236\":3,\"245\":2,\"247\":8,\"248\":10,\"249\":8,\"251\":13,\"253\":6}}],[\"tcp的挥手既可以是客户端发起的\",{\"1\":{\"187\":1}}],[\"tcp的首部包中包括\",{\"1\":{\"183\":1}}],[\"tcp的首部包中包括什么\",{\"0\":{\"183\":1}}],[\"tcp三次握手过程\",{\"0\":{\"184\":1}}],[\"tcp三次握手和四次挥手\",{\"0\":{\"180\":1}}],[\"tcp是哪一层的协议\",{\"0\":{\"181\":1}}],[\"tcp\",{\"1\":{\"83\":1}}],[\"tvm​\",{\"1\":{\"77\":1}}],[\"tfx\",{\"1\":{\"56\":4}}],[\"turbo\",{\"1\":{\"54\":1}}],[\"tuning与p\",{\"1\":{\"32\":1}}],[\"tuning中\",{\"1\":{\"32\":1}}],[\"tuning的\",{\"1\":{\"32\":1}}],[\"tuning将预训练lm参数固定\",{\"1\":{\"32\":1}}],[\"tuning示意图\",{\"1\":{\"32\":1}}],[\"tuning最开始应用在自然语言生成\",{\"1\":{\"32\":1}}],[\"tuning技术首次拓展到序列标注等复杂自然语言理解\",{\"1\":{\"32\":1}}],[\"tuning技术适配到自然语言理解任务中\",{\"1\":{\"32\":1}}],[\"tuning相似的性能\",{\"1\":{\"32\":1}}],[\"tuning进行了修改和优化\",{\"1\":{\"30\":1}}],[\"tuning和p\",{\"1\":{\"30\":1}}],[\"tuning虽然实现了部分参数调优\",{\"1\":{\"30\":1}}],[\"tuning\",{\"0\":{\"30\":1,\"32\":1,\"45\":1,\"46\":1},\"1\":{\"13\":2,\"14\":5,\"30\":8,\"31\":3,\"32\":15,\"33\":4,\"50\":1}}],[\"two\",{\"1\":{\"49\":1}}],[\"täckström\",{\"1\":{\"33\":1}}],[\"table\",{\"1\":{\"49\":2}}],[\"tau\",{\"1\":{\"33\":1}}],[\"tasks\",{\"1\":{\"33\":3}}],[\"tang\",{\"1\":{\"33\":2}}],[\"tag\",{\"1\":{\"3\":2}}],[\"timeout\",{\"1\":{\"210\":2}}],[\"time\",{\"1\":{\"70\":1}}],[\"tim\",{\"1\":{\"33\":1}}],[\"telnet\",{\"1\":{\"82\":1}}],[\"tensorflow和huggingface\",{\"1\":{\"62\":1}}],[\"tensorflow\",{\"0\":{\"56\":1},\"1\":{\"56\":3,\"69\":1,\"72\":1}}],[\"text的形式\",{\"1\":{\"24\":1}}],[\"text\",{\"1\":{\"24\":5,\"33\":2,\"50\":1}}],[\"test\",{\"1\":{\"12\":1}}],[\"trillion\",{\"1\":{\"38\":1}}],[\"transpose\",{\"1\":{\"77\":3}}],[\"transfer\",{\"1\":{\"33\":1}}],[\"transfer来自transfer\",{\"1\":{\"24\":1}}],[\"transformer已然成为llm的首选架构\",{\"1\":{\"75\":1}}],[\"transformer架构\",{\"1\":{\"40\":1}}],[\"transformer即我们在第二节中提到的\",{\"1\":{\"24\":1}}],[\"transformers\",{\"1\":{\"19\":1,\"33\":1}}],[\"transformer是一个seq2seq模型\",{\"1\":{\"19\":1}}],[\"transformer是seq2seq的全新尝试\",{\"1\":{\"18\":1}}],[\"transformer对后来语言模型的影响十分深远\",{\"1\":{\"18\":1}}],[\"transformer在这里设计的前馈网络比较简单\",{\"1\":{\"18\":1}}],[\"transformer认为\",{\"1\":{\"18\":1}}],[\"transformer中的位置编码\",{\"1\":{\"21\":1}}],[\"transformer中的位置编码采用公式计算得到\",{\"1\":{\"18\":1}}],[\"transformer中多头注意力机制是由h\",{\"1\":{\"20\":1}}],[\"transformer中对注意力机制的体现在多头注意力机制\",{\"1\":{\"18\":1}}],[\"transformer的并行训练是有代价的\",{\"1\":{\"75\":1}}],[\"transformer的decoder结构中\",{\"1\":{\"25\":1}}],[\"transformer的decoder和encoder十分相似\",{\"1\":{\"18\":1}}],[\"transformer的简写\",{\"1\":{\"24\":1}}],[\"transformer的encoder层是由n=6的单元构成的\",{\"1\":{\"20\":1}}],[\"transformer的encoder模块\",{\"1\":{\"20\":1}}],[\"transformer的模型结构已经介绍完成\",{\"1\":{\"18\":1}}],[\"transformer的编码器结构如图2\",{\"1\":{\"18\":1}}],[\"transformer的位置编码简单但是有创新性\",{\"1\":{\"18\":1}}],[\"transformer没有办法表示序列的顺序\",{\"1\":{\"18\":1}}],[\"transformer模型整体结构如图2\",{\"1\":{\"18\":1}}],[\"transformer模型\",{\"0\":{\"18\":1},\"1\":{\"18\":1}}],[\"transformer\",{\"0\":{\"15\":1},\"1\":{\"14\":1,\"33\":1}}],[\"training\",{\"1\":{\"25\":1,\"33\":3,\"41\":1}}],[\"try\",{\"1\":{\"3\":1}}],[\"t5一样\",{\"1\":{\"29\":1}}],[\"t5模型相同\",{\"1\":{\"25\":1}}],[\"t5模型\",{\"0\":{\"24\":1},\"1\":{\"24\":1}}],[\"t5和gpt\",{\"0\":{\"23\":1}}],[\"t5论文中有对比实验\",{\"1\":{\"22\":1}}],[\"t5基础模型参数量为220m\",{\"1\":{\"14\":1}}],[\"t5\",{\"1\":{\"13\":2,\"23\":1,\"24\":1}}],[\"those\",{\"1\":{\"49\":1}}],[\"that\",{\"1\":{\"49\":1}}],[\"thudm\",{\"1\":{\"35\":1}}],[\"the\",{\"1\":{\"18\":6,\"33\":5,\"49\":5}}],[\"throws\",{\"1\":{\"3\":1,\"105\":1}}],[\"this$0\",{\"1\":{\"3\":2}}],[\"this$0表示顶层外部类引用\",{\"1\":{\"3\":1}}],[\"this\",{\"1\":{\"3\":2,\"12\":2,\"49\":1,\"103\":7}}],[\"tom\",{\"1\":{\"226\":3}}],[\"torch\",{\"1\":{\"77\":1}}],[\"torchserve和其他ml库\",{\"1\":{\"69\":1}}],[\"top\",{\"1\":{\"61\":1}}],[\"tools和https\",{\"1\":{\"72\":1}}],[\"tools\",{\"1\":{\"71\":1,\"72\":2}}],[\"tools中提到了一些其他的mlops的工具\",{\"1\":{\"61\":1}}],[\"too\",{\"1\":{\"33\":1}}],[\"toutanova\",{\"1\":{\"33\":1}}],[\"token上进行训练\",{\"1\":{\"39\":1}}],[\"token\",{\"1\":{\"27\":1,\"37\":1}}],[\"to\",{\"1\":{\"1\":1,\"3\":1,\"16\":1,\"24\":4,\"33\":2}}],[\"ii\",{\"0\":{\"249\":1}}],[\"iinfrarescanmanager\",{\"1\":{\"3\":2}}],[\"i++\",{\"1\":{\"247\":1,\"248\":1,\"249\":1}}],[\"id\",{\"1\":{\"213\":7,\"235\":1,\"244\":2}}],[\"idisplaymanager\",{\"1\":{\"3\":1}}],[\"ip\",{\"1\":{\"84\":1}}],[\"i=1∣x∣​通过word\",{\"1\":{\"79\":1}}],[\"i=1x​首先被转换为x0=\",{\"1\":{\"76\":1}}],[\"io\",{\"1\":{\"64\":1,\"71\":1,\"72\":1}}],[\"icml\",{\"1\":{\"33\":1}}],[\"import\",{\"1\":{\"105\":2}}],[\"improve\",{\"1\":{\"49\":1}}],[\"improving\",{\"1\":{\"33\":1}}],[\"imap更高级\",{\"1\":{\"82\":1}}],[\"imap\",{\"1\":{\"82\":1}}],[\"imifreeformdisplaycallback\",{\"1\":{\"3\":1}}],[\"imifreeformservice\",{\"1\":{\"3\":1}}],[\"ilya\",{\"1\":{\"33\":1}}],[\"illia\",{\"1\":{\"33\":1}}],[\"i\",{\"1\":{\"18\":4,\"245\":1,\"247\":5,\"248\":4,\"249\":4,\"251\":1,\"253\":5}}],[\"issues\",{\"1\":{\"35\":1}}],[\"is\",{\"1\":{\"7\":1,\"33\":1,\"49\":4}}],[\"if\",{\"1\":{\"3\":1,\"103\":3,\"253\":2}}],[\"ibinder>\",{\"1\":{\"3\":1}}],[\"ibinder\",{\"1\":{\"3\":5}}],[\"increment\",{\"1\":{\"229\":1}}],[\"in中数据大于30\",{\"1\":{\"177\":1}}],[\"in和not\",{\"0\":{\"177\":1}}],[\"inner\",{\"1\":{\"77\":4}}],[\"inst可以是\",{\"1\":{\"51\":1}}],[\"inst\",{\"1\":{\"51\":1}}],[\"instruction\",{\"1\":{\"46\":1}}],[\"information\",{\"1\":{\"33\":1}}],[\"infilling\",{\"1\":{\"27\":1}}],[\"infrarescanmanager\",{\"1\":{\"3\":4}}],[\"infrare\",{\"1\":{\"3\":3}}],[\"input\",{\"0\":{\"21\":1},\"1\":{\"21\":1,\"22\":1}}],[\"intrinsiccandidate\",{\"1\":{\"253\":2}}],[\"integer\",{\"1\":{\"213\":1,\"226\":1}}],[\"interruptedexception\",{\"1\":{\"105\":1}}],[\"int\",{\"1\":{\"3\":6,\"247\":4,\"248\":2,\"249\":2,\"253\":9}}],[\"invoke\",{\"1\":{\"3\":2}}],[\"initservicecache\",{\"1\":{\"3\":1}}],[\"in\",{\"0\":{\"177\":1},\"1\":{\"1\":1,\"3\":2,\"33\":9,\"49\":3}}],[\"rpop\",{\"1\":{\"210\":1}}],[\"rpush\",{\"1\":{\"210\":1}}],[\"rdb\",{\"1\":{\"196\":2}}],[\"rtp\",{\"1\":{\"82\":1}}],[\"r=qkt⨀d\",{\"1\":{\"78\":1}}],[\"rl\",{\"1\":{\"50\":1}}],[\"rlhf结果\",{\"0\":{\"52\":1}}],[\"rlhf对重新缩放温度有直接影响\",{\"1\":{\"50\":1}}],[\"rlhf\",{\"0\":{\"47\":1},\"1\":{\"50\":2,\"53\":1}}],[\"r\",{\"1\":{\"49\":1,\"59\":1,\"66\":1}}],[\"rθ\",{\"1\":{\"49\":1}}],[\"rmsnorm\",{\"1\":{\"40\":1}}],[\"rfou\",{\"1\":{\"33\":1}}],[\"roots相连接的对象\",{\"1\":{\"116\":1}}],[\"roots引用到\",{\"1\":{\"96\":1}}],[\"roots\",{\"0\":{\"97\":1},\"1\":{\"96\":1}}],[\"rotary\",{\"1\":{\"40\":1}}],[\"rope\",{\"1\":{\"40\":1}}],[\"robustly\",{\"1\":{\"33\":1}}],[\"roberta\",{\"1\":{\"33\":1}}],[\"roberts\",{\"1\":{\"33\":1}}],[\"rorerta\",{\"1\":{\"26\":1}}],[\"radius\",{\"1\":{\"242\":1}}],[\"radford\",{\"1\":{\"33\":1}}],[\"raw和embstr对应数据结构sds\",{\"1\":{\"205\":1}}],[\"raw\",{\"1\":{\"205\":1}}],[\"rawgetservice\",{\"1\":{\"3\":1}}],[\"rating\",{\"1\":{\"49\":1}}],[\"rank\",{\"1\":{\"33\":1}}],[\"rami\",{\"1\":{\"33\":1}}],[\"raffel\",{\"1\":{\"33\":1}}],[\"rnn\",{\"1\":{\"16\":1}}],[\"rule中编写自定义的selinux规则\",{\"1\":{\"3\":1}}],[\"redis就把字节数组的每个bit位利用起来\",{\"1\":{\"232\":1}}],[\"redis会使用压缩列表\",{\"1\":{\"217\":1}}],[\"redis会将embstr编码转换成raw编码\",{\"1\":{\"206\":1}}],[\"redis可以实现分布式锁\",{\"1\":{\"207\":1}}],[\"redis没有为embstr编码的字符串对象提供相应的修改程序\",{\"1\":{\"206\":1}}],[\"redisobject和sds都需要重新分配\",{\"1\":{\"206\":1}}],[\"redis中有5中基本类型和4种常见类型\",{\"1\":{\"199\":1}}],[\"redis中一个string类型的最大容量是多少\",{\"0\":{\"195\":1}}],[\"redis数据类型\",{\"0\":{\"199\":1}}],[\"redis\",{\"0\":{\"260\":1},\"1\":{\"196\":1,\"223\":1,\"228\":3,\"235\":1,\"236\":2,\"237\":3,\"240\":2,\"244\":1}}],[\"redis使用队列技术把并发访问变成了串行访问\",{\"1\":{\"194\":1}}],[\"redis是单进程单线程的\",{\"1\":{\"194\":1}}],[\"redis是单进程还是多进程\",{\"0\":{\"194\":1}}],[\"redis是基于c语言开发的\",{\"1\":{\"190\":1}}],[\"redis有5种数据类型\",{\"1\":{\"193\":1}}],[\"redis有更加丰富的数据类型\",{\"1\":{\"192\":1}}],[\"redis速度比memcached更快\",{\"1\":{\"192\":1}}],[\"redis比memcached有优势\",{\"1\":{\"192\":1}}],[\"redis支持持久化存储\",{\"1\":{\"192\":1}}],[\"redis支持数据持久化\",{\"1\":{\"191\":1}}],[\"redis支持的数据类型很多\",{\"1\":{\"191\":1}}],[\"redis所有操作都是原子性的\",{\"1\":{\"191\":1}}],[\"redis的stream类型可以解决这个问题\",{\"1\":{\"215\":1}}],[\"redis的持久化机制有哪些\",{\"0\":{\"196\":1}}],[\"redis的数据类型\",{\"0\":{\"193\":1}}],[\"redis的性能极高\",{\"1\":{\"191\":1}}],[\"redis的特点\",{\"0\":{\"191\":1}}],[\"redis的优势\",{\"0\":{\"191\":1}}],[\"redis总结\",{\"0\":{\"189\":1}}],[\"removeelement\",{\"1\":{\"248\":1}}],[\"removeduplicates\",{\"1\":{\"247\":1,\"249\":1}}],[\"removelast\",{\"1\":{\"103\":1}}],[\"removeobject\",{\"1\":{\"103\":1}}],[\"remoteexception\",{\"1\":{\"3\":1}}],[\"recv状态\",{\"1\":{\"184\":1}}],[\"recurrentretention\",{\"1\":{\"77\":1}}],[\"recurrent\",{\"1\":{\"74\":1}}],[\"reconnect\",{\"1\":{\"1\":1}}],[\"retentive\",{\"0\":{\"76\":1}}],[\"retention分数归一化\",{\"1\":{\"78\":1}}],[\"retention的输出变为retention\",{\"1\":{\"78\":1}}],[\"retention的块循环表示\",{\"1\":{\"77\":1}}],[\"retention的循环表示\",{\"1\":{\"77\":1}}],[\"retention的平行表示\",{\"1\":{\"77\":1}}],[\"retention\",{\"0\":{\"77\":1,\"78\":1},\"1\":{\"75\":1,\"76\":1,\"77\":15,\"78\":1,\"79\":1}}],[\"retnet通过自回归的方式编码这个序列\",{\"1\":{\"76\":1}}],[\"retnet由l个相同的block组成\",{\"1\":{\"76\":1}}],[\"retnet还比使用了flashattention的transformer节省了25\",{\"1\":{\"75\":1}}],[\"retnet的推理延迟对batch\",{\"1\":{\"75\":1}}],[\"retnet的推理耗费是不受序列长度影响的\",{\"1\":{\"75\":1}}],[\"retnet的解码速度比具有键值缓存的transformer快8\",{\"1\":{\"75\":1}}],[\"retnet的理论来源是连接循环和注意力\",{\"1\":{\"74\":1}}],[\"retnet在尺度曲线和上下文学习方面都具有竞争力\",{\"1\":{\"75\":1}}],[\"retnet在llm上要优于transformer\",{\"1\":{\"74\":1}}],[\"retnet则可以同时实现低成本推理\",{\"1\":{\"75\":1}}],[\"retnet\",{\"0\":{\"74\":1},\"1\":{\"74\":2}}],[\"return\",{\"1\":{\"3\":4,\"12\":4,\"77\":3,\"103\":1,\"253\":1}}],[\"real\",{\"1\":{\"70\":1}}],[\"reach\",{\"1\":{\"1\":1}}],[\"rejection\",{\"1\":{\"50\":1}}],[\"rejected\",{\"1\":{\"49\":1}}],[\"reward\",{\"1\":{\"49\":1}}],[\"result\",{\"1\":{\"103\":5,\"233\":2}}],[\"rest或kafka流将它们链接起来\",{\"1\":{\"72\":1}}],[\"residual\",{\"1\":{\"50\":1,\"76\":1}}],[\"resizefreeform\",{\"1\":{\"3\":1}}],[\"responses\",{\"1\":{\"49\":3}}],[\"response\",{\"1\":{\"49\":1}}],[\"res\",{\"1\":{\"33\":1}}],[\"representations\",{\"1\":{\"19\":1}}],[\"refreshrate\",{\"1\":{\"3\":1}}],[\"registerservice\",{\"1\":{\"3\":1}}],[\"我们现在想要将nums1的内容复制到nums2中\",{\"1\":{\"253\":1}}],[\"我们稍后介绍\",{\"1\":{\"253\":1}}],[\"我们就可以把经纬度保存到\",{\"1\":{\"241\":1}}],[\"我们就知道该用户在\",{\"1\":{\"235\":1}}],[\"我们越来越依赖搜索\",{\"1\":{\"240\":1}}],[\"我们再通过\",{\"1\":{\"236\":1}}],[\"我们执行以下命令\",{\"1\":{\"213\":1}}],[\"我们应该尽可能地使用联合索引而不是单列索引\",{\"1\":{\"167\":1}}],[\"我们通过class字节码中的信息\",{\"1\":{\"134\":1}}],[\"我们通过以下方法计算第i个块的retention输出\",{\"1\":{\"77\":1}}],[\"我们这里说的类的加载流程\",{\"1\":{\"133\":1}}],[\"我们在用\",{\"1\":{\"213\":1}}],[\"我们在使用hashmap的时候\",{\"1\":{\"105\":1}}],[\"我们在不同的层之间设置了相同的γ\",{\"1\":{\"78\":1}}],[\"我们直接new出来的引用都是强引用\",{\"1\":{\"102\":1}}],[\"我们定义layer为\",{\"1\":{\"78\":1}}],[\"我们遵循并行表示\",{\"1\":{\"77\":1}}],[\"我们将输入序列划分成块\",{\"1\":{\"77\":1}}],[\"我们将在下面介绍这一部分\",{\"1\":{\"20\":1}}],[\"我们递归地得到的输出为\",{\"1\":{\"77\":1}}],[\"我们进一步地把γ简化为一个标量\",{\"1\":{\"77\":1}}],[\"我们进一步讨论transformer中应用的注意力机制\",{\"1\":{\"18\":1}}],[\"我们把每天的日期作为\",{\"1\":{\"236\":1}}],[\"我们把矩阵a对角化\",{\"1\":{\"77\":1}}],[\"我们把它投影到一维函数v\",{\"1\":{\"77\":1}}],[\"我们使用投影qn​\",{\"1\":{\"77\":1}}],[\"我们使用512的批量大小\",{\"1\":{\"50\":1}}],[\"我们规定vn​=v\",{\"1\":{\"77\":1}}],[\"我们的目标不是重新创建其他服务\",{\"1\":{\"58\":1}}],[\"我们设置β=0\",{\"1\":{\"50\":1}}],[\"我们选择p\",{\"1\":{\"30\":1}}],[\"我们介绍微调相关技术\",{\"1\":{\"29\":1}}],[\"我们称之为masked\",{\"1\":{\"27\":1}}],[\"我们给定一个文本序列\",{\"1\":{\"27\":1}}],[\"我们来介绍glm中的自回归填空思想\",{\"1\":{\"26\":1}}],[\"我们来讨论多头注意力机制\",{\"1\":{\"18\":1}}],[\"我们无法评价自回归\",{\"1\":{\"19\":1}}],[\"我们知道\",{\"1\":{\"19\":1,\"26\":1}}],[\"我们希望自注意力机制是\",{\"1\":{\"18\":1}}],[\"我们对注意力机制有了初步了解\",{\"1\":{\"18\":1}}],[\"我们往往会讨论q\",{\"1\":{\"17\":1}}],[\"我们首先讨论一下注意力机制\",{\"1\":{\"16\":1}}],[\"我们很难处理\",{\"1\":{\"3\":1}}],[\"我们需要使用binderservice的实例的classloader来加载dms类\",{\"1\":{\"3\":1}}],[\"我们需要给米窗3所需要执行的内容编写selinux规则\",{\"1\":{\"3\":1}}],[\"我们可以调用system\",{\"1\":{\"252\":1}}],[\"我们可以针对特殊情况\",{\"1\":{\"252\":1}}],[\"我们可以用一个\",{\"1\":{\"244\":1}}],[\"我们可以选择一个从库完成聚合统计\",{\"1\":{\"223\":1}}],[\"我们可以考虑第三次挥手之后\",{\"1\":{\"188\":1}}],[\"我们可以把索引联合起来\",{\"1\":{\"167\":1}}],[\"我们可以把索引放到key中\",{\"1\":{\"150\":1}}],[\"我们可以重写\",{\"1\":{\"77\":1}}],[\"我们可以看到\",{\"1\":{\"49\":1,\"51\":1}}],[\"我们可以看图5\",{\"1\":{\"27\":1}}],[\"我们可以使用volatile关键字或者synchronized代码段来保证可见性\",{\"1\":{\"128\":1}}],[\"我们可以使用\",{\"1\":{\"31\":1}}],[\"我们可以将样本与提示\",{\"1\":{\"31\":1}}],[\"我们可以抽象出他们共有的行为或者属相并将其定义成一个父类或者超类\",{\"1\":{\"7\":1}}],[\"我们可以通过这些信息来调用类的内容\",{\"1\":{\"134\":1}}],[\"我们可以通过反射的方式获取到外部类的实例\",{\"1\":{\"3\":1}}],[\"我们可以通过servicemanager\",{\"1\":{\"3\":1}}],[\"我们可以直接通过servicemanager获取到binderservice\",{\"1\":{\"3\":1}}],[\"为有序集合key中元素member的分值加上increment\",{\"1\":{\"229\":1}}],[\"为哈希表key中field键的值加上增量n\",{\"1\":{\"218\":1}}],[\"为\",{\"1\":{\"213\":1}}],[\"为您提供代码\",{\"1\":{\"67\":1}}],[\"为端到端ml管道的每个任务提供库\",{\"1\":{\"56\":1}}],[\"为效果最好的\",{\"1\":{\"27\":1}}],[\"为bert模型的结构图\",{\"1\":{\"20\":1}}],[\"为一个两层的多层感知机\",{\"1\":{\"18\":1}}],[\"为了实现快速复制\",{\"1\":{\"252\":1}}],[\"为了应对这种情况\",{\"1\":{\"251\":1}}],[\"为了避免主库因为\",{\"1\":{\"223\":1}}],[\"为了减少处理时间耗费\",{\"1\":{\"252\":1}}],[\"为了减少回表时间\",{\"1\":{\"178\":1}}],[\"为了减少微调的设备等资源的消耗\",{\"1\":{\"30\":1}}],[\"为了优化性能\",{\"1\":{\"129\":1}}],[\"为了解决上面二叉查找树会退化为一条链的情况\",{\"1\":{\"152\":1}}],[\"为了解决内存碎片问题\",{\"1\":{\"109\":1}}],[\"为了解决这一问题\",{\"1\":{\"129\":1}}],[\"为了解决这个可能影响训练的问题\",{\"1\":{\"51\":1}}],[\"为了解决这个问题\",{\"1\":{\"22\":2}}],[\"为了解决这些限制\",{\"1\":{\"51\":1}}],[\"为了简单起见\",{\"1\":{\"78\":1}}],[\"为了简化\",{\"1\":{\"77\":1}}],[\"为了评估主要模型版本的质量\",{\"1\":{\"54\":1}}],[\"为了进行公平的比较\",{\"1\":{\"53\":1}}],[\"为了说明gatt如何在微调过程中帮助重塑注意力\",{\"1\":{\"51\":1}}],[\"为了使指令更加复杂和多样化\",{\"1\":{\"51\":1}}],[\"为了获得兴趣爱好和公众人物的列表\",{\"1\":{\"51\":1}}],[\"为了快速进行大批量训练\",{\"1\":{\"50\":1}}],[\"为了训练奖励模型\",{\"1\":{\"49\":1}}],[\"为了最大限度地提高多样性\",{\"1\":{\"48\":1}}],[\"为了确保模型序列长度正确填充\",{\"1\":{\"46\":1}}],[\"为了适应下游任务\",{\"1\":{\"29\":1}}],[\"为了抵消这个效果\",{\"1\":{\"18\":1}}],[\"为了让mda与dms可通过同一classloader加载\",{\"1\":{\"3\":1}}],[\"为什么返回数值是整数\",{\"1\":{\"248\":1,\"249\":1}}],[\"为什么需要处理重复消息\",{\"1\":{\"213\":1}}],[\"为什么要用redis\",{\"0\":{\"191\":1}}],[\"为什么要进行四次挥手\",{\"0\":{\"188\":1}}],[\"为什么要进行三次握手\",{\"0\":{\"185\":1}}],[\"为什么通过反射获取displaymanagerservice时会抛出noclassdeffounderror异常\",{\"1\":{\"3\":1}}],[\"为什么用户程序仍然无法发现\",{\"1\":{\"3\":1}}],[\"为什么不能顺利添加自定义系统服务\",{\"1\":{\"3\":1}}],[\"为此米窗3需要设法获取到dms实例\",{\"1\":{\"3\":1}}],[\"为此\",{\"1\":{\"3\":4,\"49\":1}}],[\"该算法题的实现如下\",{\"1\":{\"253\":1}}],[\"该算法是rlhf文献的标准\",{\"1\":{\"50\":1}}],[\"该题来自力扣第88题\",{\"1\":{\"250\":1}}],[\"该题来自力扣27题\",{\"1\":{\"248\":1}}],[\"该题来自力扣26题\",{\"1\":{\"247\":1}}],[\"该长度范围内\",{\"1\":{\"248\":1,\"249\":1}}],[\"该公式在训练实例中很容易被并行化\",{\"1\":{\"77\":1}}],[\"该实现比较简单\",{\"1\":{\"75\":1}}],[\"该平台提供了一个配置框架和众多共享库\",{\"1\":{\"56\":1}}],[\"该数据集具有消息列表\",{\"1\":{\"51\":1}}],[\"该模型为给定提示探索k个样本\",{\"1\":{\"50\":1}}],[\"该模型号称是\",{\"1\":{\"14\":1}}],[\"该论文篇幅巨大\",{\"1\":{\"36\":1}}],[\"该issue讨论了glm在apple芯片上微调的内容\",{\"1\":{\"35\":1}}],[\"该编码没有整合进模型\",{\"1\":{\"18\":1}}],[\"该编码不是一个单一的数值\",{\"1\":{\"18\":1}}],[\"该脉络基本涉及了现代语言模型学习的全过程\",{\"1\":{\"14\":1}}],[\"该方法定义如下\",{\"1\":{\"253\":1}}],[\"该方法位于java\",{\"1\":{\"253\":1}}],[\"该方法微调效果可以与全参数微调媲美\",{\"1\":{\"14\":1}}],[\"该方式实现最佳\",{\"1\":{\"3\":1}}],[\"该引用是不能使用的\",{\"1\":{\"9\":1}}],[\"该引用变量发出的方法调用到底是哪个类中实现的方法\",{\"1\":{\"9\":1}}],[\"该类是displaymanagerservice的内部类\",{\"1\":{\"3\":1}}],[\"该类继承自imifreeformservice\",{\"1\":{\"3\":1}}],[\"该库额外提供了代理systemservice的功能\",{\"1\":{\"3\":1}}],[\"该服务是一个binder\",{\"1\":{\"3\":1}}],[\"在函数里修改输入数组对于调用者是可见的\",{\"1\":{\"248\":1,\"249\":1}}],[\"在打车软件上叫车\",{\"1\":{\"240\":1}}],[\"在日常生活中\",{\"1\":{\"240\":1}}],[\"在输入元素的数量或者体积非常非常大时\",{\"1\":{\"237\":1}}],[\"在默认情况下\",{\"1\":{\"235\":1}}],[\"在\",{\"1\":{\"228\":1,\"235\":1,\"236\":1,\"237\":1}}],[\"在主从集群中\",{\"1\":{\"223\":1}}],[\"在数据量较大的情况下\",{\"1\":{\"223\":1}}],[\"在一个哈希表key中存储多个键值对\",{\"1\":{\"218\":1}}],[\"在v3\",{\"1\":{\"209\":1}}],[\"在单服务器中\",{\"1\":{\"207\":1}}],[\"在没有进行rewrite之前\",{\"1\":{\"196\":1}}],[\"在innodb中\",{\"1\":{\"155\":1}}],[\"在准备阶段只会进行分配内存空间\",{\"1\":{\"141\":1}}],[\"在垃圾回收的时候必须暂停工作线程\",{\"1\":{\"113\":1}}],[\"在回收之前会将这个虚引用放到队列中\",{\"1\":{\"106\":1}}],[\"在测试引用的时候\",{\"1\":{\"103\":1}}],[\"在队列中只要这个对象和任何一个引用链搭上关系\",{\"1\":{\"98\":1}}],[\"在经历过一次垃圾回收并且活下来\",{\"1\":{\"91\":1}}],[\"在训练过程中\",{\"1\":{\"75\":1,\"79\":1}}],[\"在https\",{\"1\":{\"61\":1,\"71\":1}}],[\"在我们的提示集上\",{\"1\":{\"54\":1}}],[\"在对话的大部分时间里保持了对系统消息的大量注意力激活\",{\"1\":{\"51\":1}}],[\"在对话设置中\",{\"1\":{\"51\":1}}],[\"在为训练数据构建最终系统消息时\",{\"1\":{\"51\":1}}],[\"在初始版本的rlhf模型中\",{\"1\":{\"51\":1}}],[\"在有限的计算预算下\",{\"1\":{\"50\":1}}],[\"在迭代模型更新过程中\",{\"1\":{\"50\":1}}],[\"在不同温度下的最大回报曲线\",{\"1\":{\"50\":1}}],[\"在图8中\",{\"1\":{\"50\":1}}],[\"在图5\",{\"1\":{\"27\":2}}],[\"在每个块中\",{\"1\":{\"77\":1}}],[\"在每个迭代过程\",{\"1\":{\"50\":1}}],[\"在每一batch用于奖励建模的人类偏好注释上\",{\"1\":{\"49\":1}}],[\"在应用类似于sft的微调之前\",{\"1\":{\"50\":1}}],[\"在拒绝采样微调中\",{\"1\":{\"50\":1}}],[\"在拒绝采样中\",{\"1\":{\"50\":1}}],[\"在步骤t的训练期间\",{\"1\":{\"50\":1}}],[\"在更明显的反应上\",{\"1\":{\"49\":1}}],[\"在早期的实验中\",{\"1\":{\"49\":1}}],[\"在这组评估中\",{\"1\":{\"53\":1}}],[\"在这种二元排名损失的基础上\",{\"1\":{\"49\":1}}],[\"在这里你可以\",{\"1\":{\"59\":1}}],[\"在这里也可以看出\",{\"1\":{\"27\":1}}],[\"在这里\",{\"1\":{\"18\":1,\"50\":1}}],[\"在第3节\",{\"1\":{\"37\":1}}],[\"在第2节\",{\"1\":{\"37\":1}}],[\"在apple芯片上微调glm模型\",{\"0\":{\"34\":1}}],[\"在android中\",{\"1\":{\"3\":1}}],[\"在pachyderm中\",{\"1\":{\"66\":1}}],[\"在ppo中\",{\"1\":{\"50\":1}}],[\"在p\",{\"1\":{\"32\":1}}],[\"在prefix部分\",{\"1\":{\"32\":1}}],[\"在330m到10b参数规模的语言模型上\",{\"1\":{\"32\":1}}],[\"在文本序列中采样的文本域为\",{\"1\":{\"27\":1}}],[\"在其中采用多个文本域\",{\"1\":{\"27\":1}}],[\"在现在看\",{\"1\":{\"26\":1}}],[\"在现在看来\",{\"1\":{\"19\":1}}],[\"在groupnorm中乘一个标量不会影响输出和反向梯度\",{\"1\":{\"78\":1}}],[\"在glm之前\",{\"1\":{\"26\":1}}],[\"在gpt中\",{\"1\":{\"25\":1}}],[\"在国内\",{\"1\":{\"26\":1}}],[\"在t5模型中\",{\"1\":{\"24\":1}}],[\"在transformer的介绍中我们提到过\",{\"1\":{\"20\":1}}],[\"在transformer中\",{\"1\":{\"17\":1,\"18\":1,\"20\":2,\"28\":1}}],[\"在transformer之前\",{\"1\":{\"16\":1}}],[\"在实验中\",{\"1\":{\"22\":1}}],[\"在bert中\",{\"1\":{\"20\":3,\"27\":2,\"28\":1}}],[\"在解码阶段\",{\"1\":{\"18\":1}}],[\"在2\",{\"1\":{\"18\":1}}],[\"在多个公开可获得数据上进行训练\",{\"1\":{\"37\":1}}],[\"在多头注意力机制和前馈网络完成后\",{\"1\":{\"18\":1}}],[\"在多态中需要将子类的引用赋给父类对象\",{\"1\":{\"10\":1}}],[\"在多态中必须存在有继承关系的子类和父类\",{\"1\":{\"10\":1}}],[\"在自注意力机制中\",{\"1\":{\"17\":1}}],[\"在注意力机制中\",{\"1\":{\"17\":1}}],[\"在介绍transformer之前\",{\"1\":{\"16\":1}}],[\"在如此大的规模下\",{\"1\":{\"14\":1}}],[\"在当年属于标准大小\",{\"1\":{\"14\":1}}],[\"在chatgpt大火之后\",{\"1\":{\"14\":1}}],[\"在继承链中对象方法的调用存在一个优先级\",{\"1\":{\"12\":1}}],[\"在运行时\",{\"1\":{\"11\":1}}],[\"在运行时谈不上多态\",{\"1\":{\"9\":1}}],[\"在接口的多态中\",{\"1\":{\"11\":1}}],[\"在java中\",{\"1\":{\"11\":1}}],[\"在调用方面\",{\"1\":{\"253\":1}}],[\"在调用这些方法时就会调用子类的方法\",{\"1\":{\"10\":1}}],[\"在调用该些方法的时候\",{\"1\":{\"9\":1}}],[\"在调用android\",{\"1\":{\"3\":1}}],[\"在添加成功后给binder回调即可\",{\"1\":{\"3\":1}}],[\"在添加完自定义服务后\",{\"1\":{\"3\":1}}],[\"在创建完displaydevice后\",{\"1\":{\"3\":1}}],[\"在linux\",{\"1\":{\"3\":1}}],[\"在此我们会发现\",{\"1\":{\"3\":1}}],[\"在新设备上安装了android\",{\"1\":{\"0\":1}}],[\"如下命令\",{\"1\":{\"236\":1}}],[\"如下图左侧\",{\"1\":{\"51\":1}}],[\"如下图\",{\"1\":{\"46\":1}}],[\"如第3\",{\"1\":{\"49\":1}}],[\"如今\",{\"1\":{\"25\":1}}],[\"如图b\",{\"1\":{\"77\":1}}],[\"如图9\",{\"1\":{\"51\":1}}],[\"如图6\",{\"1\":{\"32\":2}}],[\"如图5\",{\"1\":{\"27\":1,\"28\":1}}],[\"如图4\",{\"1\":{\"25\":1}}],[\"如图3\",{\"1\":{\"20\":1}}],[\"如图2\",{\"1\":{\"16\":1,\"17\":2}}],[\"如果nums2中仍然还有数据没有处理怎么办\",{\"1\":{\"252\":1}}],[\"如果所有断言都通过\",{\"1\":{\"247\":1}}],[\"如果我们能对这\",{\"1\":{\"236\":1}}],[\"如果允许重复中奖\",{\"1\":{\"226\":1}}],[\"如果直接执行这些计算\",{\"1\":{\"223\":1}}],[\"如果集合中的元素都是整数且长度小于512\",{\"1\":{\"221\":1}}],[\"如果哈希类型的元素个数小于512个\",{\"1\":{\"217\":1}}],[\"如果已经处理过\",{\"1\":{\"213\":1}}],[\"如果已经建立了联合索引\",{\"1\":{\"170\":1}}],[\"如果timeout=0则一直阻塞\",{\"1\":{\"210\":2}}],[\"如果列表长度小于512\",{\"1\":{\"209\":1}}],[\"如果字符串长度增加需要重新分配内存\",{\"1\":{\"206\":1}}],[\"如果设置完锁服务崩了\",{\"1\":{\"198\":1}}],[\"如果大量的key的过期时间过于集中\",{\"1\":{\"197\":1}}],[\"如果大于1了\",{\"1\":{\"152\":1}}],[\"如果服务器宕机\",{\"1\":{\"196\":1}}],[\"如果在这之前redis崩了\",{\"1\":{\"196\":1}}],[\"如果两倍的最长报文等待时间之后服务端还没有收到第四次挥手\",{\"1\":{\"188\":1}}],[\"如果两个对象相互引用\",{\"1\":{\"95\":1}}],[\"如果只有三次挥手\",{\"1\":{\"188\":1}}],[\"如果in或者not\",{\"1\":{\"177\":1}}],[\"如果索引过多\",{\"1\":{\"165\":1}}],[\"如果查询\",{\"1\":{\"161\":1}}],[\"如果查的列是主键\",{\"1\":{\"159\":1}}],[\"如果查的列恰好是有索引的\",{\"1\":{\"159\":1}}],[\"如果查找一个值\",{\"1\":{\"151\":1}}],[\"如果是无序的\",{\"1\":{\"157\":1}}],[\"如果数据是有序的\",{\"1\":{\"157\":1}}],[\"如果有序集合的元素不满足上面的条件\",{\"1\":{\"228\":1}}],[\"如果有序集合的元素个数小于\",{\"1\":{\"228\":1}}],[\"如果有大量的key需要设置同一时间过期\",{\"0\":{\"197\":1}}],[\"如果有\",{\"1\":{\"155\":1}}],[\"如果有两个对象a和b\",{\"1\":{\"7\":1}}],[\"如果没有指定主键所以\",{\"1\":{\"155\":1}}],[\"如果产生了冲突\",{\"1\":{\"150\":1}}],[\"如果说直接去后面翻来找字的话\",{\"1\":{\"147\":1}}],[\"如果以字符串格式赋给string类型的静态变量\",{\"1\":{\"141\":1}}],[\"如果一个字符串对象保存的是一个字符串\",{\"1\":{\"205\":2}}],[\"如果一个字符串对象保存的是整数值\",{\"1\":{\"205\":1}}],[\"如果一个静态变量是final类型的\",{\"1\":{\"141\":1}}],[\"如果一个对象被判断确实有必要执行finalize方法\",{\"1\":{\"98\":1}}],[\"如果一个对象被引用结束了\",{\"1\":{\"95\":1}}],[\"如果一个对象被引用了\",{\"1\":{\"95\":1}}],[\"如果一个对象已经执行过一次finalize方法或者没有覆盖finalize方法\",{\"1\":{\"98\":1}}],[\"如果一个对象最终没有被gc\",{\"1\":{\"96\":1}}],[\"如果一个对象引用次数为0\",{\"1\":{\"95\":1}}],[\"如果存活区空间不够\",{\"1\":{\"89\":1}}],[\"如果position\",{\"1\":{\"28\":1}}],[\"如果这15\",{\"1\":{\"22\":1}}],[\"如果此时模型知道了某个词之后的信息\",{\"1\":{\"18\":1}}],[\"如果不允许重复中奖\",{\"1\":{\"226\":1}}],[\"如果不满足就会进行扩容\",{\"1\":{\"204\":1}}],[\"如果不考虑词在序列中的位置\",{\"1\":{\"18\":1}}],[\"如果不经过特殊处理\",{\"1\":{\"18\":1}}],[\"如\",{\"1\":{\"3\":1,\"27\":1}}],[\"如surfacecontrol\",{\"1\":{\"3\":1}}],[\"如何统计出这连续\",{\"1\":{\"236\":1}}],[\"如何统计这个月首次打卡时间呢\",{\"1\":{\"235\":1}}],[\"如何保证消息的可靠性\",{\"0\":{\"214\":1}}],[\"如何判断一个类可以进行回收\",{\"0\":{\"100\":1}}],[\"如何判断一个常量可以被回收\",{\"0\":{\"99\":1}}],[\"如何获取到displaymanagerservice\",{\"1\":{\"3\":1}}],[\"如何与该自定义适配器进行通信\",{\"1\":{\"3\":1}}],[\"如miui\",{\"1\":{\"3\":1}}],[\"将多个\",{\"1\":{\"238\":1}}],[\"将差集结果存入新集合destination中\",{\"1\":{\"222\":1}}],[\"将并集结果存入新集合destination中\",{\"1\":{\"222\":1}}],[\"将交集结果存入新集合destination中\",{\"1\":{\"222\":1}}],[\"将ack设置为j+1\",{\"1\":{\"187\":1}}],[\"将ack标志位设置为1\",{\"1\":{\"187\":1}}],[\"将d变为d~nm​=dnm​\",{\"1\":{\"78\":1}}],[\"将ml的最佳开源系统部署到不同的基础设施\",{\"1\":{\"58\":1}}],[\"将所有数字拆分为单个数字\",{\"1\":{\"43\":1}}],[\"将最终学习率降低到峰值学习率的10\",{\"1\":{\"42\":1}}],[\"将上下文长度增加了一倍\",{\"1\":{\"38\":1}}],[\"将prompt\",{\"1\":{\"32\":1}}],[\"将激活函数由relu调整为gelus\",{\"1\":{\"29\":1}}],[\"将模型分为多个头\",{\"1\":{\"18\":1}}],[\"将系统类起一个别名\",{\"1\":{\"3\":1}}],[\"将其添加到系统服务列表\",{\"1\":{\"3\":1}}],[\"将该类在android启动时注入framework\",{\"1\":{\"3\":1}}],[\"将应用程序decorview移至decorcaptionview\",{\"1\":{\"3\":1}}],[\"且字符串长度大于某个字节\",{\"1\":{\"205\":1}}],[\"且字符串长度小于等于某个字节\",{\"1\":{\"205\":1}}],[\"且部分应用无法在小窗中启动\",{\"1\":{\"3\":1}}],[\"且使用的virtualdisplay会有一些问题\",{\"1\":{\"3\":1}}],[\"米窗3做了以下操作\",{\"1\":{\"3\":1}}],[\"米窗3实例化了dms\",{\"1\":{\"3\":1}}],[\"米窗3如何通过display系统服务获取到dms呢\",{\"1\":{\"3\":1}}],[\"米窗3利用该功能\",{\"1\":{\"3\":1}}],[\"米窗3选择使用riru\",{\"1\":{\"3\":1}}],[\"米窗3所需要的规则如下\",{\"1\":{\"3\":1}}],[\"米窗3并不能顺利添加自定义系统服务\",{\"1\":{\"3\":1}}],[\"米窗3定义了一个专用系统服务mifreeformservice\",{\"1\":{\"3\":1}}],[\"米窗3亦是如此\",{\"1\":{\"3\":1}}],[\"米窗3采用了与virtualdisplayadapter\",{\"1\":{\"3\":1}}],[\"米窗之前的版本一直不够稳定\",{\"1\":{\"3\":1}}],[\"米窗所用技术备忘\",{\"0\":{\"2\":1}}],[\"技术相关\",{\"0\":{\"3\":1}}],[\"054579\",{\"1\":{\"244\":2}}],[\"030452\",{\"1\":{\"244\":4}}],[\"0301型号\",{\"1\":{\"54\":1}}],[\"034579\",{\"1\":{\"244\":2}}],[\"03\",{\"1\":{\"236\":1}}],[\"02\",{\"1\":{\"236\":1}}],[\"0支持\",{\"1\":{\"201\":1}}],[\"001模型\",{\"1\":{\"54\":1}}],[\"00190\",{\"1\":{\"33\":1}}],[\"005\",{\"1\":{\"50\":1}}],[\"01\",{\"1\":{\"50\":1,\"236\":1}}],[\"0的梯度剪裁\",{\"1\":{\"42\":1,\"50\":1}}],[\"09685\",{\"1\":{\"33\":1}}],[\"074\",{\"1\":{\"33\":1}}],[\"0\",{\"0\":{\"3\":1,\"181\":1},\"1\":{\"3\":1,\"28\":1,\"50\":1,\"204\":1,\"217\":1,\"228\":1,\"233\":3,\"235\":1,\"236\":2,\"237\":1,\"247\":5,\"248\":12,\"249\":7,\"251\":10,\"253\":12}}],[\"39\",{\"1\":{\"244\":4}}],[\"398651743\",{\"1\":{\"70\":1}}],[\"33\",{\"1\":{\"244\":3}}],[\"33b和falcon\",{\"1\":{\"54\":1}}],[\"32bit\",{\"1\":{\"183\":2}}],[\"34b与同等尺寸的vicuna\",{\"1\":{\"54\":1}}],[\"34b\",{\"1\":{\"37\":1}}],[\"3059\",{\"1\":{\"33\":1}}],[\"3045\",{\"1\":{\"33\":1}}],[\"3公布\",{\"1\":{\"14\":1}}],[\"3\",{\"0\":{\"3\":1,\"18\":1,\"20\":1,\"21\":1,\"22\":2,\"29\":1,\"44\":1,\"45\":1,\"46\":1,\"47\":1,\"48\":1,\"49\":1,\"50\":2,\"51\":2,\"52\":1,\"53\":1,\"54\":1,\"58\":1,\"62\":1,\"63\":1,\"64\":2,\"65\":1,\"66\":1,\"67\":1,\"68\":1,\"69\":1,\"70\":1,\"71\":1,\"79\":1,\"80\":1,\"99\":1,\"101\":1,\"102\":1,\"103\":1,\"104\":2,\"106\":1,\"110\":1,\"115\":1,\"119\":1,\"124\":1,\"130\":1,\"139\":1,\"141\":1,\"149\":1,\"150\":1,\"151\":1,\"152\":2,\"153\":1,\"154\":1,\"166\":1,\"174\":1,\"185\":1,\"211\":1,\"212\":1,\"213\":1,\"214\":2,\"215\":1,\"216\":1,\"217\":1,\"218\":1,\"219\":2,\"223\":1,\"224\":1,\"225\":1,\"226\":2,\"230\":1,\"236\":1,\"243\":1,\"244\":1,\"247\":1},\"1\":{\"12\":1,\"13\":1,\"17\":2,\"18\":1,\"20\":1,\"25\":1,\"28\":2,\"29\":1,\"32\":1,\"33\":1,\"49\":2,\"50\":1,\"54\":1,\"77\":2,\"105\":1,\"226\":5,\"235\":2,\"236\":1,\"240\":1,\"245\":3,\"247\":5,\"248\":8,\"249\":10,\"251\":7,\"253\":1}}],[\"ftp\",{\"1\":{\"82\":1}}],[\"ffn\",{\"1\":{\"76\":1,\"79\":2}}],[\"falcon\",{\"1\":{\"54\":1}}],[\"fsdp\",{\"1\":{\"50\":1}}],[\"full\",{\"1\":{\"92\":1}}],[\"fully\",{\"1\":{\"50\":1}}],[\"function\",{\"1\":{\"49\":1}}],[\"furu\",{\"1\":{\"33\":1}}],[\"feedback启发的非常简单的方法\",{\"1\":{\"51\":1}}],[\"feedback保持一致\",{\"1\":{\"50\":1}}],[\"feed\",{\"1\":{\"18\":1,\"76\":1}}],[\"float\",{\"1\":{\"3\":1}}],[\"francine\",{\"1\":{\"33\":1}}],[\"frameworks\",{\"1\":{\"3\":1}}],[\"framework\",{\"1\":{\"3\":2,\"33\":1}}],[\"from\",{\"1\":{\"19\":1,\"50\":1,\"51\":1}}],[\"freeform服务\",{\"1\":{\"3\":1}}],[\"freeform\",{\"0\":{\"3\":1},\"1\":{\"3\":5}}],[\"file\",{\"1\":{\"66\":1,\"196\":1}}],[\"field\",{\"1\":{\"3\":7,\"218\":9}}],[\"finetuned\",{\"1\":{\"46\":1}}],[\"fine\",{\"0\":{\"45\":1,\"46\":1},\"1\":{\"30\":1,\"33\":1,\"50\":1}}],[\"final\",{\"1\":{\"3\":1}}],[\"find\",{\"1\":{\"3\":2}}],[\"found\",{\"1\":{\"49\":2}}],[\"foundation\",{\"1\":{\"2\":1}}],[\"fork子进程来进行写入操作\",{\"1\":{\"196\":1}}],[\"forward\",{\"1\":{\"18\":1,\"76\":1}}],[\"forname\",{\"1\":{\"3\":1}}],[\"for\",{\"1\":{\"1\":1,\"3\":2,\"33\":6,\"49\":3,\"50\":2,\"247\":1,\"248\":1,\"249\":1}}],[\"m|km|ft|mi\",{\"1\":{\"242\":2}}],[\"mb\",{\"1\":{\"236\":2}}],[\"mq\",{\"1\":{\"213\":1}}],[\"mqa\",{\"1\":{\"41\":1}}],[\"mysql的innodb和myisam\",{\"1\":{\"154\":1}}],[\"mysql索引失效\",{\"0\":{\"171\":1}}],[\"mysql索引\",{\"0\":{\"146\":1}}],[\"myle\",{\"1\":{\"33\":1}}],[\"msr\",{\"1\":{\"76\":1,\"78\":1,\"79\":1}}],[\"ms\",{\"1\":{\"74\":1,\"207\":1}}],[\"msyncroot\",{\"1\":{\"3\":1}}],[\"ml是一个跟踪\",{\"1\":{\"62\":1}}],[\"mlflow就派上用场了\",{\"1\":{\"59\":1}}],[\"mlflow是机器学习工程师通过实验\",{\"1\":{\"59\":1}}],[\"mlflow是一个开源工具\",{\"1\":{\"59\":1}}],[\"mlflow是一个ml平台\",{\"1\":{\"59\":1}}],[\"mlflow\",{\"0\":{\"59\":1},\"1\":{\"59\":1}}],[\"ml工作流管道中的每个任务都用一个容器来处理\",{\"1\":{\"58\":1}}],[\"ml\",{\"0\":{\"61\":1,\"62\":1},\"1\":{\"55\":2,\"56\":1,\"58\":1,\"61\":1}}],[\"mlops是一门工程学科\",{\"1\":{\"55\":1}}],[\"mlops\",{\"0\":{\"55\":1},\"1\":{\"55\":1,\"61\":1,\"71\":1,\"72\":2}}],[\"mlm根据整个句子信息来推断被遮住的token\",{\"1\":{\"22\":1}}],[\"mlm的要做的事就是根据上下文来预测被遮住的token应该是什么\",{\"1\":{\"22\":1}}],[\"mlm\",{\"1\":{\"22\":1,\"27\":1}}],[\"mlm会随机地遮住输入的某些token\",{\"1\":{\"22\":1}}],[\"mpt\",{\"1\":{\"54\":1}}],[\"m\",{\"1\":{\"49\":1,\"251\":11,\"253\":7}}],[\"mha\",{\"1\":{\"41\":1}}],[\"mhandler\",{\"1\":{\"3\":1}}],[\"merge\",{\"1\":{\"253\":1}}],[\"member2\",{\"1\":{\"242\":1}}],[\"member1\",{\"1\":{\"242\":1}}],[\"member\",{\"1\":{\"222\":5,\"229\":6,\"242\":6}}],[\"memcached只有简单的字符串类型\",{\"1\":{\"192\":1}}],[\"metaflow是一个强大的\",{\"1\":{\"65\":1}}],[\"metaflow\",{\"0\":{\"65\":1}}],[\"meta\",{\"1\":{\"49\":2}}],[\"meta总共收集了27540个好数据\",{\"1\":{\"46\":1}}],[\"methods\",{\"1\":{\"33\":2}}],[\"meeting\",{\"1\":{\"33\":1}}],[\"mechanism\",{\"1\":{\"16\":1,\"17\":1,\"75\":1}}],[\"multi\",{\"0\":{\"78\":1},\"1\":{\"18\":3,\"25\":2,\"41\":1,\"75\":2,\"76\":1,\"78\":1,\"79\":1}}],[\"muihandler\",{\"1\":{\"3\":1}}],[\"monitoring检查数据的变化\",{\"1\":{\"72\":1}}],[\"monitoring在监控机器学习模型部署方面发挥着最关键的作用\",{\"1\":{\"72\":1}}],[\"monitoring\",{\"1\":{\"70\":1}}],[\"more\",{\"1\":{\"49\":2}}],[\"moens\",{\"1\":{\"33\":1}}],[\"movie\",{\"1\":{\"18\":2}}],[\"models\",{\"1\":{\"33\":2,\"41\":1,\"46\":1,\"50\":1}}],[\"model即隐层维数\",{\"1\":{\"18\":1}}],[\"model\",{\"1\":{\"14\":1,\"22\":1,\"26\":1,\"27\":1,\"33\":2,\"41\":1,\"49\":2}}],[\"mdisplaydevicerepo\",{\"1\":{\"3\":1}}],[\"mdns\",{\"1\":{\"1\":1}}],[\"mcontext\",{\"1\":{\"3\":1}}],[\"mfs\",{\"1\":{\"3\":1}}],[\"mf\",{\"1\":{\"3\":2}}],[\"mfclass\",{\"1\":{\"3\":2}}],[\"maiˈzæm\",{\"1\":{\"154\":1}}],[\"main\",{\"1\":{\"12\":1,\"105\":2}}],[\"map\",{\"1\":{\"105\":4}}],[\"map<string\",{\"1\":{\"3\":1}}],[\"major\",{\"1\":{\"92\":1}}],[\"max\",{\"1\":{\"78\":1,\"229\":3}}],[\"marry\",{\"1\":{\"226\":1}}],[\"mark\",{\"0\":{\"116\":1},\"1\":{\"226\":3}}],[\"margin\",{\"1\":{\"49\":3}}],[\"marie\",{\"1\":{\"33\":1}}],[\"mandar\",{\"1\":{\"33\":1}}],[\"manager\",{\"1\":{\"3\":2}}],[\"machine\",{\"1\":{\"55\":1}}],[\"mach\",{\"1\":{\"33\":1}}],[\"matena\",{\"1\":{\"33\":1}}],[\"masked\",{\"1\":{\"33\":1}}],[\"mask掉的15\",{\"1\":{\"27\":1}}],[\"mask\",{\"1\":{\"22\":4,\"27\":3,\"31\":1,\"77\":4}}],[\"magisk可以在sepolicy\",{\"1\":{\"3\":1}}],[\"milliseconds来在争夺锁的同时来设置过期时间\",{\"1\":{\"198\":1}}],[\"mixed\",{\"1\":{\"92\":1}}],[\"microsoft\",{\"1\":{\"60\":1}}],[\"michael\",{\"1\":{\"33\":1}}],[\"min\",{\"1\":{\"229\":3}}],[\"minor\",{\"1\":{\"89\":1,\"92\":1}}],[\"mini\",{\"1\":{\"50\":1}}],[\"ming\",{\"1\":{\"33\":4}}],[\"mike\",{\"1\":{\"33\":1}}],[\"mifreeformserver\",{\"1\":{\"3\":1}}],[\"mifreeformdisplayadapter\",{\"1\":{\"3\":2}}],[\"mi\",{\"0\":{\"3\":1},\"1\":{\"3\":4}}],[\"while\",{\"1\":{\"253\":2}}],[\"where\",{\"1\":{\"49\":3,\"161\":2,\"174\":1,\"175\":1}}],[\"w1​\",{\"1\":{\"79\":1}}],[\"w2​是参数矩阵\",{\"1\":{\"79\":1}}],[\"w2​\",{\"1\":{\"79\":1}}],[\"wv​∈rd×d\",{\"1\":{\"78\":1}}],[\"wk​\",{\"1\":{\"78\":1}}],[\"wk​中\",{\"1\":{\"77\":1}}],[\"wk​指的是可学习的矩阵\",{\"1\":{\"77\":1}}],[\"www\",{\"1\":{\"58\":2,\"61\":1,\"62\":1,\"64\":1,\"66\":1,\"68\":1,\"69\":1,\"70\":1,\"71\":1}}],[\"wuen\",{\"1\":{\"33\":1}}],[\"w\",{\"1\":{\"33\":1}}],[\"wandb\",{\"0\":{\"63\":1},\"1\":{\"63\":1}}],[\"wang\",{\"1\":{\"33\":4}}],[\"warmup\",{\"1\":{\"42\":1}}],[\"warn\",{\"1\":{\"1\":1}}],[\"wallis\",{\"1\":{\"33\":1}}],[\"wilde\",{\"1\":{\"51\":1}}],[\"window或batch\",{\"1\":{\"41\":1}}],[\"withhash\",{\"1\":{\"242\":1}}],[\"withdist\",{\"1\":{\"242\":1}}],[\"withcoord\",{\"1\":{\"242\":1}}],[\"withscores\",{\"1\":{\"229\":3}}],[\"with\",{\"1\":{\"33\":1,\"49\":3}}],[\"width\",{\"1\":{\"3\":2}}],[\"wireless\",{\"1\":{\"1\":1}}],[\"weakhashmap<>\",{\"1\":{\"105\":1}}],[\"weakhashmap<string\",{\"1\":{\"105\":1}}],[\"weakhashmap\",{\"1\":{\"105\":5}}],[\"weakhashmap测试\",{\"1\":{\"105\":1}}],[\"wenhui\",{\"1\":{\"33\":1}}],[\"wen\",{\"1\":{\"33\":1}}],[\"weight\",{\"1\":{\"50\":1}}],[\"weights每个key对应的分值乘积\",{\"1\":{\"229\":2}}],[\"weights\",{\"0\":{\"63\":1},\"1\":{\"49\":1,\"63\":1}}],[\"wei\",{\"1\":{\"33\":3}}],[\"we\",{\"1\":{\"3\":1,\"49\":2}}],[\"d240323\",{\"1\":{\"105\":1}}],[\"dns\",{\"1\":{\"82\":1}}],[\"d​\",{\"1\":{\"78\":1}}],[\"dmodel​是隐层维数\",{\"1\":{\"76\":1,\"79\":1}}],[\"dmsclass\",{\"1\":{\"3\":1}}],[\"dms\",{\"1\":{\"3\":1}}],[\"dvc不仅仅是一个数据跟踪和版本控制工具\",{\"1\":{\"67\":1}}],[\"dvc\",{\"0\":{\"67\":1},\"1\":{\"67\":1}}],[\"dags\",{\"1\":{\"57\":1}}],[\"database\",{\"1\":{\"196\":1}}],[\"data就是这一行的内容了\",{\"1\":{\"155\":1}}],[\"datacamp\",{\"1\":{\"61\":1}}],[\"data\",{\"0\":{\"67\":1},\"1\":{\"50\":1,\"67\":1}}],[\"danqi\",{\"1\":{\"33\":1}}],[\"das\",{\"1\":{\"33\":1}}],[\"dimension\",{\"1\":{\"78\":1}}],[\"dim=−2\",{\"1\":{\"77\":1}}],[\"dim\",{\"1\":{\"77\":9}}],[\"distinct\",{\"1\":{\"49\":1}}],[\"discrete\",{\"1\":{\"49\":1}}],[\"display\",{\"1\":{\"3\":2}}],[\"displaymanagerservice\",{\"1\":{\"3\":4}}],[\"display系统服务是下面类的实例\",{\"1\":{\"3\":1}}],[\"display系统服务是一个binder\",{\"1\":{\"3\":1}}],[\"dipanjan\",{\"1\":{\"33\":1}}],[\"ding\",{\"1\":{\"33\":2}}],[\"du\",{\"1\":{\"33\":3}}],[\"docs\",{\"1\":{\"72\":1}}],[\"dong\",{\"1\":{\"33\":1}}],[\"dot\",{\"1\":{\"18\":1}}],[\"do\",{\"1\":{\"18\":4}}],[\"d\",{\"1\":{\"3\":1,\"12\":9,\"18\":1,\"20\":3,\"78\":1}}],[\"destpos\",{\"1\":{\"253\":2}}],[\"dest\",{\"1\":{\"253\":2}}],[\"destmap\",{\"1\":{\"236\":4}}],[\"destkey\",{\"1\":{\"229\":2,\"233\":1,\"236\":1,\"238\":1}}],[\"destination\",{\"1\":{\"222\":3}}],[\"delegate\",{\"1\":{\"103\":2}}],[\"def\",{\"1\":{\"77\":3}}],[\"definition\",{\"1\":{\"55\":1}}],[\"default\",{\"1\":{\"3\":2}}],[\"devops管道和azure\",{\"1\":{\"61\":1}}],[\"devops管道是一个ci\",{\"1\":{\"60\":1}}],[\"devops\",{\"0\":{\"60\":1},\"1\":{\"60\":2}}],[\"developer\",{\"1\":{\"58\":1}}],[\"dev\",{\"1\":{\"55\":1,\"69\":1}}],[\"devlin\",{\"1\":{\"33\":1}}],[\"decay\",{\"1\":{\"50\":1,\"77\":10}}],[\"decomposable\",{\"1\":{\"33\":1}}],[\"decoder结构\",{\"1\":{\"25\":1}}],[\"decoder中的multi\",{\"1\":{\"18\":1}}],[\"decoder中decoder单元数n=6\",{\"1\":{\"18\":1}}],[\"decoder和encoder最大的区别\",{\"1\":{\"18\":1}}],[\"decoder模块等\",{\"1\":{\"18\":1}}],[\"decoder\",{\"1\":{\"16\":1,\"18\":1}}],[\"detailed\",{\"1\":{\"49\":1}}],[\"deep\",{\"1\":{\"33\":1}}],[\"densitydpi\",{\"1\":{\"3\":2}}],[\"dex\",{\"1\":{\"3\":2}}],[\"debugging来解决\",{\"1\":{\"1\":1}}],[\"deployment选项中的debugger\",{\"1\":{\"1\":1}}],[\"ddmlib\",{\"1\":{\"1\":1}}],[\"找到build\",{\"1\":{\"1\":1}}],[\"找到idea\",{\"1\":{\"1\":1}}],[\"这道题其实做完力扣26题后并不难\",{\"1\":{\"249\":1}}],[\"这道题来自力扣80题\",{\"1\":{\"249\":1}}],[\"这道题想要做出来并不复杂\",{\"1\":{\"246\":1}}],[\"这意味着在函数里修改输入数组对于调用者是可见的\",{\"1\":{\"248\":1,\"249\":1}}],[\"这其中的两个关键机制就是\",{\"1\":{\"241\":1}}],[\"这几种的介绍见redis数据类型\",{\"1\":{\"193\":1}}],[\"这种\",{\"1\":{\"216\":1}}],[\"这种情况就是覆盖索引了\",{\"1\":{\"159\":1}}],[\"这种温度重新缩放都会发生恒定数量的步骤\",{\"1\":{\"50\":1}}],[\"这也是为什么jvm堆要进行分代的原因\",{\"1\":{\"111\":1}}],[\"这也是transformer成功的关键\",{\"1\":{\"16\":1}}],[\"这一块整体清除\",{\"1\":{\"109\":1}}],[\"这一点是不同于p\",{\"1\":{\"32\":1}}],[\"这在flops和内存消耗方面都是比较好的\",{\"1\":{\"79\":1}}],[\"这在使用o\",{\"1\":{\"50\":1}}],[\"这两个命令\",{\"1\":{\"244\":1}}],[\"这两个模式可以利用gpu加速计算\",{\"1\":{\"79\":1}}],[\"这两个强化学习\",{\"1\":{\"50\":1}}],[\"这有利于推理\",{\"1\":{\"77\":1}}],[\"这有助于快速了解数据出了什么问题并设计下一个行动计划\",{\"1\":{\"72\":1}}],[\"这导致了部署基于transformer的模型不是很友好\",{\"1\":{\"75\":1}}],[\"这可以较好地拟合自回归解码\",{\"1\":{\"79\":1}}],[\"这可以在不牺牲性能的情况下提高解码吞吐量\",{\"1\":{\"74\":1}}],[\"这可以让模型支持记住更多对话历史信息\",{\"1\":{\"41\":1}}],[\"这支持三个模式\",{\"1\":{\"74\":1}}],[\"这些都离不开基于位置信息服务\",{\"1\":{\"240\":1}}],[\"这些都是根据上下文\",{\"1\":{\"22\":1}}],[\"这些特性使retnet可以成为大型语言模型的transformer的强大继承者\",{\"1\":{\"75\":1}}],[\"这些步骤生成了一个sft数据集\",{\"1\":{\"51\":1}}],[\"这与constitutional\",{\"1\":{\"50\":1}}],[\"这部电影是\",{\"1\":{\"31\":1}}],[\"这对很多企业和个人仍然是不能接受的\",{\"1\":{\"30\":1}}],[\"这会造成预训练和微调之间产生一个不匹配的情况\",{\"1\":{\"22\":1}}],[\"这个注解标记的方法会在jvm内部实现\",{\"1\":{\"253\":1}}],[\"这个注解可以标记\",{\"1\":{\"253\":1}}],[\"这个方法属于hotspot\",{\"1\":{\"253\":1}}],[\"这个方法共接收5个参数\",{\"1\":{\"253\":1}}],[\"这个指令用于对一个或者多个\",{\"1\":{\"236\":1}}],[\"这个还挺抽象的\",{\"1\":{\"213\":1}}],[\"这个我们可以通过三次握手分别能确认什么状态来考虑\",{\"1\":{\"185\":1}}],[\"这个不一定\",{\"1\":{\"177\":1}}],[\"这个另说\",{\"1\":{\"169\":1}}],[\"这个类就会被卸载\",{\"1\":{\"145\":1}}],[\"这个类就可以被回收\",{\"1\":{\"100\":1}}],[\"这个过程要完成三个事\",{\"1\":{\"134\":1}}],[\"这个过程很容易联想到反射\",{\"1\":{\"134\":1}}],[\"这个加载过程就是把class字节码的内容加载到内存当中去\",{\"1\":{\"134\":1}}],[\"这个速度很快\",{\"1\":{\"116\":1}}],[\"这个对象会进入老年代\",{\"1\":{\"91\":1}}],[\"这个q和k\",{\"1\":{\"77\":1}}],[\"这个增量随着样本的增加而增加\",{\"1\":{\"50\":1}}],[\"这个奖励模型优于所有base\",{\"1\":{\"49\":1}}],[\"这个都是可以学习的地方\",{\"1\":{\"49\":1}}],[\"这个现在都在用\",{\"1\":{\"40\":1}}],[\"这个比例在t5模型的论文中证明\",{\"1\":{\"27\":1}}],[\"这个关系是没有办法被语言模型直接捕获到的\",{\"1\":{\"22\":1}}],[\"这个是什么呢\",{\"1\":{\"21\":1}}],[\"这个d其实就是模型最大能接受的token长度\",{\"1\":{\"20\":1}}],[\"这个问题可以通过android\",{\"1\":{\"1\":1}}],[\"这限制了预训练\",{\"1\":{\"19\":1}}],[\"这是一道比较简单的双指针问题\",{\"1\":{\"252\":1}}],[\"这是一道数组题\",{\"1\":{\"245\":1}}],[\"这是一种受上下文蒸馏\",{\"1\":{\"51\":1}}],[\"这是进行查找和普通链表没什么区别了\",{\"1\":{\"151\":1}}],[\"这是transformer的相对位置embedding\",{\"1\":{\"77\":1}}],[\"这是很难的\",{\"1\":{\"75\":1}}],[\"这是普通公司和个人难以承受的\",{\"1\":{\"25\":1}}],[\"这是我们不希望发生的\",{\"1\":{\"18\":1}}],[\"这是因为当前的classloader中不包括\",{\"1\":{\"3\":1}}],[\"这是因为内部类持有外部类的实例\",{\"1\":{\"3\":1}}],[\"这绝对可以称之为\",{\"1\":{\"14\":1}}],[\"这就是多态性\",{\"1\":{\"9\":1}}],[\"这样一来\",{\"1\":{\"241\":1}}],[\"这样一个月也就30多bit\",{\"1\":{\"234\":1}}],[\"这样就可以根据score进行排序了\",{\"1\":{\"227\":1}}],[\"这样就可以在执行失败后读备份list中的数据执行了\",{\"1\":{\"214\":1}}],[\"这样就可以以插入成功作为加锁成功\",{\"1\":{\"207\":1}}],[\"这样效率会好一些\",{\"1\":{\"212\":1}}],[\"这样效率会高一些\",{\"1\":{\"161\":1}}],[\"这样效率比较低\",{\"1\":{\"212\":1}}],[\"这样做也有缺点\",{\"1\":{\"206\":1}}],[\"这样做的好处\",{\"1\":{\"206\":1}}],[\"这样每执行一条命令\",{\"1\":{\"196\":1}}],[\"这样只需要存一个b+树\",{\"1\":{\"167\":1}}],[\"这样查找的时候就不需要进行回表操作\",{\"1\":{\"160\":1}}],[\"这样查找效率可能会有所下降\",{\"1\":{\"153\":1}}],[\"这样会减少一次io操作\",{\"1\":{\"157\":1}}],[\"这样会导致softmax产生非常小的值\",{\"1\":{\"18\":1}}],[\"这样通过二级索引来检索数据\",{\"1\":{\"155\":1}}],[\"这样来说\",{\"1\":{\"154\":1}}],[\"这样消耗的时间比较大\",{\"1\":{\"152\":1}}],[\"这样的话更新索引也需要处理这些数据\",{\"1\":{\"157\":1}}],[\"这样的话\",{\"1\":{\"151\":1}}],[\"这样清除可能导致的最大问题就是内存碎片\",{\"1\":{\"108\":1}}],[\"这样我们可以得到an−m=λ\",{\"1\":{\"77\":1}}],[\"这样可以进一步提高预测能力\",{\"1\":{\"27\":1}}],[\"这样预测的概率可能不准确\",{\"1\":{\"22\":1}}],[\"这样看来\",{\"1\":{\"20\":1,\"32\":1}}],[\"这样\",{\"1\":{\"9\":1,\"18\":1,\"24\":1,\"78\":2,\"110\":1,\"235\":1}}],[\"这里以滴滴叫车的场景为例\",{\"1\":{\"244\":1}}],[\"这里有一个潜在的风险\",{\"1\":{\"223\":1}}],[\"这里说的是类\",{\"1\":{\"100\":1}}],[\"这里的γ\",{\"1\":{\"77\":1}}],[\"这里的wq​\",{\"1\":{\"77\":1}}],[\"这里补充一些介绍\",{\"1\":{\"72\":1}}],[\"这里\",{\"1\":{\"30\":1}}],[\"这里我们不做过多介绍\",{\"1\":{\"22\":1}}],[\"这里不再赘述\",{\"1\":{\"18\":1}}],[\"这里米窗3使用了hiddenapirefineplugin来处理\",{\"1\":{\"3\":1}}],[\"这里注意到\",{\"1\":{\"3\":1}}],[\"这里无法通过else获取到米窗3自定义的系统服务\",{\"1\":{\"3\":1}}],[\"这段时间一直采用在android\",{\"1\":{\"0\":1}}],[\"spop\",{\"1\":{\"222\":1,\"226\":4}}],[\"specia\",{\"1\":{\"33\":1}}],[\"srcpos\",{\"1\":{\"253\":2}}],[\"src\",{\"1\":{\"253\":2}}],[\"srandmember\",{\"1\":{\"222\":1,\"226\":4}}],[\"srem\",{\"1\":{\"222\":1}}],[\"sdiffstore\",{\"1\":{\"222\":1}}],[\"sdiff\",{\"1\":{\"222\":1}}],[\"sds的api是安全的\",{\"1\":{\"204\":1}}],[\"sds中拼接字符串不会导致缓冲区的溢出\",{\"1\":{\"204\":1}}],[\"sds中获取字符串长度的时间复杂度为o\",{\"1\":{\"204\":1}}],[\"sds中的所有api都会以处理二进制的方式来处理存储在buf\",{\"1\":{\"204\":1}}],[\"sds不仅可以存储文本\",{\"1\":{\"204\":1}}],[\"sds不仅可以保存文本数据\",{\"1\":{\"204\":1}}],[\"sds使用一个参数len来保存字符串长度\",{\"1\":{\"204\":1}}],[\"sds相比c标准字符串\",{\"1\":{\"204\":1}}],[\"sds和c标准字符串不太一样\",{\"1\":{\"204\":1}}],[\"sdk支持部署新训练的模型或与已部署的模型链接\",{\"1\":{\"72\":1}}],[\"sweep\",{\"0\":{\"116\":1}}],[\"swish\",{\"1\":{\"78\":1}}],[\"swiglu\",{\"1\":{\"40\":1}}],[\"syn\",{\"1\":{\"183\":1}}],[\"synchronized\",{\"1\":{\"103\":1}}],[\"systems\",{\"1\":{\"33\":1}}],[\"systemserviceregistry\",{\"1\":{\"3\":1}}],[\"systemservice\",{\"1\":{\"3\":1}}],[\"system\",{\"0\":{\"250\":1},\"1\":{\"3\":3,\"12\":9,\"105\":5,\"253\":2}}],[\"ssh\",{\"1\":{\"82\":1}}],[\"smembers\",{\"1\":{\"222\":1}}],[\"smtp\",{\"1\":{\"82\":1}}],[\"smaller\",{\"1\":{\"49\":1}}],[\"sn​=asn−1​+knt​vn​\",{\"1\":{\"77\":1}}],[\"solution\",{\"1\":{\"253\":1}}],[\"sorted\",{\"0\":{\"227\":1},\"1\":{\"193\":1,\"200\":1,\"241\":5}}],[\"softreference\",{\"1\":{\"103\":4}}],[\"softreference<object>\",{\"1\":{\"103\":1}}],[\"sourcekey\",{\"1\":{\"238\":2}}],[\"source\",{\"1\":{\"72\":1}}],[\"songhao\",{\"1\":{\"33\":1}}],[\"sql\",{\"1\":{\"66\":1}}],[\"sary\",{\"1\":{\"226\":3}}],[\"sadd\",{\"1\":{\"222\":1}}],[\"sampling\",{\"1\":{\"50\":1}}],[\"samples\",{\"1\":{\"49\":1}}],[\"safety\",{\"1\":{\"49\":1}}],[\"salimans\",{\"1\":{\"33\":1}}],[\"sign\",{\"1\":{\"235\":4}}],[\"sinterstore\",{\"1\":{\"222\":1}}],[\"sinter\",{\"1\":{\"222\":1}}],[\"since\",{\"1\":{\"105\":1}}],[\"sismember\",{\"1\":{\"222\":1}}],[\"site\",{\"1\":{\"62\":1,\"63\":1}}],[\"similar\",{\"1\":{\"49\":1}}],[\"size不敏感\",{\"1\":{\"75\":1}}],[\"size\",{\"1\":{\"46\":1,\"50\":2,\"77\":4,\"103\":1}}],[\"size的增加\",{\"1\":{\"41\":1}}],[\"sft\",{\"0\":{\"46\":1},\"1\":{\"50\":1}}],[\"shuzu\",{\"0\":{\"261\":1}}],[\"shen\",{\"1\":{\"33\":1}}],[\"sharded\",{\"1\":{\"50\":1}}],[\"sharan\",{\"1\":{\"33\":1}}],[\"shazeer\",{\"1\":{\"33\":2}}],[\"shown\",{\"1\":{\"49\":1}}],[\"show\",{\"1\":{\"12\":17}}],[\"shouldshowsystemdecorations\",{\"1\":{\"3\":1}}],[\"s~z<i~指的是partb的部分\",{\"1\":{\"27\":1}}],[\"s~2~对应着\",{\"1\":{\"27\":1}}],[\"s\",{\"1\":{\"27\":6,\"33\":1,\"103\":2}}],[\"scott\",{\"1\":{\"33\":1}}],[\"score是可以重复\",{\"1\":{\"227\":1}}],[\"scores\",{\"1\":{\"78\":1}}],[\"score的过程如下\",{\"1\":{\"17\":1}}],[\"score\",{\"1\":{\"17\":2,\"49\":1,\"229\":2}}],[\"scard\",{\"1\":{\"222\":1}}],[\"scavenge是一个重视cpu效率的回收器\",{\"1\":{\"115\":1}}],[\"scavenge\",{\"0\":{\"115\":1},\"1\":{\"112\":1}}],[\"scalar\",{\"1\":{\"49\":1}}],[\"scaling\",{\"1\":{\"46\":1,\"50\":1}}],[\"scale\",{\"0\":{\"78\":1},\"1\":{\"33\":1,\"75\":1,\"76\":1,\"78\":1,\"79\":1}}],[\"scales\",{\"1\":{\"33\":1}}],[\"scaled\",{\"1\":{\"18\":1}}],[\"scan\",{\"1\":{\"3\":3}}],[\"scache\",{\"1\":{\"3\":1}}],[\"sup>\",{\"1\":{\"245\":3,\"247\":3,\"249\":3,\"251\":2}}],[\"supervised\",{\"0\":{\"46\":1}}],[\"super\",{\"1\":{\"12\":4}}],[\"sunionstore\",{\"1\":{\"222\":1}}],[\"sunion\",{\"1\":{\"222\":1}}],[\"sum\",{\"1\":{\"77\":1}}],[\"summarization类似\",{\"1\":{\"50\":1}}],[\"sutskever\",{\"1\":{\"33\":1}}],[\"surface\",{\"1\":{\"3\":2}}],[\"start\",{\"1\":{\"210\":1,\"229\":2,\"233\":2,\"235\":2}}],[\"static\",{\"1\":{\"3\":1,\"12\":1,\"105\":1,\"253\":1}}],[\"stream\",{\"1\":{\"193\":1,\"201\":1}}],[\"string类型会保存为二进制的字节数组\",{\"1\":{\"232\":1}}],[\"string内部编码有int\",{\"1\":{\"205\":1}}],[\"string底层的数据结构主要是int和sds\",{\"1\":{\"204\":1}}],[\"string是最基本的key\",{\"1\":{\"202\":1}}],[\"string>\",{\"1\":{\"105\":2}}],[\"string\",{\"0\":{\"202\":1},\"1\":{\"3\":3,\"12\":5,\"103\":3,\"105\":3,\"193\":1,\"200\":1}}],[\"step\",{\"1\":{\"50\":1}}],[\"steps\",{\"1\":{\"42\":1,\"49\":1}}],[\"storedist\",{\"1\":{\"242\":1}}],[\"store\",{\"1\":{\"242\":1}}],[\"story\",{\"1\":{\"18\":2}}],[\"stock\",{\"1\":{\"213\":1}}],[\"stop\",{\"1\":{\"210\":1,\"229\":2}}],[\"stoyanov\",{\"1\":{\"33\":1}}],[\"stub\",{\"1\":{\"3\":4}}],[\"studio的日志文件夹\",{\"1\":{\"1\":1}}],[\"studio的菜单\",{\"1\":{\"1\":2}}],[\"studio启动之前手动启动一个adb程序来临时解决\",{\"1\":{\"0\":1}}],[\"studio\",{\"0\":{\"0\":1},\"1\":{\"0\":1}}],[\"sean\",{\"1\":{\"226\":2}}],[\"search\",{\"1\":{\"50\":1}}],[\"session可以存到服务器上\",{\"1\":{\"207\":1}}],[\"seconds来给锁设置一个过期时间\",{\"1\":{\"198\":1}}],[\"secure\",{\"1\":{\"3\":1}}],[\"setbit\",{\"1\":{\"233\":1,\"235\":1}}],[\"set可以保证一个id只点赞一次\",{\"1\":{\"224\":1}}],[\"set底层数据结构有两种\",{\"1\":{\"221\":1}}],[\"set无序\",{\"1\":{\"220\":1}}],[\"set只能存储非重复数据\",{\"1\":{\"220\":1}}],[\"set和list的区别\",{\"1\":{\"220\":1}}],[\"set是一个无序且唯一的键值集合\",{\"1\":{\"220\":1}}],[\"set命令还有一个参数px\",{\"1\":{\"207\":1}}],[\"set\",{\"0\":{\"220\":1,\"227\":1},\"1\":{\"193\":2,\"200\":2,\"223\":3,\"229\":1,\"237\":1,\"241\":5}}],[\"send状态\",{\"1\":{\"184\":1}}],[\"sentence\",{\"1\":{\"22\":1}}],[\"serial使用了标记\",{\"1\":{\"113\":2}}],[\"serial使用了分代收集\",{\"1\":{\"113\":1}}],[\"serial是jvm第一个垃圾收集器\",{\"1\":{\"113\":1}}],[\"serial\",{\"0\":{\"113\":1},\"1\":{\"112\":1}}],[\"serving集群的帮助下实现\",{\"1\":{\"72\":1}}],[\"serving\",{\"1\":{\"69\":1}}],[\"servicenotfoundexception\",{\"1\":{\"3\":1}}],[\"service`s\",{\"1\":{\"3\":1}}],[\"services\",{\"1\":{\"3\":1}}],[\"servicemanager\",{\"1\":{\"3\":6}}],[\"service\",{\"1\":{\"3\":15,\"240\":1}}],[\"server\",{\"1\":{\"1\":1,\"3\":3}}],[\"separable\",{\"1\":{\"49\":1}}],[\"select\",{\"0\":{\"173\":1},\"1\":{\"161\":3,\"174\":1,\"175\":1}}],[\"self\",{\"1\":{\"25\":3}}],[\"selinux对每个角色可以执行什么操作进行了严格限制\",{\"1\":{\"3\":1}}],[\"selinux限制\",{\"1\":{\"3\":1}}],[\"seq\",{\"1\":{\"183\":1}}],[\"seq2seq\",{\"1\":{\"16\":1}}],[\"sequence\",{\"1\":{\"16\":2,\"46\":1}}],[\"c字符串通过遍历的方式来获取字符串长度\",{\"1\":{\"204\":1}}],[\"c=1\",{\"1\":{\"161\":1}}],[\"cms没有使用分代回收\",{\"1\":{\"116\":1}}],[\"cms更加关注于用户的体验\",{\"1\":{\"116\":1}}],[\"cms\",{\"0\":{\"116\":1},\"1\":{\"112\":1}}],[\"cross\",{\"1\":{\"77\":2}}],[\"createfreeform\",{\"1\":{\"3\":1}}],[\"createservice\",{\"1\":{\"3\":1}}],[\"current\",{\"1\":{\"77\":5}}],[\"censius\",{\"0\":{\"71\":1},\"1\":{\"71\":1}}],[\"c++方法的位置\",{\"1\":{\"125\":1}}],[\"c++\",{\"1\":{\"66\":1}}],[\"csv\",{\"1\":{\"66\":1}}],[\"cd自动化工具\",{\"1\":{\"60\":1}}],[\"cn\",{\"1\":{\"56\":2,\"60\":1,\"171\":1}}],[\"cnn也罢\",{\"1\":{\"16\":1}}],[\"cnn\",{\"1\":{\"16\":1}}],[\"check\",{\"1\":{\"196\":1}}],[\"chen\",{\"1\":{\"33\":2}}],[\"chunk\",{\"1\":{\"77\":6}}],[\"chunkwiseretention\",{\"1\":{\"77\":1}}],[\"chunkwise\",{\"1\":{\"74\":1}}],[\"choose\",{\"1\":{\"49\":1}}],[\"chat模型与chatgpt具有竞争力\",{\"1\":{\"54\":1}}],[\"chat模型与开源模型\",{\"1\":{\"54\":1}}],[\"chat模型在单回合和多回合提示上都显著优于开源模型\",{\"1\":{\"54\":1}}],[\"chat获得了超过60\",{\"1\":{\"53\":1}}],[\"chat输出在gpt\",{\"1\":{\"53\":1}}],[\"chat生成它\",{\"1\":{\"51\":1}}],[\"chat会忘记指示\",{\"1\":{\"51\":1}}],[\"chat进行拒绝采样\",{\"1\":{\"50\":1}}],[\"chat使用占总步数3\",{\"1\":{\"49\":1}}],[\"chat使用与基本模型相同的优化器参数\",{\"1\":{\"49\":1}}],[\"chat的胜率不那么明显\",{\"1\":{\"53\":1}}],[\"chat的性能\",{\"1\":{\"49\":1}}],[\"chat的最大学习率为5×10−6\",{\"1\":{\"49\":1}}],[\"chat的微调方法\",{\"1\":{\"37\":1}}],[\"chat在训练数据上训练一个epoch\",{\"1\":{\"49\":1}}],[\"chat基于llama\",{\"1\":{\"45\":1}}],[\"chat是基于llama\",{\"1\":{\"37\":1}}],[\"chat\",{\"1\":{\"37\":1,\"50\":3,\"51\":1,\"53\":1,\"54\":5}}],[\"chatglm\",{\"1\":{\"35\":1}}],[\"chatgpt和palm\",{\"1\":{\"54\":1}}],[\"chatgpt和llama\",{\"1\":{\"53\":1}}],[\"chatgpt是基于gpt\",{\"1\":{\"25\":1}}],[\"chatgpt的成功不是偶然的\",{\"1\":{\"13\":1}}],[\"chatgpt悄然进入大众的视线\",{\"1\":{\"13\":1}}],[\"chang\",{\"1\":{\"33\":1}}],[\"cloud是一个托管服务\",{\"1\":{\"64\":1}}],[\"cloud作为数据库\",{\"1\":{\"64\":1}}],[\"clipping\",{\"1\":{\"50\":1}}],[\"cls\",{\"1\":{\"32\":1}}],[\"class对象是一个对象\",{\"1\":{\"135\":1}}],[\"class<\",{\"1\":{\"3\":2}}],[\"classloader\",{\"1\":{\"3\":8}}],[\"class\",{\"1\":{\"3\":7,\"12\":5,\"105\":1,\"253\":1}}],[\"c\",{\"1\":{\"12\":7,\"66\":1,\"161\":1}}],[\"ctx\",{\"1\":{\"3\":2}}],[\"count\",{\"1\":{\"222\":2,\"229\":3,\"242\":2,\"244\":1}}],[\"counterpart\",{\"1\":{\"49\":1}}],[\"cortex扩展到docker\",{\"1\":{\"69\":1}}],[\"cortex允许您在生产环境中部署\",{\"1\":{\"69\":1}}],[\"cortex\",{\"0\":{\"69\":1},\"1\":{\"69\":1}}],[\"core\",{\"1\":{\"3\":1}}],[\"concurrent\",{\"0\":{\"116\":1}}],[\"connection\",{\"1\":{\"76\":1}}],[\"constitutional\",{\"1\":{\"51\":1}}],[\"constant\",{\"1\":{\"33\":1}}],[\"conference\",{\"1\":{\"33\":1}}],[\"control是一个开源的\",{\"1\":{\"67\":1}}],[\"control\",{\"0\":{\"67\":1}}],[\"continuous\",{\"1\":{\"33\":1}}],[\"contextimpl\",{\"1\":{\"3\":1}}],[\"context\",{\"1\":{\"3\":2}}],[\"colin\",{\"1\":{\"33\":1}}],[\"comet\",{\"0\":{\"62\":1},\"1\":{\"62\":2}}],[\"component\",{\"1\":{\"49\":1}}],[\"compose\",{\"1\":{\"2\":1}}],[\"completion\",{\"1\":{\"49\":1}}],[\"computational\",{\"1\":{\"33\":2}}],[\"comparable\",{\"1\":{\"33\":1}}],[\"com\",{\"1\":{\"1\":1,\"3\":2,\"35\":1,\"58\":3,\"59\":1,\"60\":1,\"61\":1,\"62\":1,\"66\":1,\"68\":3,\"70\":2,\"73\":1,\"133\":1}}],[\"cars\",{\"1\":{\"244\":3}}],[\"can\",{\"1\":{\"33\":1,\"49\":2}}],[\"cannot\",{\"1\":{\"1\":1}}],[\"cast\",{\"1\":{\"18\":2}}],[\"callback\",{\"1\":{\"3\":1}}],[\"cachedservicefetcher<infrarescanmanager>\",{\"1\":{\"3\":1}}],[\"cache\",{\"1\":{\"3\":3}}],[\"catch\",{\"1\":{\"3\":1}}],[\"2位置即可\",{\"1\":{\"246\":1}}],[\"2之后\",{\"1\":{\"209\":1}}],[\"2之前\",{\"1\":{\"209\":1}}],[\"2支持\",{\"1\":{\"201\":2}}],[\"268637051\",{\"1\":{\"133\":1}}],[\"23\",{\"1\":{\"105\":1}}],[\"28\",{\"1\":{\"49\":1}}],[\"2微调而来\",{\"1\":{\"45\":1}}],[\"2选择使用gqa而不是mqa\",{\"1\":{\"41\":1}}],[\"2模型\",{\"1\":{\"41\":1}}],[\"2万亿\",{\"1\":{\"38\":1}}],[\"2t\",{\"1\":{\"38\":1}}],[\"2的ppo剪辑阈值\",{\"1\":{\"50\":1}}],[\"2的训练数据来自混合后的公开数据\",{\"1\":{\"39\":1}}],[\"2的推理可扩展性\",{\"1\":{\"38\":1}}],[\"2的预训练特性如下\",{\"1\":{\"38\":1}}],[\"2的预训练方法\",{\"1\":{\"37\":1}}],[\"2的预训练和微调过程\",{\"1\":{\"36\":1}}],[\"2执行了更稳健的数据清理\",{\"1\":{\"38\":1}}],[\"2基本还是采用llama\",{\"1\":{\"38\":1}}],[\"2预训练模型微调得来的\",{\"1\":{\"37\":1}}],[\"2预训练的语料库增加了40\",{\"1\":{\"37\":1}}],[\"2和基于它的微调模型llama\",{\"1\":{\"37\":1}}],[\"2共包括两大版本\",{\"1\":{\"37\":1}}],[\"2106\",{\"1\":{\"33\":1}}],[\"2101\",{\"1\":{\"33\":1}}],[\"2103\",{\"1\":{\"33\":2}}],[\"21\",{\"1\":{\"33\":2}}],[\"2表示被masked的词在partb中的位置\",{\"1\":{\"28\":1}}],[\"24\",{\"1\":{\"20\":1}}],[\"2节中\",{\"1\":{\"18\":1}}],[\"2公开\",{\"1\":{\"14\":1}}],[\"200\",{\"1\":{\"251\":2}}],[\"2000\",{\"1\":{\"42\":1}}],[\"20\",{\"1\":{\"33\":1}}],[\"2016\",{\"1\":{\"33\":1}}],[\"2018a\",{\"1\":{\"33\":1}}],[\"2018年\",{\"1\":{\"14\":2}}],[\"2019\",{\"1\":{\"33\":3}}],[\"2019年\",{\"1\":{\"14\":2}}],[\"2017\",{\"1\":{\"33\":1}}],[\"2024\",{\"1\":{\"105\":1}}],[\"202206\",{\"1\":{\"235\":4}}],[\"2022\",{\"1\":{\"33\":1,\"235\":2}}],[\"2022年底\",{\"1\":{\"13\":1}}],[\"2021\",{\"1\":{\"33\":5}}],[\"2021年\",{\"1\":{\"14\":1}}],[\"2020\",{\"1\":{\"33\":4}}],[\"2023\",{\"1\":{\"1\":1}}],[\"2\",{\"0\":{\"16\":1,\"17\":2,\"18\":1,\"21\":1,\"25\":1,\"28\":1,\"32\":1,\"36\":1,\"38\":1,\"39\":1,\"40\":2,\"41\":1,\"44\":1,\"47\":1,\"48\":1,\"49\":2,\"50\":1,\"54\":1,\"56\":1,\"57\":2,\"58\":1,\"59\":1,\"60\":1,\"61\":1,\"63\":1,\"76\":1,\"77\":1,\"78\":2,\"79\":1,\"87\":1,\"88\":1,\"94\":1,\"95\":1,\"96\":2,\"99\":1,\"100\":1,\"103\":1,\"109\":1,\"114\":1,\"123\":1,\"127\":1,\"128\":1,\"129\":2,\"130\":1,\"131\":1,\"136\":1,\"137\":1,\"138\":2,\"139\":1,\"140\":1,\"148\":1,\"151\":1,\"158\":1,\"165\":1,\"173\":1,\"180\":1,\"184\":1,\"186\":1,\"187\":1,\"188\":2,\"199\":1,\"205\":1,\"207\":1,\"208\":1,\"209\":1,\"210\":2,\"211\":1,\"212\":1,\"213\":2,\"214\":1,\"215\":1,\"218\":1,\"222\":1,\"225\":1,\"229\":1,\"233\":1,\"234\":1,\"235\":1,\"236\":1,\"239\":1,\"242\":1,\"248\":1},\"1\":{\"3\":1,\"12\":1,\"13\":1,\"17\":2,\"18\":1,\"20\":1,\"21\":1,\"25\":2,\"26\":1,\"27\":2,\"28\":1,\"29\":1,\"32\":4,\"33\":2,\"37\":4,\"38\":1,\"45\":1,\"49\":6,\"50\":5,\"51\":3,\"53\":4,\"54\":7,\"77\":1,\"161\":1,\"213\":1,\"226\":7,\"235\":2,\"237\":1,\"240\":1,\"245\":8,\"247\":10,\"248\":17,\"249\":10,\"251\":9,\"253\":1}}],[\"27\",{\"1\":{\"1\":1,\"49\":1}}],[\"150\",{\"0\":{\"261\":1}}],[\"1表示签到\",{\"1\":{\"234\":1}}],[\"1设置\",{\"1\":{\"231\":1}}],[\"1个值\",{\"1\":{\"220\":1}}],[\"1也不是索引的内容\",{\"1\":{\"175\":1}}],[\"1​​\",{\"1\":{\"76\":1}}],[\"1节所示\",{\"1\":{\"49\":1}}],[\"1一致\",{\"1\":{\"46\":1}}],[\"1一样\",{\"1\":{\"43\":1}}],[\"1相同的标记器\",{\"1\":{\"43\":1}}],[\"1差异\",{\"1\":{\"40\":1}}],[\"1基本一致\",{\"1\":{\"40\":1}}],[\"1论文链接\",{\"1\":{\"38\":1}}],[\"1的权重衰减\",{\"1\":{\"50\":1}}],[\"1的权重衰减和1\",{\"1\":{\"42\":1}}],[\"1的变化内容介绍\",{\"0\":{\"41\":1}}],[\"1的训练方法\",{\"1\":{\"38\":1}}],[\"1的升级版本\",{\"1\":{\"37\":1}}],[\"1~\",{\"1\":{\"27\":1}}],[\"13b\",{\"1\":{\"37\":1}}],[\"130b进行全参数微调\",{\"1\":{\"30\":1,\"32\":1}}],[\"13\",{\"1\":{\"26\":1,\"33\":1}}],[\"1750亿\",{\"1\":{\"25\":1}}],[\"16bit\",{\"1\":{\"183\":2}}],[\"161641400\",{\"1\":{\"59\":1}}],[\"16\",{\"1\":{\"20\":1}}],[\"128\",{\"1\":{\"228\":1}}],[\"12=600个参数需要微调\",{\"1\":{\"32\":1}}],[\"12\",{\"1\":{\"20\":2,\"26\":1,\"33\":1,\"236\":1,\"237\":1}}],[\"116\",{\"1\":{\"244\":4}}],[\"111000102\",{\"1\":{\"213\":2}}],[\"119\",{\"1\":{\"33\":1}}],[\"11\",{\"1\":{\"16\":1,\"33\":1}}],[\"1公开\",{\"1\":{\"14\":1}}],[\"1\",{\"0\":{\"16\":1,\"20\":1,\"24\":1,\"27\":1,\"31\":1,\"37\":1,\"39\":1,\"41\":1,\"46\":1,\"48\":1,\"53\":1,\"56\":1,\"62\":1,\"72\":1,\"73\":1,\"75\":1,\"77\":1,\"86\":1,\"88\":1,\"95\":1,\"102\":1,\"108\":1,\"113\":1,\"120\":1,\"122\":2,\"123\":1,\"124\":1,\"125\":1,\"126\":1,\"128\":1,\"132\":1,\"133\":1,\"134\":2,\"135\":3,\"136\":1,\"137\":2,\"138\":1,\"139\":1,\"140\":1,\"141\":1,\"142\":1,\"143\":1,\"144\":1,\"145\":1,\"147\":1,\"150\":1,\"157\":1,\"161\":1,\"164\":1,\"172\":1,\"182\":1,\"183\":2,\"184\":1,\"185\":1,\"187\":1,\"189\":1,\"202\":1,\"203\":2,\"204\":3,\"205\":2,\"207\":1,\"209\":1,\"212\":1,\"217\":1,\"221\":1,\"224\":1,\"228\":1,\"232\":1,\"235\":1,\"238\":1,\"241\":1,\"244\":1,\"250\":1},\"1\":{\"3\":1,\"12\":1,\"13\":1,\"16\":2,\"18\":1,\"20\":3,\"24\":1,\"27\":4,\"28\":1,\"29\":1,\"32\":5,\"33\":2,\"37\":1,\"46\":1,\"50\":5,\"74\":1,\"75\":2,\"76\":1,\"77\":6,\"78\":1,\"79\":1,\"121\":3,\"150\":1,\"161\":1,\"175\":1,\"204\":1,\"208\":1,\"213\":1,\"226\":9,\"233\":3,\"235\":3,\"236\":4,\"245\":6,\"246\":1,\"247\":11,\"248\":5,\"249\":18,\"251\":16,\"253\":6}}],[\"140\",{\"1\":{\"33\":1}}],[\"14\",{\"1\":{\"1\":1}}],[\"10<sup>9<\",{\"1\":{\"245\":2,\"251\":2}}],[\"10<sup>4<\",{\"1\":{\"245\":1,\"247\":3,\"249\":3}}],[\"10^8\",{\"1\":{\"236\":1}}],[\"100\",{\"1\":{\"50\":1,\"235\":6,\"248\":2}}],[\"10385\",{\"1\":{\"33\":1}}],[\"10360\",{\"1\":{\"33\":1}}],[\"1024行\",{\"1\":{\"49\":1}}],[\"1024\",{\"1\":{\"20\":1,\"236\":2}}],[\"10\",{\"0\":{\"71\":1},\"1\":{\"1\":2,\"14\":1,\"22\":2,\"27\":1,\"30\":1,\"33\":1,\"244\":1}}],[\">=\",{\"1\":{\"161\":1}}],[\">根据类模板对象来创建class对象\",{\"1\":{\"134\":1}}],[\">\",{\"1\":{\"3\":2,\"51\":1,\"103\":1,\"161\":2,\"213\":1,\"226\":6,\"253\":4}}],[\">sadd\",{\"1\":{\"226\":1}}],[\">settings\",{\"1\":{\"1\":1}}],[\">show\",{\"1\":{\"1\":1}}],[\">file\",{\"1\":{\"1\":1}}],[\">help\",{\"1\":{\"1\":1}}],[\"aof机制的rewrite模式\",{\"1\":{\"196\":1}}],[\"aof工具解决数据一致性问题\",{\"1\":{\"196\":1}}],[\"aof可以设置appendsync属性为always\",{\"1\":{\"196\":1}}],[\"aof\",{\"1\":{\"196\":1}}],[\"aosp和部分国产rom\",{\"1\":{\"3\":1}}],[\"a=1\",{\"1\":{\"161\":1}}],[\"a=λ\",{\"1\":{\"77\":1}}],[\"a<200\",{\"1\":{\"150\":1}}],[\"author\",{\"1\":{\"105\":1}}],[\"automl\",{\"1\":{\"73\":1}}],[\"autokeras是一个用于自动机器学习\",{\"1\":{\"73\":1}}],[\"autokeras\",{\"0\":{\"73\":1},\"1\":{\"73\":1}}],[\"autoregressive\",{\"1\":{\"27\":1}}],[\"autoregression\",{\"1\":{\"19\":2}}],[\"autoencoder\",{\"1\":{\"19\":1}}],[\"aka\",{\"1\":{\"74\":1}}],[\"azure\",{\"0\":{\"60\":1,\"61\":1},\"1\":{\"60\":2}}],[\"api管理机器学习实验和模型元数据\",{\"1\":{\"59\":1}}],[\"apache\",{\"1\":{\"57\":1}}],[\"append\",{\"1\":{\"196\":1}}],[\"appendix\",{\"1\":{\"49\":1}}],[\"approach\",{\"1\":{\"33\":1}}],[\"apptoken\",{\"1\":{\"3\":1}}],[\"app\",{\"1\":{\"3\":2}}],[\"abstractive\",{\"1\":{\"50\":1}}],[\"ablation\",{\"1\":{\"49\":1}}],[\"ack=j+1报文\",{\"1\":{\"184\":1}}],[\"ack标志位为1\",{\"1\":{\"184\":1}}],[\"ack等\",{\"1\":{\"183\":1}}],[\"ack\",{\"1\":{\"183\":1}}],[\"act\",{\"1\":{\"51\":1}}],[\"accuracy\",{\"1\":{\"49\":1}}],[\"across\",{\"1\":{\"33\":1}}],[\"an\",{\"1\":{\"51\":1}}],[\"analysis\",{\"1\":{\"49\":1}}],[\"annotators\",{\"1\":{\"49\":1}}],[\"annual\",{\"1\":{\"33\":1}}],[\"ankur\",{\"1\":{\"33\":1}}],[\"and\",{\"1\":{\"12\":4,\"33\":13,\"49\":4,\"55\":1,\"76\":1,\"150\":1,\"161\":3,\"233\":1,\"236\":3}}],[\"android下\",{\"1\":{\"3\":1}}],[\"androidx\",{\"1\":{\"2\":1}}],[\"android\",{\"0\":{\"0\":1,\"254\":1},\"1\":{\"1\":1,\"3\":6}}],[\"aliyun\",{\"1\":{\"58\":1}}],[\"al\",{\"1\":{\"33\":2}}],[\"alec\",{\"1\":{\"33\":1}}],[\"allen\",{\"1\":{\"33\":1}}],[\"all\",{\"1\":{\"33\":2}}],[\"allowblocking\",{\"1\":{\"3\":1}}],[\"allow\",{\"1\":{\"3\":2}}],[\"airflow是一种任务和工作流程编排工具\",{\"1\":{\"57\":1}}],[\"airflow\",{\"0\":{\"57\":1},\"1\":{\"57\":1}}],[\"ai\",{\"0\":{\"71\":1},\"1\":{\"50\":2,\"51\":2,\"63\":1,\"71\":1,\"72\":2}}],[\"aidan\",{\"1\":{\"33\":1}}],[\"ai的能力在逐步增强\",{\"1\":{\"25\":1}}],[\"attention和multi\",{\"1\":{\"25\":1}}],[\"attention和上述的一致\",{\"1\":{\"18\":1}}],[\"attention的k\",{\"1\":{\"18\":1}}],[\"attention是双向的\",{\"1\":{\"18\":1}}],[\"attention\",{\"1\":{\"16\":1,\"17\":1,\"18\":4,\"25\":2,\"33\":2,\"41\":1,\"45\":1,\"51\":1,\"75\":1}}],[\"attempting\",{\"1\":{\"1\":1}}],[\"a2\",{\"1\":{\"12\":4}}],[\"a100\",{\"1\":{\"32\":2}}],[\"a100服务器\",{\"1\":{\"30\":1}}],[\"a1\",{\"1\":{\"12\":4,\"51\":1}}],[\"arraycopy\",{\"0\":{\"250\":1,\"253\":1},\"1\":{\"252\":1,\"253\":3}}],[\"arraymap<>\",{\"1\":{\"3\":1}}],[\"article\",{\"1\":{\"58\":1,\"71\":1}}],[\"architecture这篇论文中介绍了11个mlops相关工具\",{\"1\":{\"55\":1}}],[\"arxiv\",{\"1\":{\"33\":8}}],[\"are\",{\"1\":{\"33\":1,\"49\":1}}],[\"args\",{\"1\":{\"12\":1,\"105\":1}}],[\"a是继承者称之为子类或者派生类\",{\"1\":{\"7\":1}}],[\"a是b\",{\"1\":{\"7\":1}}],[\"a\",{\"0\":{\"41\":1},\"1\":{\"7\":1,\"12\":11,\"27\":1,\"33\":4,\"49\":4,\"103\":2,\"105\":1,\"161\":4,\"174\":3,\"175\":3}}],[\"asc\",{\"1\":{\"244\":1}}],[\"asc|desc\",{\"1\":{\"242\":1}}],[\"association\",{\"1\":{\"33\":2}}],[\"assert\",{\"1\":{\"3\":1,\"247\":2}}],[\"ashish\",{\"1\":{\"33\":1}}],[\"asinterface\",{\"1\":{\"3\":2}}],[\"as\",{\"1\":{\"3\":1,\"51\":1}}],[\"adversarial\",{\"1\":{\"50\":1}}],[\"advances\",{\"1\":{\"33\":1}}],[\"adaptation\",{\"1\":{\"33\":1}}],[\"adamw优化器\",{\"1\":{\"42\":1}}],[\"adam\",{\"1\":{\"33\":1}}],[\"addfirst\",{\"1\":{\"103\":1}}],[\"addlistener\",{\"1\":{\"3\":1}}],[\"adddexpath\",{\"1\":{\"3\":1}}],[\"add\",{\"1\":{\"3\":4,\"18\":1}}],[\"addservice\",{\"1\":{\"3\":2}}],[\"adb\",{\"1\":{\"1\":2}}],[\"adb异常重启问题解决\",{\"0\":{\"0\":1}}]],\"serializationVersion\":2}";