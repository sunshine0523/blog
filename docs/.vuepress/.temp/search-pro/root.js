export default "{\"documentCount\":42,\"nextId\":42,\"documentIds\":{\"0\":\"v-35b84e56\",\"1\":\"v-35b84e56#一、主要研究点\",\"2\":\"v-35b84e56#二、transformer\",\"3\":\"v-35b84e56#_2-1-序列转换模型\",\"4\":\"v-35b84e56#_2-2-注意力机制\",\"5\":\"v-35b84e56#_2-3-transformer模型\",\"6\":\"v-35b84e56#三、bert\",\"7\":\"v-35b84e56#_3-1-bert与transformer\",\"8\":\"v-35b84e56#_3-2-input-embedding\",\"9\":\"v-35b84e56#_3-3-预训练bert\",\"10\":\"v-35b84e56#四、t5和gpt\",\"11\":\"v-35b84e56#_4-1-t5模型\",\"12\":\"v-35b84e56#_4-2-gpt模型\",\"13\":\"v-35b84e56#五、glm\",\"14\":\"v-35b84e56#_5-1-自回归填空\",\"15\":\"v-35b84e56#_5-2-二维位置编码\",\"16\":\"v-35b84e56#_5-3-glm与transformer\",\"17\":\"v-35b84e56#六、p-tuning-v2\",\"18\":\"v-35b84e56#_6-1-提示微调\",\"19\":\"v-35b84e56#_6-2-p-tuning-v2\",\"20\":\"v-35b84e56#参考文献\",\"21\":\"v-d1b8fcd8\",\"22\":\"v-d1b8fcd8#_1-介绍\",\"23\":\"v-d1b8fcd8#_2-预训练方法\",\"24\":\"v-d1b8fcd8#_2-1-训练数据\",\"25\":\"v-d1b8fcd8#_2-2-训练详情\",\"26\":\"v-d1b8fcd8#a-2-1-额外的预训练信息之与llama-1的变化内容介绍-p-46\",\"27\":\"v-d1b8fcd8#预训练超参设置\",\"28\":\"v-d1b8fcd8#预训练tokenizer\",\"29\":\"v-d1b8fcd8#_2-3-评测\",\"30\":\"v-d1b8fcd8#_3-fine-tuning\",\"31\":\"v-d1b8fcd8#_3-1-supervised-fine-tuning-sft\",\"32\":\"v-d1b8fcd8#_3-2-rlhf\",\"33\":\"v-d1b8fcd8#_3-2-1-人类偏好数据收集\",\"34\":\"v-d1b8fcd8#_3-2-2-奖励建模\",\"35\":\"v-d1b8fcd8#_3-2-3-迭代微调\",\"36\":\"v-d1b8fcd8#_3-3-多轮一致性的指令\",\"37\":\"v-d1b8fcd8#_3-4-rlhf结果\",\"38\":\"v-d1b8fcd8#_3-4-1-基于模型的评估\",\"39\":\"v-d1b8fcd8#_3-4-2-人类评价\",\"40\":\"v-743a817b\",\"41\":\"v-8daa1a0e\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[3,33],\"1\":[2,85],\"2\":[2],\"3\":[3,38],\"4\":[2,41],\"5\":[3,169],\"6\":[2,39],\"7\":[3,54],\"8\":[4,27],\"9\":[2,76],\"10\":[2,5],\"11\":[3,35],\"12\":[3,54],\"13\":[2,43],\"14\":[3,104],\"15\":[3,30],\"16\":[3,28],\"17\":[4,49],\"18\":[3,27],\"19\":[5,90],\"20\":[1,303],\"21\":[3,5],\"22\":[2,35],\"23\":[2,25],\"24\":[3,5],\"25\":[2,18],\"26\":[7,46],\"27\":[1,15],\"28\":[1,13],\"29\":[3,2],\"30\":[3,11],\"31\":[7,43],\"32\":[3],\"33\":[4,20],\"34\":[3,145],\"35\":[3,167],\"36\":[2,99],\"37\":[3],\"38\":[4,28],\"39\":[4,44],\"40\":[2],\"41\":[1]},\"averageFieldLength\":[2.880952380952382,55.416427364864866],\"storedFields\":{\"0\":{\"h\":\"从BERT到GLM，NLP经历了什么？\",\"t\":[\"2022年底，ChatGPT悄然进入大众的视线，受到了业内人士和广大群众的注意。ChatGPT的成功不是偶然的，是其总结前人经验、反复打磨多年才得以形成的。本篇文章将顺着现代自然语言处理方法和模型的脉络，即Transformer[1]、BERT[2]、T5[3]、GPT[4]、GLM[5]和P-Tuning v2[6]几个方面来介绍。其中Transformer是一种全新的序列转换模型，BERT、T5、GPT和GLM均为预训练语言模型，P-Tuning v2是一种对预训练语言模型进行高效微调的方法。通过以上几个部分，本篇文章对现代语言模型的学习的全过程：即结构、训练和微调均进行了介绍。其中因为内容相对重复，本篇文章对T5和GPT进行简要介绍。\"]},\"1\":{\"h\":\"一、主要研究点\",\"t\":[\"在ChatGPT大火之后，各种语言模型层出不穷，比如百度的文心、讯飞的星火等。但是语言模型的研究并不是最近才兴起的。早在2017年，Google Brain就发布了一个全新的序列转换模型—Transformer，后续的语言模型，基本上都与Transformer有着千丝万缕的联系。2018年，Google公开了以Transformer作为基础的语言模型BERT，轰动一时。BERT的基础模型有110M参数，在当年属于标准大小，但是其自然语言理解能力非常强。2019年，Google公开了T5模型，该模型号称是“全能模型”，即所有的自然语言理解任务都可划分为“文本到文本”的任务，T5基础模型参数量为220M，但是最大的T5模型达到了11B，是BERT_base的100倍大小。这绝对可以称之为“大模型”了。\",\"BERT和T5都是Google的精彩操作，而另一边的OpenAI也不甘落后，2018年， GPT-1公开，其参数量有117M；2019年，GPT-2公开，其参数量有1.5B；而2020年，GPT-3公布，其参数量已经达到了175B。\",\"放眼国内，清华大学在语言模型上研究较早。2021年，清华大学语言模型GLM发布，意为通用语言模型(General Language Model)。目前，最大的GLM参数量已经达到了130B。\",\"基本已经可以确定的是，小模型（低于10B参数）的能力不是很强，所以语言模型方向模型参数规模越来越大，百亿、千亿、万亿模型都不足为奇。然而，抛开预训练不谈，在如此大的规模下，大部分个人和团队都已经没有能力去做模型的全参数微调了。因此，国内外研究者先后提出了P-Tuning[7]、Prefix-Tuning[8]、P-Tuning v2[6]、Prompt Tuning[9]和LoRA[10]等部分参数微调方法。本篇文章选择了清华大学的P-Tuning v2，该方法微调效果可以与全参数微调媲美，但是微调参数量仅为全参数微调的3%左右。\",\"本篇文章会对上述模型和技术进行简要介绍，该脉络基本涉及了现代语言模型学习的全过程。\"]},\"2\":{\"h\":\"二、Transformer\"},\"3\":{\"h\":\"2.1 序列转换模型\",\"t\":[\"一个序列转换(Sequence-to-Sequence, Seq2Seq)模型一般包括一个编码器(Encoder)和一个解码器(Decoder)，如图2-1。序列转换模型并不是Transformer首次提出的。在Transformer之前，序列转换模型一般由循环神经网络(RNN)或卷积神经网络(CNN)作为编码器和解码器。然而无论是RNN也好，CNN也罢，它们都足够复杂，导致序列转换模型的效率不高。后来，有的学者尝试将注意力机制(Attention Mechanism)引入序列转换模型[11]，让序列转换模型的效率得以一定的提升。不过它们仍然没有脱离RNN或者CNN。\",\"图2-1 序列转换模型\",\"而Transformer，彻底抛弃了复杂的RNN和CNN，只依赖于注意力机制，这也是Transformer成功的关键。在介绍Transformer之前，我们首先讨论一下注意力机制。\"]},\"4\":{\"h\":\"2.2 注意力机制\",\"t\":[\"注意力机制(Attention Mechanism)是人们在机器学习模型中嵌入的一种特殊结构，用来自动学习和计算输入数据对输出数据的贡献大小。在注意力机制中，我们往往会讨论Q、K、V，它们分别代表Query、Key、Value。注意力机制就是给定一个Query，经过一系列的Key来获取Value，从而得到Attention Score。如图2-2。\",\"图2-2 注意力机制\",\"实际上，计算Attention Score的过程如下：首先，由Query和Key做向量比对，得到Query和Key的相似度，然后归一化相似度，并用相似度与Key所对应的Value做矩阵运算并求和，得到Attention Score。注意力机制公式如下：\",\"下面我们来介绍自注意力机制。自注意力机制是注意力机制的一种。在自注意力机制中，注意力集中在上述公式中Source的内部元素，如图2-3。而在计算方式上，与传统注意力机制完全相同。在Transformer中，自注意力机制得到了应用，因为Transformer需要判断序列中词与词之间的关系强度，自注意力机制正符合这一点。\",\"图2-3 自注意力机制示例\"]},\"5\":{\"h\":\"2.3 Transformer模型\",\"t\":[\"Transformer是Seq2Seq的全新尝试，其抛弃了RNN和CNN作为Encoder和Decoder，采用了注意力机制，提高了Seq2Seq的效率。\",\"图2‑4 Transformer模型\",\"Transformer模型整体结构如图2-4。其结构可以分为输入输出嵌入向量、位置编码、Encoder模块、Decoder模块等。\",\"对输入、输出进行向量化(Embedding)已经是广为应用的做法，其比独热编码(One-Hot)拥有更加优秀的能力，这里不再赘述。下面我们讨论位置编码。\",\"位置编码 。因为Transformer抛弃了RNN和CNN，这样，如果不经过特殊处理，Transformer没有办法表示序列的顺序。但是，序列的顺序中往往蕴含着一些重要信息，比如：\",\"I do not like the story of the movie, but I do like the cast.\",\"I do like the story of the movie, but I do not like the cast.\",\"上述两个句子序列的词完全相同，只不过是某些词的顺序不同。如果不考虑词在序列中的位置，那么Encoder会认为这两个序列完全相同。因此，选择一种合适的方式表示词在序列中的顺序非常重要。\",\"一个好的位置编码方案需要满足以下几个条件：1.它能为每个时间步输出一个独一无二的编码；2.不同长度的句子之间，任何两个时间步之间的距离应该保持一致；3.模型应该能毫不费力地泛化更长的句子，它的值应该是有界的；4.它必须是确定性的。\",\"位置编码可以通过训练得到，也可以通过公式计算得到。Transformer中的位置编码采用公式计算得到，公式如下：\",\"Transformer的位置编码简单但是有创新性。该编码不是一个单一的数值，而是包含句子中特定位置信息的d维向量（d_model即隐层维数）。此外，该编码没有整合进模型，而是用这个向量让每个词具有它在句子序列中的位置信息，即通过注入词的顺序信息来增强模型的输入。最后，采用三角函数来作为位置编码公式，对于相对位置的计算更加方便，因为三角函数具有周期性。\",\"Encoder 。Transformer的编码器结构如图2-5。可以看到，Encoder部分由N个Encoder单元构成。在Transformer中，N=6。一个Encoder单元，由一个多头注意力机制(Multi-Head Attention)和一个前馈网络(Feed Forward)构成。在多头注意力机制和前馈网络完成后，会计算残差和(Add)并正规化(Norm)。首先，我们来讨论多头注意力机制。\",\"图2-5 Encoder结构图\",\"在2.2节中，我们对注意力机制有了初步了解。在这里，我们进一步讨论Transformer中应用的注意力机制。Transformer中对注意力机制的体现在多头注意力机制。而多头注意力机制是由缩放点积注意力机制(Scaled Dot-Product Attention)构成，它是注意力机制的一种，其计算过程与注意力机制一致，其计算公式如下：\",\"缩放点积注意力机制在做完Query和Key的点积之后，会进行一个缩放，即除以d的开方。之所以要缩放，是因为对于输入的d大值，会导致Query和Key的点积非常大，这样会导致SoftMax产生非常小的值，为了抵消这个效果，缩放点击注意力机制会进行一个缩放。\",\"多头注意力机制如图2-6。Transformer认为，将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息。多头注意力机制就是将缩放点积注意力机制的过程做h次，再把输出合并起来。多头注意力机制的公式如下：\",\"图2-6 多头注意力机制\",\"Encoder单元中的另一个部分是一个前馈网络。Transformer在这里设计的前馈网络比较简单，为一个两层的多层感知机，第一层有一个ReLU激活函数，第二层为一个线性变换。公式如下：\",\"Decoder 。Transformer的Decoder和Encoder十分相似。Decoder中Decoder单元数N=6，与Encoder一致。Decoder和Encoder最大的区别，是Decoder单元中多了一层Masked Multi-Head Attention。\",\"什么是Masked Multi-Head Attention？其实Multi-Head Attention和上述的一致。但是单纯的Multi-Head Attention是双向的，也就是某个词既可以看到它之后的词，也可以看到它之前的词。\",\"但是在解码阶段，模型需要做的是通过已经有的信息来预测下一个位置会出现什么，如果此时模型知道了某个词之后的信息，模型就失去了“预测”，相当于看到了未来的信息。这是我们不希望发生的。所以，在解码阶段，我们希望自注意力机制是“单向”的，所以这里就用了Masked自注意力机制，组织模型看到将要预测的信息。\",\"Decoder中的Multi-Head Attention的K V是Encoder的输出计算的。\",\"至此，Transformer的模型结构已经介绍完成。Transformer对后来语言模型的影响十分深远，后续的语言模型，基本上都采用了Transformer或者基于Transformer修改的模型。\"]},\"6\":{\"h\":\"三、BERT\",\"t\":[\"Transformer是一个Seq2Seq模型，但并不是一个实际的语言模型。而首个将Transformer应用到实际语言模型中的，正是BERT。BERT全称Bidirectional Encoder Representations from Transformers。从全称中可以看出，BERT是一个双向编码模型，并且和Transformer有关。下面我们来介绍一下BERT。\",\"我们知道，GPT是一个自回归(Autoregression)语言模型，即，当前的Token只能看到它和它之前的Token，而不能看到它之后的Token。BERT在当时的条件下认为，这限制了预训练，特别是微调的能力。模型应该是双向的才好。因此，BERT诞生了，一个双向编码语言模型。\",\"当然，在现在看来，我们无法评价自回归(Autoregression)模型和自编码(Autoencoder)模型，或者说单向模型和双向模型谁好谁坏，它们各有优缺点。比如，自回归语言模型更加适合自然语言生成任务，而自编码模型更加适合自然语言理解任务。\"]},\"7\":{\"h\":\"3.1 BERT与Transformer\",\"t\":[\"BERT并没有把Transformer拿来直接用，而是只用到了Encoder部分。如图3-1，为BERT模型的结构图，其中蓝色阴影部分，就是Transformer的Encoder部分。可以看出，Transformer的Encoder模块，也是BERT的核心。\",\"图3-1 BERT模型结构图\",\"BERT只用了Transformer的Encoder部分，并且相对Transformer来说，BERT对Encoder进行了一些修改。其主要的修改如下：1.在Transformer的介绍中我们提到过，Transformer的Encoder层是由N=6的单元构成的。在BERT中，BERT_base N = 12 BERT_large N = 24。2.在Transformer中，注意力机制体现在模型中是多头注意力机制，Transformer中多头注意力机制是由h = 8，即8个缩放点积注意力叠加而成。在BERT中，BERT_base h = 12，BERT_large h = 16。3.在Transformer中，d = 512，在BERT中，BERT_base d = 768，BERT_large d = 1024。这个d其实就是模型最大能接受的Token长度。4.Embedding部分有了一些调整，多了一个Segment Embedding，Positional Embedding也有调整。我们将在下面介绍这一部分。\",\"这样看来，BERT_base的参数规模是110M，BERT_large的参数规模是340M。虽然把这个模型放到现在看起来规模不大，但是在当时，大家的参数量还没有那么夸张。\"]},\"8\":{\"h\":\"3.2 Input Embedding\",\"t\":[\"BERT相对Transformer来说，对Input Embedding部分做了一些修改，增加了一个Segment Embedding。这个是什么呢？由于BERT的主要目的是构建一个通用的预训练模型，因此难免需要兼顾到各种NLP任务场景下的输入。因此Segment Embedding的作用便是用来区分输入序列中的不同序列，其本质就是通过一个普通的词嵌入来区分每一个序列所处的位置。例如在NSP任务中，那么对于任意一个序列的每一位置都将用同一个向量来进行表示，即此时Segment词表的长度为2。\",\"此外，BERT对Positional Embedding也有调整。Transformer中的位置编码，是由三角函数计算出来的，而BERT的位置编码，是训练出来的。\",\"最终的Input Embedding，是这三个嵌入式张量的和：\",\"图3-2 BERT Input Embedding\"]},\"9\":{\"h\":\"3.3 预训练BERT\",\"t\":[\"BERT的预训练没有采用传统的自左向右或者自右向左语言模型来训练BERT，而是采用的MLM，即Masked Language Model。那么具体是怎么做的呢？\",\"MLM会随机地遮住输入的某些Token，比如：\",\"Input: 今天天气真好呀！\",\"MLM: 今天天气真[MASK]呀！\",\"遮住之后，MLM的要做的事就是根据上下文来预测被遮住的Token应该是什么，是“好”，还是“坏”？这些都是根据上下文，来计算概率的。也就是在这里，可以体现出BERT是双向模型，因为这里的上下文，既包括Token左边的，也包括Token右边的。MLM根据整个句子信息来推断被遮住的Token。\",\"根据经验，一般会MASK掉句子的15%的Token来进行训练，效果比较好（BERT的论文中没有提为什么是15%，T5论文中有对比实验，证明了15%是效果最好的）。不过，如果这15%全部把输入的Token替换成[MASK]，可能会有一个问题：这会造成预训练和微调之间产生一个不匹配的情况，因为在微调的过程中，[MASK]并不会出现，这样预测的概率可能不准确。为了解决这个问题，BERT把这15%中的80%用[MASK]替换，10%不变，10%随机替换为其他Token。\",\"接下来我们来介绍BERT中另外一个部分，Next Sentence Prediction。很多下游任务，比如问题回答、自然语言推理等，都要基于多个句子之间的关系，这个关系是没有办法被语言模型直接捕获到的。为了解决这个问题，BERT在预训练中加入了NSP。NSP是一个二分类下句预测任务。具体地，对于每个样本来说都是由A和B两句话构成，其中的情况B确实为A的下一句话（标签为IsNext），另外的的情况是B为语料中其它的随机句子（标签为NotNext），然后模型来预测B是否为A的下一句话。NSP的位置在BERT模型图中有所体现。\",\"在实验中，BERT的效果遥遥领先于同期其他语言模型，取得了喜人的成绩。这里我们不做过多介绍。\"]},\"10\":{\"h\":\"四、T5和GPT\",\"t\":[\"T5，特别是GPT，其能力大家有目共睹。本篇文章中将简单对其进行介绍。\"]},\"11\":{\"h\":\"4.1 T5模型\",\"t\":[\"T5，是Transfer Text-to-Text Transformer的简写。Transfer来自Transfer Learning，预训练模型基本上属于这个范畴，Transformer即我们在第二节中提到的，那么什么是Text-to-Text？它是T5提出的一个统一框架，用于将所有的自然语言处理(NLP)任务都转化为文本到文本(Text-to-Text)任务。\",\"图4‑1 T5模型\",\"比如，自然语言处理中常见的翻译任务，在T5模型中，只需要在给模型输入的部分加上前缀：“给我从英语翻译成汉语”，然后再加上要翻译的内容即可。通过这样的方式，就可以将NLP任务都转成Text-to-Text的形式，这样，就可以用同样的模型、同样的损失函数、同样的训练过程、同样的解码过程来完成所有的NLP任务。本文对T5模型的介绍就到这里，更多内容可以阅读原论文。\"]},\"12\":{\"h\":\"4.2 GPT模型\",\"t\":[\"说到GPT(Generative Pre-Training)语言模型，大家首先想起的一定是ChatGPT。ChatGPT是基于GPT-3.5的对话聊天机器人，其能力大家有目共睹。其成功的关键在于超大的参数规模(1750亿)和超多的预训练语料，这是普通公司和个人难以承受的。本篇文章将回到最初的GPT，来讨论GPT的基本结构。\",\"GPT模型与BERT模型不同。BERT模型是自编码模型，而GPT模型是自回归模型。自回归模型对自然语言生成有着天然的优势。不过与BERT、T5模型相同，GPT同样也抛弃了传统的RNN和CNN，转而采用Transformer结构。与BERT不同的是，GPT采用的不是Transformer的Encoder部分，而是其Decoder部分。\",\"GPT的核心部分是N=12的Transformer Decoder结构，如图4-2。\",\"图4-2 GPT模型结构\",\"同样，GPT也并没有将Transformer的Decoder拿过来直接用，而是做了一些修改。Transformer的Decoder结构中，包含了Masked Multi Self Attention和Multi Self Attention，在GPT中，只保留了Masked Multi Self Attention。其余基本没有变化。\",\"如今，GPT已经迎来了第四个大版本，GPT-4。其参数规模进一步增大，能力进一步增强，更是拥有了理解图像的能力。AI的能力在逐步增强。\"]},\"13\":{\"h\":\"五、GLM\",\"t\":[\"在国内，清华大学发布的GLM(General Language Model)应该是效果比较不错的一个模型。当然，其优良的效果源于其创新的思想和持续的研究。\",\"GLM意为通用语言模型，其通用性体现在哪里？我们知道，预训练语言模型可以分为三种：自回归模型(e.g. GPT[4])、自编码模型(e.g. BERT[2])、编码-解码模型(e.g. RoRERTa[12])。它们各有各的擅长之处。比如自回归模型擅长自然语言生成任务，而自编码模型擅长自然语言理解任务。在GLM之前，也有研究人员尝试将上述三种模型结合[13]，以胜任多种自然语言处理任务，不过因为自回归和自编码在模型结构上相差太多，所以效果不是很好（在现在看，GPT似乎有能力处理自然语言理解和自然语言生成任务，不过在当时的条件下并没有很出色的能力）。\",\"同样，GLM也尝试能够同时处理自然语言理解和自然语言生成等多种NLP任务。不同于之前研究的简单结合，GLM创新地应用了自回归填空思想。下面，我们来介绍GLM中的自回归填空思想。\"]},\"14\":{\"h\":\"5.1 自回归填空\",\"t\":[\"GLM模型应用了名为自回归填空(Autoregressive Blank Infilling)的思想。我们给定一个文本序列[x~1~, …, x ~n~ ]，在其中采用多个文本域{s ~1~ , …, s ~m~ }，其中每个文本域s~i ~都对应x中的连续Token [s ~i,1~ ,…,s ~i,li~ ]，而每一个文本域都会被一个单独的Token [MASK]所替代。也许通过文字描述比较难以理解。我们可以看图5-1：\",\"图5-1 GLM自回归填空示意图之一\",\"在图5-1(a)中，可以看到我们给定的文本序列为[x~1~, …, x~6~]，在文本序列中采样的文本域为{s ~1~ , s ~2~ }，其中s~1~对应着[x ~3~ ]，s~2~对应着[x ~5~ , x ~6~ ]（图中含有色块部分）。在图5-1(b)中可以看到，GLM把s~1~和s~2~对应的x部分替换为一个[MASK] Token，而与BERT等Masked不同，GLM没有选择直接丢失这些x，而是将其放到了Part B部分。\",\"GLM随机Masked掉一些文本，其实在BERT中也是这样做的，我们称之为Masked Language Model(MLM)。只不过，在BERT中，一般Masked掉的是一个词，而GLM中可能会Masked掉连续的多个字。个人猜测，可能是因为在中文中，一个字可能意义不如多个字组成的意义大（如“玩”和“玩笑”可能差别很大），所以Masked掉多个字，可能效果会更好一些。GLM随机Masked掉的比例为15%，沿用了BERT和T5的Masked的比例，这个比例在T5模型的论文中证明，为效果最好的。\",\"对于Masked掉的词，GLM采用自回归的方式尝试还原它们，即“自回归填空”，公式如下：\",\"其中x~corrupt~就是partA，即带MASK部分的句子，s~z<i~指的是partB的部分，不过看到它只用了z<i的部分，也就是单向的，即自回归的。不过考虑到span之间可能也有关系，所以s~z~的顺序是随机打乱的。如图5-2。\",\"在这里也可以看出，GLM预测的条件比BERT多了一个partB。在BERT中，MASK掉的15%的输入，其中只有80%被[MASK]替代，而另外10%不变，10%随机变为其他Token，BERT通过这种方式来保留一些被Masked的原始信息。但是GLM没有这样做，GLM把Masked掉的信息全部保留在了partB，这样可以进一步提高预测能力。不过partA是看不到partB的，partB可以自回归地看到已经走过的partB和全部的partA。\",\"图5-2 GLM自回归填空示意图二\"]},\"15\":{\"h\":\"5.2 二维位置编码\",\"t\":[\"在Transformer中，位置编码采用了三角函数计算的方式得到；在BERT中，位置编码采用了预训练的方式训练而得。GLM仍然是以Transformer为基础的结构，自然也没有原生的表示位置信息的能力，所以，也只能够通过位置编码的形式来获取位置信息。\",\"与Transformer和BERT的一维位置编码不同，GLM采用了二维位置编码。\",\"通过上一节的介绍我们知道，GLM把输入的文本分为两个部分：partA和partB。所谓二维编码，即对partA部分和partB部分都进行编码。如图5-3。\",\"图5-3 GLM二维编码\",\"对于Position 1，表示词在partA中的位置，Position 2表示被Masked的词在partB中的位置，如果Position 2 = 0，表示非Masked的词。\"]},\"16\":{\"h\":\"5.3 GLM与Transformer\",\"t\":[\"上述我们也提到，GLM同样是基于Transformer的结构，不过与BERT、T5一样，GLM同样对Transformer的结构进行了修改。GLM同样只使用了Transformer的Encoder部分，并且做了以下修改：1.重新调整了LN和残差连接的顺序。2.对于Token的预测输出用的是单个的线形层。3.将激活函数由ReLU调整为GeLUs，因为GeLUs效果更好。\",\"至此，GLM的基本结构已经介绍完毕。四个基于Transformer的预训练语言模型也已经介绍完毕。它们整体相似，但是都有自己的创新点。相信在未来，会有更多更好的语言模型诞生。但是，为了适应下游任务，对于预训练好的语言模型，往往需要经过下游数据进行微调后才可更好的发挥它的能力。下面一节，我们介绍微调相关技术。\"]},\"17\":{\"h\":\"六、P-Tuning v2\",\"t\":[\"训练语言模型的成本是巨大的，往往小的企业或者个人应用的，都是在一个良好的预训练语言模型上进行微调。从前，预训练语言模型的方式只有全参数微调(Fine-Tuning)，全参数微调效果相对较好，可以让微调后的预训练模型在处理下游任务时得到良好的效果。但是全参数微调的设备需求仍然很大，比如，对于GLM-130B进行全参数微调，需要10台DGX A100服务器，设备就需要千万级别。这对很多企业和个人仍然是不能接受的。\",\"为了减少微调的设备等资源的消耗，研究者着手设计部分参数微调的方法，包括P-Tuning[7]、Prefix-Tuning[8]、Prompt-Tuning[9]、LoRA[10]和P-Tuning v2[6]。但是，P-Tuning、Prefix-Tuning虽然实现了部分参数调优，让微调的资源消耗降下来了，但是其性能仍然不如全参数微调。LoRA同样也是一种部分参数微调的方法，其在挖掘语言模型的潜在能力上有着不错的成绩，并且其资源消耗极低，受到了大家的关注。LoRA在文生图领域应用广泛。\",\"Prompt-Tuning和P-Tuning v2基本上是同一时期发布的，它们均基于Prefix-Tuning进行了修改和优化，结构基本一致。这里，我们选择P-Tuning v2进行介绍。\"]},\"18\":{\"h\":\"6.1 提示微调\",\"t\":[\"上述我们介绍P-Tuning v2属于部分参数微调，更准确地说，P-Tuning v2应该属于提示微调(Prompt Tuning)。提示微调只用一个冻结的语言模型来微调连续的提示，大大减少了训练时的存储和内存使用。\",\"提示微调冻结了预训练模型的所有参数，并使用自然语言提示来查询语言模型。比如，对于情感分析问题，我们可以将样本与提示“这部电影是[MASK]”串联起来，要求预训练语言模型预测被Masked的标注。然后，我们可以使用“好”与“坏”是被Masked标注的预测概率来预测样本的标签。提示微调完全不需要训练，只需要存储一份模型参数。\"]},\"19\":{\"h\":\"6.2 P-Tuning v2\",\"t\":[\"P-Tuning v2并不是一个全新的方法，其事实上是将文本生成的Prefix-Tuning技术适配到自然语言理解任务中，其主要结果如下：1.仅精调0.1%参数量（固定语言模型(LM)参数），在330M到10B参数规模的语言模型上，均取得和Fine-Tuning相似的性能。2.将Prompt-Tuning技术首次拓展到序列标注等复杂自然语言理解(NLU)任务上。\",\"P-Tuning v2的关键所在就是引入了Prefix-Tuning。Prefix-Tuning最开始应用在自然语言生成(NLG)，由[Prefix, x, y]三部分构成，如图6-1。Prefix为前缀，\",\"图6-1 Prefix-Tuning示意图\",\"x为输入，y为输出。Prefix-Tuning将预训练LM参数固定，Prefix参数进行微调，它不仅只在Embedding层进行微调，而是在每一层都进行微调。\",\"P-Tuning v2实际上就是Prefix-Tuning，如图6-2(b)。在Prefix部分，每一层Transformer的Embedding输入都需要被微调，这一点是不同于P-Tuning的，在P-Tuning中，只有第一层Embedding才需要被微调。这样看来，P-Tuning v2可以微调的参数变多了，假设Prefix部分由50个Token组成，那么P-Tuning v2共有50*12=600个参数需要微调。可微调的参数多了，效果自然也会好一些。\",\"图6-2 P-Tuning与P-Tuning v2\",\"此外，P-Tuning v2还包括以下改进：1.移除了Reparamerization加速训练方式；2.采用了多任务学习优化：基于多任务数据集的Prompt进行预训练，然后再适配下游任务；3.舍弃了词汇Mapping的Verbalizer的使用，重新利用[CLS]和字符标签，跟全参数微调一样利用CLS或者Token的输出做NLU，以增强通用性，可以适配到序列标注任务。\",\"与GLM一样，P-Tuning v2也是清华大学发布的，那么自然GLM原生地支持P-Tuning v2，并且更推荐使用P-Tuning v2对GLM进行微调。上述我们介绍，对GLM-130B进行全参数微调，需要10台DGX A100，而如果改为使用P-Tuning v2进行微调，近似性能下可以将设备减少为1台DGX A100。而对GLM进行微调同样还可以使用LoRA，虽然所需设备与P-Tuning v2几乎一致，但是其性能并没有P-Tuning v2好。\"]},\"20\":{\"h\":\"参考文献\",\"t\":[\"[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000–6010.\",\"[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL 2019, pages 4171–4186.\",\"[3] Raffel Colin, Shazeer Noam, Roberts Adam, Lee Katherine, Narang Sharan, Matena Michael, Zhou Yanqi, Li Wei, and Liu Peter J.. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 140 (2020), 1–67. http://jmlr.org/papers/v21/20-074.html.\",\"[4] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018a. Improving Language Understanding by Generative Pre-Training.\",\"[5] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2021. All nlp tasks are generation tasks: A general pretraining framework. arXiv preprint arXiv:2103.10360.\",\"[6] Liu, X. et al. P-tuning: prompt tuning can be comparable to fine-tuning universally across scales and tasks. In Proc. the 60th Annual Meeting of the Association for Computational Linguistics. 2, 61–68 (2022).\",\"[7] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. arXiv:2103.10385.\",\"[8] Xiang Lisa Li and Percy Liang. 2021. Prefixtuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190.\",\"[9] Lester Brian, Al-Rfou Rami, and Constant Noah. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’21), Moens Marie-Francine, Huang Xuanjing, Specia Lucia, and Yih Scott Wen-tau (Eds.). Association for Computational Linguistics, 3045–3059\",\"[10] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)\",\"[11] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\",\"[12] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv e-prints.\",\"[13] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang,\\nXiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, and Hsiao-Wuen\\nHon. 2020. Unilmv2: Pseudo-masked language models for unified language model\\npre-training. In ICML 2020,volume 119, pages 642–652.\"]},\"21\":{\"h\":\"LLaMA 2 论文笔记\",\"t\":[\"该论文篇幅巨大，非常详细地介绍了LLaMA 2的预训练和微调过程，本篇笔记对其内容进行简要记录。论文链接\"]},\"22\":{\"h\":\"1 介绍\",\"t\":[\"LLaMA 2共包括两大版本：预训练模型LLaMA 2和基于它的微调模型LLaMA 2-Chat。共有7B 13B 34B 70B四种参数规模。\",\"LLaMA 2，是LLaMA 1的升级版本，在多个公开可获得数据上进行训练。相比LLaMA 1，LLaMA 2预训练的语料库增加了40%，上下文长度（即可接受的输入长度）增加一倍（最多支持4K token)，并且采用了分组查询注意力（grouped-query，后续有介绍）机制。\",\"LLaMA 2-Chat是基于LLaMA 2预训练模型微调得来的，后续有详细介绍。\",\"在第2节，本文介绍LLaMA 2的预训练方法；在第3节，本文介绍LLaMA 2-Chat的微调方法；\"]},\"23\":{\"h\":\"2 预训练方法\",\"t\":[\"LLaMA 2的预训练特性如下：\",\"LLaMA 2基本还是采用LLaMA 1的训练方法：LLaMA 1论文链接\",\"优化的自回归Transformer（归一化、激活函数等有变化，下面有介绍）\",\"具体而言，LLaMA 2执行了更稳健的数据清理，更新了数据混合，对总token进行了40%以上的训练，将上下文长度增加了一倍，并使用分组查询注意力（GQA）来提高LLaMA 2的推理可扩展性\",\"（2T = 2 trillion = 2万亿）\"]},\"24\":{\"h\":\"2.1 训练数据\",\"t\":[\"LLaMA 2的训练数据来自混合后的公开数据，共在2T token上进行训练。\"]},\"25\":{\"h\":\"2.2 训练详情\",\"t\":[\"预训练的设置和模型架构和LLaMA 1基本一致：\",\"Transformer架构\",\"归一化 RMSNorm\",\"激活函数 SwiGLU\",\"旋转位置Embedding RoPE rotary positional embeddings，这个现在都在用，包括GLM\",\"与LLaMA 1差异：增加了上下文长度、增加了GQA\"]},\"26\":{\"h\":\"A.2.1 额外的预训练信息之与LLaMA 1的变化内容介绍 p.46\",\"t\":[\"上下文长度 更长的上下文长度可以让模型处理更多信息，这可以让模型支持记住更多对话历史信息、更多的总结任务、理解更长的文本\",\"Grouped-Query Attention 自回归解码的标准做法是缓存序列中先前token的key（K）和value（V）对，从而加快注意力计算。然而，随着context window或batch size的增加，与多头注意力（MHA）模型中的KV缓存大小相关的内存成本显著增长。对于KV缓存大小成为瓶颈的大型模型，可以在多个头之间共享key和value预测，而不会导致性能大幅下降。可以使用具有单个KV投影的原始多查询格式(MQA)或具有8KV投影的分组查询注意力(GQA)变体。基于消融结果和易于缩放推断，对于34B和70B Llama 2模型，LLaMA 2选择使用GQA而不是MQA。\",\"额外发现：一个多卡并行训练的论文：Training multi-billion parameter language models using model parallelism\"]},\"27\":{\"h\":\"预训练超参设置\",\"t\":[\"AdamW优化器 β1=0.9，β2=0.95，eps=10e-5\",\"余弦学习率，warmup 2000 steps\",\"将最终学习率降低到峰值学习率的10%\",\"使用0.1的权重衰减和1.0的梯度剪裁\"]},\"28\":{\"h\":\"预训练Tokenizer\",\"t\":[\"使用与LLaMA 1相同的标记器；它采用了字节对编码（BPE）算法，使用了来自SentencePiece的实现。与LLaMA 1一样，将所有数字拆分为单个数字，并使用字节分解未知的UTF-8字符。总词汇大小为32k个标记。\"]},\"29\":{\"h\":\"2.3 评测\",\"t\":[\"评测结果如下：\"]},\"30\":{\"h\":\"3. Fine-tuning\",\"t\":[\"LLaMA 2-Chat基于LLaMA 2微调而来，包括指令微调和RLHF\",\"本节报告了使用监督微调以及初始和迭代奖励建模和RLHF进行的实验和发现。\",\"提出一种新技术，Ghost Attention (GAtt)，用于帮助控制多轮对话流\"]},\"31\":{\"h\":\"3.1 Supervised Fine-Tuning (SFT)\",\"t\":[\"开始：用公开可获得的指令微调数据，和LLaMA 1一致，使用了 Scaling Instruction-Finetuned Language Models\",\"公开的指令微调数据质量参差不齐，首先就是要收集大量的高质量SFT数据，如下图：\",\"高质量指令微调数据，即使是少量的，也可以让结果很好。万级别的好的数据足够了，Meta总共收集了27540个好数据。\",\"对于监督微调，使用了余弦学习率，LR=2e-5，权重衰减=0.1，batch size = 64，sequence lenght = 4096\",\"对于微调过程，每个样本都包含一个提示和一个答案。为了确保模型序列长度正确填充，作者将训练集中的所有提示和答案连接起来。使用一个特殊的令牌来分隔提示段和应答段。作者使用自回归目标，并从用户提示中消除令牌的损失，因此，作者只对回答令牌进行反向传播。最后，作者对模型进行了2个epochs的微调。\"]},\"32\":{\"h\":\"3.2 RLHF\"},\"33\":{\"h\":\"3.2.1 人类偏好数据收集\",\"t\":[\"与其他方案相比，作者选择了二进制比较协议，主要是因为它使作者能够最大限度地提高收集到的提示的多样性。作者的注释过程如下。作者要求注释器首先编写一个提示，然后根据提供的标准在两个采样的模型响应之间进行选择。为了最大限度地提高多样性，从两个不同的模型变量中对给定提示的两个响应进行采样，并改变温度超参数。除了给参与者一个被迫的选择之外，作者还要求注释者标注他们更喜欢自己选择的回答而不是选择的程度：要么他们的选择明显更好，要么更好，要么稍微好一点，要么好到可以忽略不计/不确定。\",\"（就是给个输入，然后有两种输出，看哪个更符合标准）\",\"用到的一些人类偏好开源数据集\"]},\"34\":{\"h\":\"3.2.2 奖励建模\",\"t\":[\"奖励建模就是拿一个模型的结果和它相关的Prompt作为输入，然后输出一个分数来表明这个结果的质量（有用性、安全性等），用这个分数，就可以在RLHF中优化模型了\",\"为了训练奖励模型，作者将收集的成对人类偏好数据转换为二元排名标签格式（即选择和拒绝），并强制选择的响应比对应的响应具有更高的分数。作者使用了二元排名损失：\",\"where rθ(x, y) is the scalar score output for prompt x and completion y with model weights θ. yc is the preferred response that annotators choose and yr is the rejected counterpart.\",\"在这种二元排名损失的基础上，作者进一步修改它，如第3.2.1节所示，利用这些信息来明确教导奖励模型为具有更多差异的世代分配更多不一致的分数可能是有用的。为此，作者在损失中进一步添加了一个margin成分：\",\"where the margin m(r) is a discrete function of the preference rating. Naturally, we use a large margin for pairs with distinct responses, and a smaller one for those with similar responses (shown in Table 27). We found this margin component can improve Helpfulness reward model accuracy especially on samples where two responses are more separable. More detailed ablation and analysis can be found in Table 28 in Appendix A.3.3.\",\"奖励建模的训练详情。LLaMA 2-Chat在训练数据上训练一个epoch。在早期的实验中，发现训练时间过长会导致过度拟合。LLaMA 2-Chat使用与基本模型相同的优化器参数。70B参数LLaMA 2-Chat的最大学习率为5×10−6，其余参数为1×10−5。学习率按余弦学习率计划降低，降至最大学习率的10%。 LLaMA 2-chat使用占总步数3%的warm-up，最少5 steps。有效batch大小固定为512对，即每batch 1024行。\",\"奖励建模的结果。在每一batch用于奖励建模的人类偏好注释上，都拿出1000个例子作为测试集来评估模型(这个都是可以学习的地方，按他的来)。作者将相应测试集的所有提示的并集分别称为“Meta Helpfulness”和“Meta Safety”。总体而言，这个奖励模型优于所有base-line，包括GPT-4。\",\"当作者在表8中按偏好评级对分数进行分组时，我们可以看到“明显更好”的测试集，并随着比较对变得更加相似而逐渐退化（例如，“稍微好一点”）。当在两个相似的模型反应之间做出决定时，由于注释者的主观性和他们对可能区分反应的细微细节的依赖，学习对人类偏好进行建模将变得具有挑战性。作者强调，对于提高LLaMA 2-Chat的性能，更明显的响应的准确性最为重要。与相似对相比，在更明显的反应上，人类偏好注释一致率也更高。\"]},\"35\":{\"h\":\"3.2.3 迭代微调\",\"t\":[\"作者通过两个主要的算法来探索RLHF微调：\",\"近端策略优化(Proximal Policy Optimization, PPO)，论文链接，该算法是RLHF文献的标准。\",\"拒绝采样微调(Rejection Sampling fine-tuning)，作者对模型中的K个输出进行采样，并用我们的奖励选择最佳候选者，这与Constitutional AI: Harmlessness from AI Feedback保持一致。Residual Energy-Based Models for Text Generation也提出了同样的LLM重新排序策略，其中奖励被视为能量函数。在这里，作者更进一步，使用选定的输出进行梯度更新。对于每个Prompt，获得最高奖励分数的样本被视为新的金标准。与Discriminative Adversarial Search for Abstractive Summarization类似，然后作者在新的一组排序样本上微调模型，以增强奖励。\",\"这两个强化学习(RL)算法的主要不同：\",\"广度。在拒绝采样中，该模型为给定提示探索K个样本，而对PPO只进行一次生成。\",\"深度。在PPO中，在步骤t的训练期间，样本是前一步骤的梯度更新后从t−1更新的模型策略的函数。在拒绝采样微调中，在应用类似于SFT的微调之前，作者在给定模型的初始策略的情况下对所有输出进行采样，以收集新的数据集。然而，由于作者应用了迭代模型更新，两种RL算法之间的基本差异就不那么明显了。\",\"拒绝采样的介绍。作者只对最大的70B LLaMA 2-Chat进行拒绝采样。所有较小的模型都根据较大模型的拒绝采样数据进行微调，从而将较大模型的能力提取到较小的模型中。在每个迭代过程，作者从最新的模型中对每个Prompt采样K个答案，作者为每个样本打分，给出实验时可访问的最佳奖励模型，然后为给定提示选择最佳答案。\",\"作者在图7中说明了拒绝采样的好处。最大曲线和中值曲线之间的增量可以解释为对最佳输出进行微调的潜在增益。正如预期的那样，这个增量随着样本的增加而增加，因为最大值增加（即，更多的样本，产生良好轨迹的机会更多），而中值保持不变。探索和作者能在样本中获得的最大回报之间有着直接的联系。温度参数对勘探也起着重要作用，因为更高的温度使作者能够对更多样的输出进行采样。\",\"在图8中，作者报告了LLaMA 2-Chat-SFT（左）和LLaMA 2-Chat-RLHF（右），N个样本（其中N∈[1，…，100]）在不同温度下的最大回报曲线。可以观察到，在迭代模型更新过程中，最佳温度不是恒定的：RLHF对重新缩放温度有直接影响。对于LLaMA 2-Chat-RLHF，当在10到100个输出之间采样时，最佳温度为T∈[1.2，1.3]。因此，在有限的计算预算下，有必要逐步重新调整温度。请注意，对于每个模型，这种温度重新缩放都会发生恒定数量的步骤，并且总是从每个新RLHF版本的基本模型开始。\",\"PPO介绍。对于所有模型，作者使用AdamW优化器，其中β1=0.9，β2=0.95，eps=10−5。使用0.1的权重衰减(weight decay)、1.0的梯度剪裁(gradient clipping)和10e−6的恒定学习率(lr) 对于每个PPO迭代，我们使用512的批量大小(batch size)、0.2的PPO剪辑阈值、64的小批量大小(mini-batch size)，并且每个小批量采取一个梯度步骤(step)。对于7B和13B模型，我们设置β=0.01（KL惩罚），对于34B和70B模型，设置β=0.005。\",\"作者为所有模型进行了200到400次迭代的训练，并对延迟的提示进行了评估，以提前停止。70B模型上的PPO每次迭代平均耗时≈330秒。为了快速进行大批量训练，作者使用PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel。这在使用O（1）正向或反向传播时是有效的，但在生成过程中会导致很大的减慢（≈20×），即使在使用大批量和KV缓存时也是如此。作者能够通过在生成之前将模型权重合并到每个节点一次，然后在生成之后释放内存，恢复训练循环的其余部分来缓解这种情况。\"]},\"36\":{\"h\":\"3.3 多轮一致性的指令\",\"t\":[\"在对话设置中，一些指示应适用于所有的对话轮数中，例如，简洁地回应，或“扮演”某个公众人物。然而，在初始版本的RLHF模型中，LLaMA 2-Chat会忘记指示，如下图左侧（右侧是使用GAtt优化后的结果）：\",\"为了解决这些限制，作者提出了Ghost Attention（GAtt），这是一种受上下文蒸馏，Constitutional AI: Harmlessness from AI Feedback启发的非常简单的方法，它可以破解微调数据，以帮助在多阶段过程中集中注意力。GAtt允许对多轮进行对话控制，如图9（右）所示。\",\"Gatt方法。假设可以访问两个人（例如，用户和助手）之间的多回合对话数据集，该数据集具有消息列表[u1，a1，…，un，an]，其中un和an分别对应于回合n的用户和助手消息。然后定义了一个指令，inst，应该贯穿在整个对话中。例如，inst可以是“扮演...”。然后可以将此指令综合连接到会话的所有用户消息。\",\"接下来可以使用最新的RLHF模型对这些合成数据进行采样。现在有了一个上下文对话和样本，可以在类似于拒绝采样的过程中对模型进行微调。可以在除第一个回合外的所有回合中放弃它，而不是用指令来增加所有上下文对话回合，但这会导致系统消息（即最后一个回合之前的所有中间辅助消息）与样本在训练时间不匹配。为了解决这个可能影响训练的问题，只需将前几轮中的所有token（包括助手的消息）的loss设置为0。\",\"对于训练指令，作者创建了一些综合约束条件：爱好（“你喜欢例如网球”）、语言（“用例如法语说话”）或公众人物（“扮演例如拿破仑”）。为了获得兴趣爱好和公众人物的列表，作者要求LLaMA 2-Chat生成它，以避免教学和模型知识之间的不匹配（例如，要求模型扮演训练中没有遇到的人）。为了使指令更加复杂和多样化，作者通过随机组合上述约束来构建最终指令。在为训练数据构建最终系统消息时，作者也会在一半的时间内修改原始指令，使其不那么冗长，例如，“从现在起始终充当拿破仑”-> “人物：拿破仑。”这些步骤生成了一个SFT数据集，可以在该数据集上微调LLaMA 2-Chat。\",\"为了说明GAtt如何在微调过程中帮助重塑注意力，作者在下图中显示了模型的最大注意力激活。每个图的左侧对应系统信息（“Act as Oscar Wilde，扮演奥斯卡·王尔德”）。我们可以看到，与没有GAtt的模型（左）相比，配备GAtt的型号（右）在对话的大部分时间里保持了对系统消息的大量注意力激活。（？差别很大吗）\"]},\"37\":{\"h\":\"3.4 RLHF结果\"},\"38\":{\"h\":\"3.4.1 基于模型的评估\",\"t\":[\"模型的进展。图11报告了作者针对安全和帮助轴的不同SFT和RLHF版本的进展，通过Meta内部的安全和帮助奖励模型进行测量。在这组评估中，RLHF-V3之后的两个轴上都优于ChatGPT（无害和有用>50%）。尽管前面提到了使用Meta的奖励作为逐点衡量标准的相关性，但可以说，它可能偏向于LLaMA 2-Chat。因此，为了进行公平的比较，作者使用GPT-4额外计算最终结果，以评估哪一代是优选的。ChatGPT和LLaMA 2-Chat输出在GPT-4提示中出现的顺序是随机交换的，以避免任何偏差。正如预期的那样，支持LLaMA 2-Chat的胜率不那么明显，尽管我们最新的LLaMA 2-Chat获得了超过60%的胜率。\"]},\"39\":{\"h\":\"3.4.2 人类评价\",\"t\":[\"人类评价通常被认为是评判自然语言生成模型（包括对话模型）的黄金标准。为了评估主要模型版本的质量，作者要求人类评估人员对其有用性和安全性进行评分。作者将LLaMA 2-Chat模型与开源模型（Falcon，MPT ，Vicuna）以及4000多个单回合和多回合提示的闭源模型（ChatGPT和PaLM）进行了比较。对于ChatGPT，作者使用gpt-3.5-turbo-0301型号。对于PaLM，作者使用chat-bison-001模型。\",\"结果如图12所示，LLaMA 2-Chat模型在单回合和多回合提示上都显著优于开源模型。特别是，LLaMA 2-Chat 7B模型在60%的提示上优于MPT-7B-Chat。LLaMA 2-Chat 34B与同等尺寸的Vicuna-33B和Falcon 40B型号相比，总体胜率超过75%。 最大的LLaMA 2-Chat模型与ChatGPT具有竞争力。LLaMA 2-Chat 70B模型相对于ChatGPT的胜率为36%，平局率为31.5%。在我们的提示集上，LLaMA 2-Chat 70B模型在很大程度上优于PaLM bison聊天模型。\"]},\"40\":{\"h\":\"L L M\"},\"41\":{\"h\":\"\"}},\"dirtCount\":0,\"index\":[[\"平局率为31\",{\"1\":{\"39\":1}}],[\"结果如图12所示\",{\"1\":{\"39\":1}}],[\"结构基本一致\",{\"1\":{\"17\":1}}],[\"进行了比较\",{\"1\":{\"39\":1}}],[\"尽管我们最新的llama\",{\"1\":{\"38\":1}}],[\"尽管前面提到了使用meta的奖励作为逐点衡量标准的相关性\",{\"1\":{\"38\":1}}],[\"支持llama\",{\"1\":{\"38\":1}}],[\"无害和有用>50\",{\"1\":{\"38\":1}}],[\"差别很大吗\",{\"1\":{\"36\":1}}],[\"配备gatt的型号\",{\"1\":{\"36\":1}}],[\"王尔德\",{\"1\":{\"36\":1}}],[\"拿破仑\",{\"1\":{\"36\":1}}],[\"人类评价通常被认为是评判自然语言生成模型\",{\"1\":{\"39\":1}}],[\"人类评价\",{\"0\":{\"39\":1}}],[\"人类偏好注释一致率也更高\",{\"1\":{\"34\":1}}],[\"人类偏好数据收集\",{\"0\":{\"33\":1}}],[\"人物\",{\"1\":{\"36\":1}}],[\">\",{\"1\":{\"36\":1}}],[\"使其不那么冗长\",{\"1\":{\"36\":1}}],[\"使用选定的输出进行梯度更新\",{\"1\":{\"35\":1}}],[\"使用一个特殊的令牌来分隔提示段和应答段\",{\"1\":{\"31\":1}}],[\"使用了余弦学习率\",{\"1\":{\"31\":1}}],[\"使用了\",{\"1\":{\"31\":1}}],[\"使用了来自sentencepiece的实现\",{\"1\":{\"28\":1}}],[\"使用与llama\",{\"1\":{\"28\":1}}],[\"使用0\",{\"1\":{\"27\":1,\"35\":1}}],[\"语言\",{\"1\":{\"36\":1}}],[\"语言模型\",{\"1\":{\"6\":1,\"12\":1}}],[\"你喜欢例如网球\",{\"1\":{\"36\":1}}],[\"爱好\",{\"1\":{\"36\":1}}],[\"现在有了一个上下文对话和样本\",{\"1\":{\"36\":1}}],[\"接下来可以使用最新的rlhf模型对这些合成数据进行采样\",{\"1\":{\"36\":1}}],[\"接下来我们来介绍bert中另外一个部分\",{\"1\":{\"9\":1}}],[\"应该贯穿在整个对话中\",{\"1\":{\"36\":1}}],[\"应该是效果比较不错的一个模型\",{\"1\":{\"13\":1}}],[\"之间的多回合对话数据集\",{\"1\":{\"36\":1}}],[\"之所以要缩放\",{\"1\":{\"5\":1}}],[\"假设可以访问两个人\",{\"1\":{\"36\":1}}],[\"假设prefix部分由50个token组成\",{\"1\":{\"19\":1}}],[\"某个公众人物\",{\"1\":{\"36\":1}}],[\"扮演奥斯卡\",{\"1\":{\"36\":1}}],[\"扮演例如拿破仑\",{\"1\":{\"36\":1}}],[\"扮演\",{\"1\":{\"36\":2}}],[\"简洁地回应\",{\"1\":{\"36\":1}}],[\"恢复训练循环的其余部分来缓解这种情况\",{\"1\":{\"35\":1}}],[\"≈20×\",{\"1\":{\"35\":1}}],[\"设置β=0\",{\"1\":{\"35\":1}}],[\"设备就需要千万级别\",{\"1\":{\"17\":1}}],[\"请注意\",{\"1\":{\"35\":1}}],[\"右侧是使用gatt优化后的结果\",{\"1\":{\"36\":1}}],[\"右\",{\"1\":{\"35\":1,\"36\":2}}],[\"左\",{\"1\":{\"35\":1,\"36\":1}}],[\"左右\",{\"1\":{\"1\":1}}],[\"温度参数对勘探也起着重要作用\",{\"1\":{\"35\":1}}],[\"探索和作者能在样本中获得的最大回报之间有着直接的联系\",{\"1\":{\"35\":1}}],[\"产生良好轨迹的机会更多\",{\"1\":{\"35\":1}}],[\"正向或反向传播时是有效的\",{\"1\":{\"35\":1}}],[\"正如预期的那样\",{\"1\":{\"35\":1,\"38\":1}}],[\"正是bert\",{\"1\":{\"6\":1}}],[\"给出实验时可访问的最佳奖励模型\",{\"1\":{\"35\":1}}],[\"给我从英语翻译成汉语\",{\"1\":{\"11\":1}}],[\"拒绝采样的介绍\",{\"1\":{\"35\":1}}],[\"拒绝采样微调\",{\"1\":{\"35\":1}}],[\"两种rl算法之间的基本差异就不那么明显了\",{\"1\":{\"35\":1}}],[\"样本是前一步骤的梯度更新后从t−1更新的模型策略的函数\",{\"1\":{\"35\":1}}],[\"深度\",{\"1\":{\"35\":1}}],[\"广度\",{\"1\":{\"35\":1}}],[\"获得最高奖励分数的样本被视为新的金标准\",{\"1\":{\"35\":1}}],[\"近端策略优化\",{\"1\":{\"35\":1}}],[\"近似性能下可以将设备减少为1台dgx\",{\"1\":{\"19\":1}}],[\"迭代微调\",{\"0\":{\"35\":1}}],[\"学习对人类偏好进行建模将变得具有挑战性\",{\"1\":{\"34\":1}}],[\"学习率按余弦学习率计划降低\",{\"1\":{\"34\":1}}],[\"稍微好一点\",{\"1\":{\"34\":1}}],[\"例如\",{\"1\":{\"34\":1,\"36\":5}}],[\"例如在nsp任务中\",{\"1\":{\"8\":1}}],[\"明显更好\",{\"1\":{\"34\":1}}],[\"总体胜率超过75\",{\"1\":{\"39\":1}}],[\"总体而言\",{\"1\":{\"34\":1}}],[\"总词汇大小为32k个标记\",{\"1\":{\"28\":1}}],[\"按他的来\",{\"1\":{\"34\":1}}],[\"降至最大学习率的10\",{\"1\":{\"34\":1}}],[\"发现训练时间过长会导致过度拟合\",{\"1\":{\"34\":1}}],[\"利用这些信息来明确教导奖励模型为具有更多差异的世代分配更多不一致的分数可能是有用的\",{\"1\":{\"34\":1}}],[\"θ\",{\"1\":{\"34\":1}}],[\"安全性等\",{\"1\":{\"34\":1}}],[\"有必要逐步重新调整温度\",{\"1\":{\"35\":1}}],[\"有效batch大小固定为512对\",{\"1\":{\"34\":1}}],[\"有用性\",{\"1\":{\"34\":1}}],[\"有的学者尝试将注意力机制\",{\"1\":{\"3\":1}}],[\"奖励建模的结果\",{\"1\":{\"34\":1}}],[\"奖励建模的训练详情\",{\"1\":{\"34\":1}}],[\"奖励建模就是拿一个模型的结果和它相关的prompt作为输入\",{\"1\":{\"34\":1}}],[\"奖励建模\",{\"0\":{\"34\":1}}],[\"看哪个更符合标准\",{\"1\":{\"33\":1}}],[\"要求模型扮演训练中没有遇到的人\",{\"1\":{\"36\":1}}],[\"要求预训练语言模型预测被masked的标注\",{\"1\":{\"18\":1}}],[\"要么好到可以忽略不计\",{\"1\":{\"33\":1}}],[\"要么稍微好一点\",{\"1\":{\"33\":1}}],[\"要么更好\",{\"1\":{\"33\":1}}],[\"要么他们的选择明显更好\",{\"1\":{\"33\":1}}],[\"除了给参与者一个被迫的选择之外\",{\"1\":{\"33\":1}}],[\"主要是因为它使作者能够最大限度地提高收集到的提示的多样性\",{\"1\":{\"33\":1}}],[\"主要研究点\",{\"0\":{\"1\":1}}],[\"作者也会在一半的时间内修改原始指令\",{\"1\":{\"36\":1}}],[\"作者通过随机组合上述约束来构建最终指令\",{\"1\":{\"36\":1}}],[\"作者通过两个主要的算法来探索rlhf微调\",{\"1\":{\"35\":1}}],[\"作者要求人类评估人员对其有用性和安全性进行评分\",{\"1\":{\"39\":1}}],[\"作者要求llama\",{\"1\":{\"36\":1}}],[\"作者要求注释器首先编写一个提示\",{\"1\":{\"33\":1}}],[\"作者创建了一些综合约束条件\",{\"1\":{\"36\":1}}],[\"作者提出了ghost\",{\"1\":{\"36\":1}}],[\"作者能够通过在生成之前将模型权重合并到每个节点一次\",{\"1\":{\"35\":1}}],[\"作者为所有模型进行了200到400次迭代的训练\",{\"1\":{\"35\":1}}],[\"作者为每个样本打分\",{\"1\":{\"35\":1}}],[\"作者报告了llama\",{\"1\":{\"35\":1}}],[\"作者从最新的模型中对每个prompt采样k个答案\",{\"1\":{\"35\":1}}],[\"作者只对最大的70b\",{\"1\":{\"35\":1}}],[\"作者只对回答令牌进行反向传播\",{\"1\":{\"31\":1}}],[\"作者在下图中显示了模型的最大注意力激活\",{\"1\":{\"36\":1}}],[\"作者在图7中说明了拒绝采样的好处\",{\"1\":{\"35\":1}}],[\"作者在给定模型的初始策略的情况下对所有输出进行采样\",{\"1\":{\"35\":1}}],[\"作者在损失中进一步添加了一个margin成分\",{\"1\":{\"34\":1}}],[\"作者更进一步\",{\"1\":{\"35\":1}}],[\"作者对模型中的k个输出进行采样\",{\"1\":{\"35\":1}}],[\"作者对模型进行了2个epochs的微调\",{\"1\":{\"31\":1}}],[\"作者强调\",{\"1\":{\"34\":1}}],[\"作者进一步修改它\",{\"1\":{\"34\":1}}],[\"作者使用chat\",{\"1\":{\"39\":1}}],[\"作者使用gpt\",{\"1\":{\"38\":1,\"39\":1}}],[\"作者使用pytorch\",{\"1\":{\"35\":1}}],[\"作者使用adamw优化器\",{\"1\":{\"35\":1}}],[\"作者使用了二元排名损失\",{\"1\":{\"34\":1}}],[\"作者使用自回归目标\",{\"1\":{\"31\":1}}],[\"作者将llama\",{\"1\":{\"39\":1}}],[\"作者将相应测试集的所有提示的并集分别称为\",{\"1\":{\"34\":1}}],[\"作者将收集的成对人类偏好数据转换为二元排名标签格式\",{\"1\":{\"34\":1}}],[\"作者将训练集中的所有提示和答案连接起来\",{\"1\":{\"31\":1}}],[\"作者还要求注释者标注他们更喜欢自己选择的回答而不是选择的程度\",{\"1\":{\"33\":1}}],[\"作者的注释过程如下\",{\"1\":{\"33\":1}}],[\"作者选择了二进制比较协议\",{\"1\":{\"33\":1}}],[\"作为编码器和解码器\",{\"1\":{\"3\":1}}],[\"每个图的左侧对应系统信息\",{\"1\":{\"36\":1}}],[\"每个样本都包含一个提示和一个答案\",{\"1\":{\"31\":1}}],[\"每一层transformer的embedding输入都需要被微调\",{\"1\":{\"19\":1}}],[\"权重衰减=0\",{\"1\":{\"31\":1}}],[\"万级别的好的数据足够了\",{\"1\":{\"31\":1}}],[\"万亿模型都不足为奇\",{\"1\":{\"1\":1}}],[\"高质量指令微调数据\",{\"1\":{\"31\":1}}],[\"公开的指令微调数据质量参差不齐\",{\"1\":{\"31\":1}}],[\"公式如下\",{\"1\":{\"5\":2,\"14\":1}}],[\"开始\",{\"1\":{\"31\":1}}],[\"评测结果如下\",{\"1\":{\"29\":1}}],[\"评测\",{\"0\":{\"29\":1}}],[\"算法的主要不同\",{\"1\":{\"35\":1}}],[\"算法\",{\"1\":{\"28\":1}}],[\"余弦学习率\",{\"1\":{\"27\":1}}],[\"β2=0\",{\"1\":{\"27\":1,\"35\":1}}],[\"β1=0\",{\"1\":{\"27\":1}}],[\"额外发现\",{\"1\":{\"26\":1}}],[\"额外的预训练信息之与llama\",{\"0\":{\"26\":1}}],[\"变体\",{\"1\":{\"26\":1}}],[\"随着context\",{\"1\":{\"26\":1}}],[\"随机变为其他token\",{\"1\":{\"14\":1}}],[\"随机替换为其他token\",{\"1\":{\"9\":1}}],[\"理解更长的文本\",{\"1\":{\"26\":1}}],[\"旋转位置embedding\",{\"1\":{\"25\":1}}],[\"激活函数\",{\"1\":{\"25\":1}}],[\"激活函数等有变化\",{\"1\":{\"23\":1}}],[\"共在2t\",{\"1\":{\"24\":1}}],[\"共有7b\",{\"1\":{\"22\":1}}],[\"具体而言\",{\"1\":{\"23\":1}}],[\"具体地\",{\"1\":{\"9\":1}}],[\"归一化\",{\"1\":{\"23\":1,\"25\":1}}],[\"优化的自回归transformer\",{\"1\":{\"23\":1}}],[\"机制\",{\"1\":{\"22\":1}}],[\"query\",{\"1\":{\"22\":1,\"26\":1}}],[\"qiu\",{\"1\":{\"20\":1}}],[\"qian\",{\"1\":{\"20\":2}}],[\"增加了gqa\",{\"1\":{\"25\":1}}],[\"增加了上下文长度\",{\"1\":{\"25\":1}}],[\"增加了一个segment\",{\"1\":{\"8\":1}}],[\"增加一倍\",{\"1\":{\"22\":1}}],[\"上下文长度\",{\"1\":{\"22\":1,\"26\":1}}],[\"上述我们介绍\",{\"1\":{\"19\":1}}],[\"上述我们介绍p\",{\"1\":{\"18\":1}}],[\"上述我们也提到\",{\"1\":{\"16\":1}}],[\"上述两个句子序列的词完全相同\",{\"1\":{\"5\":1}}],[\"介绍\",{\"0\":{\"22\":1}}],[\"论文链接\",{\"1\":{\"21\":1,\"35\":1}}],[\"论文笔记\",{\"0\":{\"21\":1}}],[\"非常详细地介绍了llama\",{\"1\":{\"21\":1}}],[\"zettlemoyer\",{\"1\":{\"20\":1}}],[\"z\",{\"1\":{\"20\":1}}],[\"zhu\",{\"1\":{\"20\":1}}],[\"zheng\",{\"1\":{\"20\":1}}],[\"zhengxiao\",{\"1\":{\"20\":2}}],[\"zhilin\",{\"1\":{\"20\":2}}],[\"zhou\",{\"1\":{\"20\":2}}],[\"where\",{\"1\":{\"34\":3}}],[\"wilde\",{\"1\":{\"36\":1}}],[\"window或batch\",{\"1\":{\"26\":1}}],[\"with\",{\"1\":{\"20\":1,\"34\":3}}],[\"wuen\",{\"1\":{\"20\":1}}],[\"w\",{\"1\":{\"20\":1}}],[\"warmup\",{\"1\":{\"27\":1}}],[\"wang\",{\"1\":{\"20\":4}}],[\"wallis\",{\"1\":{\"20\":1}}],[\"we\",{\"1\":{\"34\":2}}],[\"wenhui\",{\"1\":{\"20\":1}}],[\"wen\",{\"1\":{\"20\":1}}],[\"weight\",{\"1\":{\"35\":1}}],[\"weights\",{\"1\":{\"34\":1}}],[\"wei\",{\"1\":{\"20\":3}}],[\"u1\",{\"1\":{\"36\":1}}],[\"up\",{\"1\":{\"34\":1}}],[\"use\",{\"1\":{\"34\":1}}],[\"using\",{\"1\":{\"26\":1}}],[\"uszkoreit\",{\"1\":{\"20\":2}}],[\"un\",{\"1\":{\"36\":1}}],[\"understands\",{\"1\":{\"20\":1}}],[\"understanding\",{\"1\":{\"20\":2}}],[\"unilmv2\",{\"1\":{\"20\":1}}],[\"universally\",{\"1\":{\"20\":1}}],[\"unified\",{\"1\":{\"20\":2}}],[\"joshi\",{\"1\":{\"20\":1}}],[\"jones\",{\"1\":{\"20\":1}}],[\"jianfeng\",{\"1\":{\"20\":1}}],[\"jingfei\",{\"1\":{\"20\":1}}],[\"jie\",{\"1\":{\"20\":2}}],[\"jiezhong\",{\"1\":{\"20\":1}}],[\"jmlr\",{\"1\":{\"20\":1}}],[\"j\",{\"1\":{\"20\":3}}],[\"jacob\",{\"1\":{\"20\":1}}],[\"jakob\",{\"1\":{\"20\":2}}],[\"参考文献\",{\"0\":{\"20\":1}}],[\"参数\",{\"1\":{\"19\":1}}],[\"参数量\",{\"1\":{\"19\":1}}],[\"虽然所需设备与p\",{\"1\":{\"19\":1}}],[\"虽然把这个模型放到现在看起来规模不大\",{\"1\":{\"7\":1}}],[\"以及4000多个单回合和多回合提示的闭源模型\",{\"1\":{\"39\":1}}],[\"以避免任何偏差\",{\"1\":{\"38\":1}}],[\"以避免教学和模型知识之间的不匹配\",{\"1\":{\"36\":1}}],[\"以评估哪一代是优选的\",{\"1\":{\"38\":1}}],[\"以帮助在多阶段过程中集中注意力\",{\"1\":{\"36\":1}}],[\"以提前停止\",{\"1\":{\"35\":1}}],[\"以收集新的数据集\",{\"1\":{\"35\":1}}],[\"以增强奖励\",{\"1\":{\"35\":1}}],[\"以增强通用性\",{\"1\":{\"19\":1}}],[\"以上的训练\",{\"1\":{\"23\":1}}],[\"以胜任多种自然语言处理任务\",{\"1\":{\"13\":1}}],[\"跟全参数微调一样利用cls或者token的输出做nlu\",{\"1\":{\"19\":1}}],[\"重新利用\",{\"1\":{\"19\":1}}],[\"重新调整了ln和残差连接的顺序\",{\"1\":{\"16\":1}}],[\"舍弃了词汇mapping的verbalizer的使用\",{\"1\":{\"19\":1}}],[\"基于模型的评估\",{\"0\":{\"38\":1}}],[\"基于消融结果和易于缩放推断\",{\"1\":{\"26\":1}}],[\"基于多任务数据集的prompt进行预训练\",{\"1\":{\"19\":1}}],[\"基本上都采用了transformer或者基于transformer修改的模型\",{\"1\":{\"5\":1}}],[\"基本上都与transformer有着千丝万缕的联系\",{\"1\":{\"1\":1}}],[\"基本已经可以确定的是\",{\"1\":{\"1\":1}}],[\"移除了reparamerization加速训练方式\",{\"1\":{\"19\":1}}],[\"效果自然也会好一些\",{\"1\":{\"19\":1}}],[\"效果比较好\",{\"1\":{\"9\":1}}],[\"yr\",{\"1\":{\"34\":1}}],[\"yc\",{\"1\":{\"34\":1}}],[\"yu\",{\"1\":{\"20\":1}}],[\"yujie\",{\"1\":{\"20\":2}}],[\"yinhan\",{\"1\":{\"20\":1}}],[\"yih\",{\"1\":{\"20\":1}}],[\"yanan\",{\"1\":{\"20\":1}}],[\"yang\",{\"1\":{\"20\":3}}],[\"yanqi\",{\"1\":{\"20\":1}}],[\"you\",{\"1\":{\"20\":1}}],[\"y为输出\",{\"1\":{\"19\":1}}],[\"y\",{\"1\":{\"19\":1,\"20\":2,\"34\":2}}],[\"均取得和fine\",{\"1\":{\"19\":1}}],[\"固定语言模型\",{\"1\":{\"19\":1}}],[\"仅精调0\",{\"1\":{\"19\":1}}],[\"串联起来\",{\"1\":{\"18\":1}}],[\"提出一种新技术\",{\"1\":{\"30\":1}}],[\"提示微调完全不需要训练\",{\"1\":{\"18\":1}}],[\"提示微调冻结了预训练模型的所有参数\",{\"1\":{\"18\":1}}],[\"提示微调只用一个冻结的语言模型来微调连续的提示\",{\"1\":{\"18\":1}}],[\"提示微调\",{\"0\":{\"18\":1}}],[\"提高了seq2seq的效率\",{\"1\":{\"5\":1}}],[\"受到了大家的关注\",{\"1\":{\"17\":1}}],[\"受到了业内人士和广大群众的注意\",{\"1\":{\"0\":1}}],[\"让微调的资源消耗降下来了\",{\"1\":{\"17\":1}}],[\"让序列转换模型的效率得以一定的提升\",{\"1\":{\"3\":1}}],[\"包括对话模型\",{\"1\":{\"39\":1}}],[\"包括助手的消息\",{\"1\":{\"36\":1}}],[\"包括gpt\",{\"1\":{\"34\":1}}],[\"包括glm\",{\"1\":{\"25\":1}}],[\"包括指令微调和rlhf\",{\"1\":{\"30\":1}}],[\"包括p\",{\"1\":{\"17\":1}}],[\"包含了masked\",{\"1\":{\"12\":1}}],[\"研究者着手设计部分参数微调的方法\",{\"1\":{\"17\":1}}],[\"需要10台dgx\",{\"1\":{\"17\":1,\"19\":1}}],[\"都拿出1000个例子作为测试集来评估模型\",{\"1\":{\"34\":1}}],[\"都是在一个良好的预训练语言模型上进行微调\",{\"1\":{\"17\":1}}],[\"都要基于多个句子之间的关系\",{\"1\":{\"9\":1}}],[\"往往小的企业或者个人应用的\",{\"1\":{\"17\":1}}],[\"往往需要经过下游数据进行微调后才可更好的发挥它的能力\",{\"1\":{\"16\":1}}],[\"训练详情\",{\"0\":{\"25\":1}}],[\"训练数据\",{\"0\":{\"24\":1}}],[\"训练语言模型的成本是巨大的\",{\"1\":{\"17\":1}}],[\"训练和微调均进行了介绍\",{\"1\":{\"0\":1}}],[\"六\",{\"0\":{\"17\":1}}],[\"相比\",{\"1\":{\"36\":1}}],[\"相比llama\",{\"1\":{\"22\":1}}],[\"相信在未来\",{\"1\":{\"16\":1}}],[\"相当于看到了未来的信息\",{\"1\":{\"5\":1}}],[\"将所有数字拆分为单个数字\",{\"1\":{\"28\":1}}],[\"将最终学习率降低到峰值学习率的10\",{\"1\":{\"27\":1}}],[\"将上下文长度增加了一倍\",{\"1\":{\"23\":1}}],[\"将prompt\",{\"1\":{\"19\":1}}],[\"将激活函数由relu调整为gelus\",{\"1\":{\"16\":1}}],[\"将模型分为多个头\",{\"1\":{\"5\":1}}],[\"表示非masked的词\",{\"1\":{\"15\":1}}],[\"表示词在parta中的位置\",{\"1\":{\"15\":1}}],[\"0301型号\",{\"1\":{\"39\":1}}],[\"001模型\",{\"1\":{\"39\":1}}],[\"00190\",{\"1\":{\"20\":1}}],[\"005\",{\"1\":{\"35\":1}}],[\"01\",{\"1\":{\"35\":1}}],[\"0的梯度剪裁\",{\"1\":{\"27\":1,\"35\":1}}],[\"09685\",{\"1\":{\"20\":1}}],[\"074\",{\"1\":{\"20\":1}}],[\"0\",{\"1\":{\"15\":1,\"35\":1}}],[\"替代\",{\"1\":{\"14\":1}}],[\"替换\",{\"1\":{\"9\":1}}],[\"被\",{\"1\":{\"14\":1}}],[\"沿用了bert和t5的masked的比例\",{\"1\":{\"14\":1}}],[\"玩笑\",{\"1\":{\"14\":1}}],[\"玩\",{\"1\":{\"14\":1}}],[\"个人猜测\",{\"1\":{\"14\":1}}],[\"中可以看到\",{\"1\":{\"14\":1}}],[\"中\",{\"1\":{\"14\":1}}],[\"中的80\",{\"1\":{\"9\":1}}],[\"所示\",{\"1\":{\"36\":1}}],[\"所有较小的模型都根据较大模型的拒绝采样数据进行微调\",{\"1\":{\"35\":1}}],[\"所谓二维编码\",{\"1\":{\"15\":1}}],[\"所替代\",{\"1\":{\"14\":1}}],[\"所以s~z~的顺序是随机打乱的\",{\"1\":{\"14\":1}}],[\"所以masked掉多个字\",{\"1\":{\"14\":1}}],[\"所以效果不是很好\",{\"1\":{\"13\":1}}],[\"所以这里就用了masked自注意力机制\",{\"1\":{\"5\":1}}],[\"所以\",{\"1\":{\"5\":1,\"15\":1}}],[\"所以语言模型方向模型参数规模越来越大\",{\"1\":{\"1\":1}}],[\"~6~\",{\"1\":{\"14\":1}}],[\"~5~\",{\"1\":{\"14\":1}}],[\"~3~\",{\"1\":{\"14\":1}}],[\"~2~\",{\"1\":{\"14\":1}}],[\"~i\",{\"1\":{\"14\":2}}],[\"~都对应x中的连续token\",{\"1\":{\"14\":1}}],[\"~m~\",{\"1\":{\"14\":1}}],[\"~1~\",{\"1\":{\"14\":2}}],[\"~n~\",{\"1\":{\"14\":1}}],[\"xuanjing\",{\"1\":{\"20\":1}}],[\"xiang\",{\"1\":{\"20\":1}}],[\"xiaodong\",{\"1\":{\"20\":1}}],[\"xiao\",{\"1\":{\"20\":2}}],[\"x为输入\",{\"1\":{\"19\":1}}],[\"x~6~\",{\"1\":{\"14\":1}}],[\"x~1~\",{\"1\":{\"14\":2}}],[\"x\",{\"1\":{\"14\":4,\"19\":1,\"20\":1,\"34\":2}}],[\"下面有介绍\",{\"1\":{\"23\":1}}],[\"下面一节\",{\"1\":{\"16\":1}}],[\"下面\",{\"1\":{\"13\":1}}],[\"下面我们来介绍一下bert\",{\"1\":{\"6\":1}}],[\"下面我们来介绍自注意力机制\",{\"1\":{\"4\":1}}],[\"下面我们讨论位置编码\",{\"1\":{\"5\":1}}],[\"解码模型\",{\"1\":{\"13\":1}}],[\"编码\",{\"1\":{\"13\":1}}],[\"五\",{\"0\":{\"13\":1}}],[\"更明显的响应的准确性最为重要\",{\"1\":{\"34\":1}}],[\"更多的样本\",{\"1\":{\"35\":1}}],[\"更多的总结任务\",{\"1\":{\"26\":1}}],[\"更多内容可以阅读原论文\",{\"1\":{\"11\":1}}],[\"更长的上下文长度可以让模型处理更多信息\",{\"1\":{\"26\":1}}],[\"更新了数据混合\",{\"1\":{\"23\":1}}],[\"更准确地说\",{\"1\":{\"18\":1}}],[\"更是拥有了理解图像的能力\",{\"1\":{\"12\":1}}],[\"能力进一步增强\",{\"1\":{\"12\":1}}],[\"同样\",{\"1\":{\"12\":1,\"13\":1}}],[\"同样的解码过程来完成所有的nlp任务\",{\"1\":{\"11\":1}}],[\"同样的训练过程\",{\"1\":{\"11\":1}}],[\"同样的损失函数\",{\"1\":{\"11\":1}}],[\"转而采用transformer结构\",{\"1\":{\"12\":1}}],[\"来提高llama\",{\"1\":{\"23\":1}}],[\"来讨论gpt的基本结构\",{\"1\":{\"12\":1}}],[\"来计算概率的\",{\"1\":{\"9\":1}}],[\"说到gpt\",{\"1\":{\"12\":1}}],[\"本节报告了使用监督微调以及初始和迭代奖励建模和rlhf进行的实验和发现\",{\"1\":{\"30\":1}}],[\"本文介绍llama\",{\"1\":{\"22\":2}}],[\"本文对t5模型的介绍就到这里\",{\"1\":{\"11\":1}}],[\"本篇笔记对其内容进行简要记录\",{\"1\":{\"21\":1}}],[\"本篇文章将回到最初的gpt\",{\"1\":{\"12\":1}}],[\"本篇文章将顺着现代自然语言处理方法和模型的脉络\",{\"1\":{\"0\":1}}],[\"本篇文章中将简单对其进行介绍\",{\"1\":{\"10\":1}}],[\"本篇文章会对上述模型和技术进行简要介绍\",{\"1\":{\"1\":1}}],[\"本篇文章选择了清华大学的p\",{\"1\":{\"1\":1}}],[\"本篇文章对t5和gpt进行简要介绍\",{\"1\":{\"0\":1}}],[\"本篇文章对现代语言模型的学习的全过程\",{\"1\":{\"0\":1}}],[\"就是给个输入\",{\"1\":{\"33\":1}}],[\"就是transformer的encoder部分\",{\"1\":{\"7\":1}}],[\"就可以在rlhf中优化模型了\",{\"1\":{\"34\":1}}],[\"就可以用同样的模型\",{\"1\":{\"11\":1}}],[\"就可以将nlp任务都转成text\",{\"1\":{\"11\":1}}],[\"通过meta内部的安全和帮助奖励模型进行测量\",{\"1\":{\"38\":1}}],[\"通过上一节的介绍我们知道\",{\"1\":{\"15\":1}}],[\"通过这样的方式\",{\"1\":{\"11\":1}}],[\"通过以上几个部分\",{\"1\":{\"0\":1}}],[\"任务上\",{\"1\":{\"19\":1}}],[\"任务\",{\"1\":{\"11\":1}}],[\"任务都转化为文本到文本\",{\"1\":{\"11\":1}}],[\"任何两个时间步之间的距离应该保持一致\",{\"1\":{\"5\":1}}],[\"特别是\",{\"1\":{\"39\":1}}],[\"特别是gpt\",{\"1\":{\"10\":1}}],[\"特别是微调的能力\",{\"1\":{\"6\":1}}],[\"四个基于transformer的预训练语言模型也已经介绍完毕\",{\"1\":{\"16\":1}}],[\"四\",{\"0\":{\"10\":1}}],[\"取得了喜人的成绩\",{\"1\":{\"9\":1}}],[\"标签为notnext\",{\"1\":{\"9\":1}}],[\"标签为isnext\",{\"1\":{\"9\":1}}],[\"另外的的情况是b为语料中其它的随机句子\",{\"1\":{\"9\":1}}],[\"很多下游任务\",{\"1\":{\"9\":1}}],[\"用例如法语说话\",{\"1\":{\"36\":1}}],[\"用户和助手\",{\"1\":{\"36\":1}}],[\"用这个分数\",{\"1\":{\"34\":1}}],[\"用到的一些人类偏好开源数据集\",{\"1\":{\"33\":1}}],[\"用公开可获得的指令微调数据\",{\"1\":{\"31\":1}}],[\"用于帮助控制多轮对话流\",{\"1\":{\"30\":1}}],[\"用于将所有的自然语言处理\",{\"1\":{\"11\":1}}],[\"用\",{\"1\":{\"9\":1}}],[\"用来自动学习和计算输入数据对输出数据的贡献大小\",{\"1\":{\"4\":1}}],[\"可微调的参数多了\",{\"1\":{\"19\":1}}],[\"可能效果会更好一些\",{\"1\":{\"14\":1}}],[\"可能差别很大\",{\"1\":{\"14\":1}}],[\"可能是因为在中文中\",{\"1\":{\"14\":1}}],[\"可能会有一个问题\",{\"1\":{\"9\":1}}],[\"可以在该数据集上微调llama\",{\"1\":{\"36\":1}}],[\"可以在除第一个回合外的所有回合中放弃它\",{\"1\":{\"36\":1}}],[\"可以在类似于拒绝采样的过程中对模型进行微调\",{\"1\":{\"36\":1}}],[\"可以在多个头之间共享key和value预测\",{\"1\":{\"26\":1}}],[\"可以观察到\",{\"1\":{\"35\":1}}],[\"可以使用具有单个kv投影的原始多查询格式\",{\"1\":{\"26\":1}}],[\"可以适配到序列标注任务\",{\"1\":{\"19\":1}}],[\"可以让微调后的预训练模型在处理下游任务时得到良好的效果\",{\"1\":{\"17\":1}}],[\"可以让模型去关注不同方面的信息\",{\"1\":{\"5\":1}}],[\"可以体现出bert是双向模型\",{\"1\":{\"9\":1}}],[\"可以看出\",{\"1\":{\"7\":1}}],[\"可以看到我们给定的文本序列为\",{\"1\":{\"14\":1}}],[\"可以看到\",{\"1\":{\"5\":1}}],[\"全参数微调效果相对较好\",{\"1\":{\"17\":1}}],[\"全部把输入的token替换成\",{\"1\":{\"9\":1}}],[\"全能模型\",{\"1\":{\"1\":1}}],[\"证明了15\",{\"1\":{\"9\":1}}],[\"根据经验\",{\"1\":{\"9\":1}}],[\"既包括token左边的\",{\"1\":{\"9\":1}}],[\"坏\",{\"1\":{\"9\":1,\"18\":1}}],[\"还是\",{\"1\":{\"9\":1}}],[\"好\",{\"1\":{\"9\":1,\"18\":1}}],[\"遮住之后\",{\"1\":{\"9\":1}}],[\"呀\",{\"1\":{\"9\":1}}],[\"今天天气真\",{\"1\":{\"9\":1}}],[\"今天天气真好呀\",{\"1\":{\"9\":1}}],[\"预训练tokenizer\",{\"0\":{\"28\":1}}],[\"预训练超参设置\",{\"0\":{\"27\":1}}],[\"预训练的设置和模型架构和llama\",{\"1\":{\"25\":1}}],[\"预训练方法\",{\"0\":{\"23\":1}}],[\"预训练模型llama\",{\"1\":{\"22\":1}}],[\"预训练模型基本上属于这个范畴\",{\"1\":{\"11\":1}}],[\"预训练语言模型的方式只有全参数微调\",{\"1\":{\"17\":1}}],[\"预训练语言模型可以分为三种\",{\"1\":{\"13\":1}}],[\"预训练bert\",{\"0\":{\"9\":1}}],[\"预测\",{\"1\":{\"5\":1}}],[\"那么自然glm原生地支持p\",{\"1\":{\"19\":1}}],[\"那么p\",{\"1\":{\"19\":1}}],[\"那么什么是text\",{\"1\":{\"11\":1}}],[\"那么具体是怎么做的呢\",{\"1\":{\"9\":1}}],[\"那么对于任意一个序列的每一位置都将用同一个向量来进行表示\",{\"1\":{\"8\":1}}],[\"那么encoder会认为这两个序列完全相同\",{\"1\":{\"5\":1}}],[\"多轮一致性的指令\",{\"0\":{\"36\":1}}],[\"多了一个segment\",{\"1\":{\"7\":1}}],[\"多头注意力机制\",{\"1\":{\"5\":1}}],[\"多头注意力机制的公式如下\",{\"1\":{\"5\":1}}],[\"多头注意力机制就是将缩放点积注意力机制的过程做h次\",{\"1\":{\"5\":1}}],[\"多头注意力机制如图2\",{\"1\":{\"5\":1}}],[\"=\",{\"1\":{\"7\":8,\"15\":1,\"23\":2,\"31\":2}}],[\"图11报告了作者针对安全和帮助轴的不同sft和rlhf版本的进展\",{\"1\":{\"38\":1}}],[\"图6\",{\"1\":{\"19\":2}}],[\"图中含有色块部分\",{\"1\":{\"14\":1}}],[\"图5\",{\"1\":{\"14\":2,\"15\":1}}],[\"图4\",{\"1\":{\"11\":1,\"12\":1}}],[\"图3\",{\"1\":{\"7\":1,\"8\":1}}],[\"图2\",{\"1\":{\"3\":1,\"4\":2,\"5\":3}}],[\"自然也没有原生的表示位置信息的能力\",{\"1\":{\"15\":1}}],[\"自然语言处理中常见的翻译任务\",{\"1\":{\"11\":1}}],[\"自然语言推理等\",{\"1\":{\"9\":1}}],[\"自编码模型\",{\"1\":{\"13\":1}}],[\"自回归解码的标准做法是缓存序列中先前token的key\",{\"1\":{\"26\":1}}],[\"自回归填空\",{\"0\":{\"14\":1},\"1\":{\"14\":1}}],[\"自回归模型\",{\"1\":{\"13\":1}}],[\"自回归模型对自然语言生成有着天然的优势\",{\"1\":{\"12\":1}}],[\"自回归语言模型更加适合自然语言生成任务\",{\"1\":{\"6\":1}}],[\"自注意力机制示例\",{\"1\":{\"4\":1}}],[\"自注意力机制正符合这一点\",{\"1\":{\"4\":1}}],[\"自注意力机制得到了应用\",{\"1\":{\"4\":1}}],[\"自注意力机制是注意力机制的一种\",{\"1\":{\"4\":1}}],[\"或公众人物\",{\"1\":{\"36\":1}}],[\"或\",{\"1\":{\"36\":1}}],[\"或具有8kv投影的分组查询注意力\",{\"1\":{\"26\":1}}],[\"或者说单向模型和双向模型谁好谁坏\",{\"1\":{\"6\":1}}],[\"或卷积神经网络\",{\"1\":{\"3\":1}}],[\"当在10到100个输出之间采样时\",{\"1\":{\"35\":1}}],[\"当在两个相似的模型反应之间做出决定时\",{\"1\":{\"34\":1}}],[\"当作者在表8中按偏好评级对分数进行分组时\",{\"1\":{\"34\":1}}],[\"当然\",{\"1\":{\"6\":1,\"13\":1}}],[\"当前的token只能看到它和它之前的token\",{\"1\":{\"6\":1}}],[\"rl\",{\"1\":{\"35\":1}}],[\"rlhf结果\",{\"0\":{\"37\":1}}],[\"rlhf对重新缩放温度有直接影响\",{\"1\":{\"35\":1}}],[\"rlhf\",{\"0\":{\"32\":1},\"1\":{\"35\":2,\"38\":1}}],[\"r\",{\"1\":{\"34\":1}}],[\"rθ\",{\"1\":{\"34\":1}}],[\"rmsnorm\",{\"1\":{\"25\":1}}],[\"rfou\",{\"1\":{\"20\":1}}],[\"rating\",{\"1\":{\"34\":1}}],[\"rank\",{\"1\":{\"20\":1}}],[\"rami\",{\"1\":{\"20\":1}}],[\"radford\",{\"1\":{\"20\":1}}],[\"raffel\",{\"1\":{\"20\":1}}],[\"rejection\",{\"1\":{\"35\":1}}],[\"rejected\",{\"1\":{\"34\":1}}],[\"reward\",{\"1\":{\"34\":1}}],[\"residual\",{\"1\":{\"35\":1}}],[\"responses\",{\"1\":{\"34\":3}}],[\"response\",{\"1\":{\"34\":1}}],[\"res\",{\"1\":{\"20\":1}}],[\"representations\",{\"1\":{\"6\":1}}],[\"rotary\",{\"1\":{\"25\":1}}],[\"rope\",{\"1\":{\"25\":1}}],[\"robustly\",{\"1\":{\"20\":1}}],[\"roberta\",{\"1\":{\"20\":1}}],[\"roberts\",{\"1\":{\"20\":1}}],[\"rorerta\",{\"1\":{\"13\":1}}],[\"rnn\",{\"1\":{\"3\":1}}],[\"但可以说\",{\"1\":{\"38\":1}}],[\"但这会导致系统消息\",{\"1\":{\"36\":1}}],[\"但在生成过程中会导致很大的减慢\",{\"1\":{\"35\":1}}],[\"但并不是一个实际的语言模型\",{\"1\":{\"6\":1}}],[\"但是其性能并没有p\",{\"1\":{\"19\":1}}],[\"但是其性能仍然不如全参数微调\",{\"1\":{\"17\":1}}],[\"但是其自然语言理解能力非常强\",{\"1\":{\"1\":1}}],[\"但是全参数微调的设备需求仍然很大\",{\"1\":{\"17\":1}}],[\"但是都有自己的创新点\",{\"1\":{\"16\":1}}],[\"但是glm没有这样做\",{\"1\":{\"14\":1}}],[\"但是在当时\",{\"1\":{\"7\":1}}],[\"但是在解码阶段\",{\"1\":{\"5\":1}}],[\"但是单纯的multi\",{\"1\":{\"5\":1}}],[\"但是\",{\"1\":{\"5\":1,\"16\":1,\"17\":1}}],[\"但是微调参数量仅为全参数微调的3\",{\"1\":{\"1\":1}}],[\"但是最大的t5模型达到了11b\",{\"1\":{\"1\":1}}],[\"但是语言模型的研究并不是最近才兴起的\",{\"1\":{\"1\":1}}],[\"三部分构成\",{\"1\":{\"19\":1}}],[\"三\",{\"0\":{\"6\":1}}],[\"至此\",{\"1\":{\"5\":1,\"16\":1}}],[\"组织模型看到将要预测的信息\",{\"1\":{\"5\":1}}],[\"单向\",{\"1\":{\"5\":1}}],[\"也只能够通过位置编码的形式来获取位置信息\",{\"1\":{\"15\":1}}],[\"也许通过文字描述比较难以理解\",{\"1\":{\"14\":1}}],[\"也有研究人员尝试将上述三种模型结合\",{\"1\":{\"13\":1}}],[\"也包括token右边的\",{\"1\":{\"9\":1}}],[\"也就是单向的\",{\"1\":{\"14\":1}}],[\"也就是在这里\",{\"1\":{\"9\":1}}],[\"也就是某个词既可以看到它之后的词\",{\"1\":{\"5\":1}}],[\"也是bert的核心\",{\"1\":{\"7\":1}}],[\"也可以让结果很好\",{\"1\":{\"31\":1}}],[\"也可以看到它之前的词\",{\"1\":{\"5\":1}}],[\"也可以通过公式计算得到\",{\"1\":{\"5\":1}}],[\"什么是masked\",{\"1\":{\"5\":1}}],[\"与没有gatt的模型\",{\"1\":{\"36\":1}}],[\"与样本在训练时间不匹配\",{\"1\":{\"36\":1}}],[\"与discriminative\",{\"1\":{\"35\":1}}],[\"与相似对相比\",{\"1\":{\"34\":1}}],[\"与其他方案相比\",{\"1\":{\"33\":1}}],[\"与多头注意力\",{\"1\":{\"26\":1}}],[\"与llama\",{\"1\":{\"25\":1,\"28\":1}}],[\"与glm一样\",{\"1\":{\"19\":1}}],[\"与\",{\"1\":{\"18\":1}}],[\"与transformer和bert的一维位置编码不同\",{\"1\":{\"15\":1}}],[\"与bert不同的是\",{\"1\":{\"12\":1}}],[\"与encoder一致\",{\"1\":{\"5\":1}}],[\"与传统注意力机制完全相同\",{\"1\":{\"4\":1}}],[\"第二层为一个线性变换\",{\"1\":{\"5\":1}}],[\"第一层有一个relu激活函数\",{\"1\":{\"5\":1}}],[\"为此\",{\"1\":{\"34\":1}}],[\"为效果最好的\",{\"1\":{\"14\":1}}],[\"为了评估主要模型版本的质量\",{\"1\":{\"39\":1}}],[\"为了进行公平的比较\",{\"1\":{\"38\":1}}],[\"为了说明gatt如何在微调过程中帮助重塑注意力\",{\"1\":{\"36\":1}}],[\"为了使指令更加复杂和多样化\",{\"1\":{\"36\":1}}],[\"为了获得兴趣爱好和公众人物的列表\",{\"1\":{\"36\":1}}],[\"为了解决这个可能影响训练的问题\",{\"1\":{\"36\":1}}],[\"为了解决这个问题\",{\"1\":{\"9\":2}}],[\"为了解决这些限制\",{\"1\":{\"36\":1}}],[\"为了快速进行大批量训练\",{\"1\":{\"35\":1}}],[\"为了训练奖励模型\",{\"1\":{\"34\":1}}],[\"为了最大限度地提高多样性\",{\"1\":{\"33\":1}}],[\"为了确保模型序列长度正确填充\",{\"1\":{\"31\":1}}],[\"为了减少微调的设备等资源的消耗\",{\"1\":{\"17\":1}}],[\"为了适应下游任务\",{\"1\":{\"16\":1}}],[\"为了抵消这个效果\",{\"1\":{\"5\":1}}],[\"为bert模型的结构图\",{\"1\":{\"7\":1}}],[\"为一个两层的多层感知机\",{\"1\":{\"5\":1}}],[\"再把输出合并起来\",{\"1\":{\"5\":1}}],[\"形成多个子空间\",{\"1\":{\"5\":1}}],[\"缩放点击注意力机制会进行一个缩放\",{\"1\":{\"5\":1}}],[\"缩放点积注意力机制在做完query和key的点积之后\",{\"1\":{\"5\":1}}],[\"会有更多更好的语言模型诞生\",{\"1\":{\"16\":1}}],[\"会导致query和key的点积非常大\",{\"1\":{\"5\":1}}],[\"会进行一个缩放\",{\"1\":{\"5\":1}}],[\"会计算残差和\",{\"1\":{\"5\":1}}],[\"并对延迟的提示进行了评估\",{\"1\":{\"35\":1}}],[\"并用我们的奖励选择最佳候选者\",{\"1\":{\"35\":1}}],[\"并用相似度与key所对应的value做矩阵运算并求和\",{\"1\":{\"4\":1}}],[\"并随着比较对变得更加相似而逐渐退化\",{\"1\":{\"34\":1}}],[\"并强制选择的响应比对应的响应具有更高的分数\",{\"1\":{\"34\":1}}],[\"并改变温度超参数\",{\"1\":{\"33\":1}}],[\"并从用户提示中消除令牌的损失\",{\"1\":{\"31\":1}}],[\"并使用字节分解未知的utf\",{\"1\":{\"28\":1}}],[\"并使用分组查询注意力\",{\"1\":{\"23\":1}}],[\"并使用自然语言提示来查询语言模型\",{\"1\":{\"18\":1}}],[\"并不会出现\",{\"1\":{\"9\":1}}],[\"并且每个小批量采取一个梯度步骤\",{\"1\":{\"35\":1}}],[\"并且总是从每个新rlhf版本的基本模型开始\",{\"1\":{\"35\":1}}],[\"并且采用了分组查询注意力\",{\"1\":{\"22\":1}}],[\"并且更推荐使用p\",{\"1\":{\"19\":1}}],[\"并且其资源消耗极低\",{\"1\":{\"17\":1}}],[\"并且做了以下修改\",{\"1\":{\"16\":1}}],[\"并且相对transformer来说\",{\"1\":{\"7\":1}}],[\"并且和transformer有关\",{\"1\":{\"6\":1}}],[\"并正规化\",{\"1\":{\"5\":1}}],[\"a1\",{\"1\":{\"36\":1}}],[\"a100\",{\"1\":{\"19\":2}}],[\"a100服务器\",{\"1\":{\"17\":1}}],[\"abstractive\",{\"1\":{\"35\":1}}],[\"ablation\",{\"1\":{\"34\":1}}],[\"appendix\",{\"1\":{\"34\":1}}],[\"approach\",{\"1\":{\"20\":1}}],[\"act\",{\"1\":{\"36\":1}}],[\"accuracy\",{\"1\":{\"34\":1}}],[\"across\",{\"1\":{\"20\":1}}],[\"as\",{\"1\":{\"36\":1}}],[\"association\",{\"1\":{\"20\":2}}],[\"ashish\",{\"1\":{\"20\":1}}],[\"an\",{\"1\":{\"36\":1}}],[\"analysis\",{\"1\":{\"34\":1}}],[\"annotators\",{\"1\":{\"34\":1}}],[\"annual\",{\"1\":{\"20\":1}}],[\"ankur\",{\"1\":{\"20\":1}}],[\"and\",{\"1\":{\"20\":13,\"34\":4}}],[\"arxiv\",{\"1\":{\"20\":8}}],[\"are\",{\"1\":{\"20\":1,\"34\":1}}],[\"al\",{\"1\":{\"20\":2}}],[\"alec\",{\"1\":{\"20\":1}}],[\"allen\",{\"1\":{\"20\":1}}],[\"all\",{\"1\":{\"20\":2}}],[\"adversarial\",{\"1\":{\"35\":1}}],[\"advances\",{\"1\":{\"20\":1}}],[\"adaptation\",{\"1\":{\"20\":1}}],[\"adamw优化器\",{\"1\":{\"27\":1}}],[\"adam\",{\"1\":{\"20\":1}}],[\"add\",{\"1\":{\"5\":1}}],[\"ai\",{\"1\":{\"35\":2,\"36\":2}}],[\"aidan\",{\"1\":{\"20\":1}}],[\"ai的能力在逐步增强\",{\"1\":{\"12\":1}}],[\"a\",{\"0\":{\"26\":1},\"1\":{\"14\":1,\"20\":4,\"34\":4}}],[\"autoregressive\",{\"1\":{\"14\":1}}],[\"autoregression\",{\"1\":{\"6\":2}}],[\"autoencoder\",{\"1\":{\"6\":1}}],[\"attention和multi\",{\"1\":{\"12\":1}}],[\"attention和上述的一致\",{\"1\":{\"5\":1}}],[\"attention的k\",{\"1\":{\"5\":1}}],[\"attention是双向的\",{\"1\":{\"5\":1}}],[\"attention\",{\"1\":{\"3\":1,\"4\":1,\"5\":4,\"12\":2,\"20\":2,\"26\":1,\"30\":1,\"36\":1}}],[\"构成\",{\"1\":{\"5\":2}}],[\"falcon\",{\"1\":{\"39\":1}}],[\"fsdp\",{\"1\":{\"35\":1}}],[\"found\",{\"1\":{\"34\":2}}],[\"for\",{\"1\":{\"20\":6,\"34\":3,\"35\":2}}],[\"forward\",{\"1\":{\"5\":1}}],[\"fully\",{\"1\":{\"35\":1}}],[\"function\",{\"1\":{\"34\":1}}],[\"furu\",{\"1\":{\"20\":1}}],[\"francine\",{\"1\":{\"20\":1}}],[\"framework\",{\"1\":{\"20\":1}}],[\"from\",{\"1\":{\"6\":1,\"35\":1,\"36\":1}}],[\"finetuned\",{\"1\":{\"31\":1}}],[\"fine\",{\"0\":{\"30\":1,\"31\":1},\"1\":{\"17\":1,\"20\":1,\"35\":1}}],[\"feedback启发的非常简单的方法\",{\"1\":{\"36\":1}}],[\"feedback保持一致\",{\"1\":{\"35\":1}}],[\"feed\",{\"1\":{\"5\":1}}],[\"harmlessness\",{\"1\":{\"35\":1,\"36\":1}}],[\"hangbo\",{\"1\":{\"20\":1}}],[\"helpfulness\",{\"1\":{\"34\":2}}],[\"head\",{\"1\":{\"5\":6}}],[\"hon\",{\"1\":{\"20\":1}}],[\"hot\",{\"1\":{\"5\":1}}],[\"hsiao\",{\"1\":{\"20\":1}}],[\"hu\",{\"1\":{\"20\":1}}],[\"huang\",{\"1\":{\"20\":1}}],[\"html\",{\"1\":{\"20\":1}}],[\"http\",{\"1\":{\"20\":1}}],[\"h\",{\"1\":{\"7\":2}}],[\"由于作者应用了迭代模型更新\",{\"1\":{\"35\":1}}],[\"由于注释者的主观性和他们对可能区分反应的细微细节的依赖\",{\"1\":{\"34\":1}}],[\"由于bert的主要目的是构建一个通用的预训练模型\",{\"1\":{\"8\":1}}],[\"由\",{\"1\":{\"19\":1}}],[\"由一个多头注意力机制\",{\"1\":{\"5\":1}}],[\"由query和key做向量比对\",{\"1\":{\"4\":1}}],[\"对\",{\"1\":{\"26\":1}}],[\"对总token进行了40\",{\"1\":{\"23\":1}}],[\"对glm\",{\"1\":{\"19\":1}}],[\"对于palm\",{\"1\":{\"39\":1}}],[\"对于position\",{\"1\":{\"15\":1}}],[\"对于chatgpt\",{\"1\":{\"39\":1}}],[\"对于训练指令\",{\"1\":{\"36\":1}}],[\"对于7b和13b模型\",{\"1\":{\"35\":1}}],[\"对于所有模型\",{\"1\":{\"35\":1}}],[\"对于llama\",{\"1\":{\"35\":1}}],[\"对于每个ppo迭代\",{\"1\":{\"35\":1}}],[\"对于每个prompt\",{\"1\":{\"35\":1}}],[\"对于每个模型\",{\"1\":{\"35\":1}}],[\"对于每个样本来说都是由a和b两句话构成\",{\"1\":{\"9\":1}}],[\"对于提高llama\",{\"1\":{\"34\":1}}],[\"对于微调过程\",{\"1\":{\"31\":1}}],[\"对于监督微调\",{\"1\":{\"31\":1}}],[\"对于34b和70b模型\",{\"1\":{\"35\":1}}],[\"对于34b和70b\",{\"1\":{\"26\":1}}],[\"对于kv缓存大小成为瓶颈的大型模型\",{\"1\":{\"26\":1}}],[\"对于情感分析问题\",{\"1\":{\"18\":1}}],[\"对于glm\",{\"1\":{\"17\":1}}],[\"对于预训练好的语言模型\",{\"1\":{\"16\":1}}],[\"对于token的预测输出用的是单个的线形层\",{\"1\":{\"16\":1}}],[\"对于masked掉的词\",{\"1\":{\"14\":1}}],[\"对于相对位置的计算更加方便\",{\"1\":{\"5\":1}}],[\"对input\",{\"1\":{\"8\":1}}],[\"对输入\",{\"1\":{\"5\":1}}],[\"采用了多任务学习优化\",{\"1\":{\"19\":1}}],[\"采用了注意力机制\",{\"1\":{\"5\":1}}],[\"采用三角函数来作为位置编码公式\",{\"1\":{\"5\":1}}],[\"最佳温度为t∈\",{\"1\":{\"35\":1}}],[\"最佳温度不是恒定的\",{\"1\":{\"35\":1}}],[\"最大的llama\",{\"1\":{\"39\":1}}],[\"最大的glm参数量已经达到了130b\",{\"1\":{\"1\":1}}],[\"最大曲线和中值曲线之间的增量可以解释为对最佳输出进行微调的潜在增益\",{\"1\":{\"35\":1}}],[\"最少5\",{\"1\":{\"34\":1}}],[\"最多支持4k\",{\"1\":{\"22\":1}}],[\"最终的input\",{\"1\":{\"8\":1}}],[\"最后\",{\"1\":{\"5\":1,\"31\":1}}],[\"此外\",{\"1\":{\"5\":1,\"8\":1,\"19\":1}}],[\"模型的进展\",{\"1\":{\"38\":1}}],[\"模型中的kv缓存大小相关的内存成本显著增长\",{\"1\":{\"26\":1}}],[\"模型\",{\"1\":{\"6\":1}}],[\"模型和自编码\",{\"1\":{\"6\":1}}],[\"模型应该是双向的才好\",{\"1\":{\"6\":1}}],[\"模型应该能毫不费力地泛化更长的句子\",{\"1\":{\"5\":1}}],[\"模型就失去了\",{\"1\":{\"5\":1}}],[\"模型需要做的是通过已经有的信息来预测下一个位置会出现什么\",{\"1\":{\"5\":1}}],[\"模型一般包括一个编码器\",{\"1\":{\"3\":1}}],[\"不确定\",{\"1\":{\"33\":1}}],[\"不同于之前研究的简单结合\",{\"1\":{\"13\":1}}],[\"不同长度的句子之间\",{\"1\":{\"5\":1}}],[\"不变\",{\"1\":{\"9\":1,\"14\":1}}],[\"不过parta是看不到partb的\",{\"1\":{\"14\":1}}],[\"不过考虑到span之间可能也有关系\",{\"1\":{\"14\":1}}],[\"不过看到它只用了z<i的部分\",{\"1\":{\"14\":1}}],[\"不过在当时的条件下并没有很出色的能力\",{\"1\":{\"13\":1}}],[\"不过因为自回归和自编码在模型结构上相差太多\",{\"1\":{\"13\":1}}],[\"不过与bert\",{\"1\":{\"12\":1,\"16\":1}}],[\"不过\",{\"1\":{\"9\":1}}],[\"不过它们仍然没有脱离rnn或者cnn\",{\"1\":{\"3\":1}}],[\"它可能偏向于llama\",{\"1\":{\"38\":1}}],[\"它可以破解微调数据\",{\"1\":{\"36\":1}}],[\"它采用了字节对编码\",{\"1\":{\"28\":1}}],[\"它不仅只在embedding层进行微调\",{\"1\":{\"19\":1}}],[\"它是t5提出的一个统一框架\",{\"1\":{\"11\":1}}],[\"它是注意力机制的一种\",{\"1\":{\"5\":1}}],[\"它必须是确定性的\",{\"1\":{\"5\":1}}],[\"它的值应该是有界的\",{\"1\":{\"5\":1}}],[\"它能为每个时间步输出一个独一无二的编码\",{\"1\":{\"5\":1}}],[\"它们均基于prefix\",{\"1\":{\"17\":1}}],[\"它们整体相似\",{\"1\":{\"16\":1}}],[\"它们各有各的擅长之处\",{\"1\":{\"13\":1}}],[\"它们各有优缺点\",{\"1\":{\"6\":1}}],[\"它们分别代表query\",{\"1\":{\"4\":1}}],[\"它们都足够复杂\",{\"1\":{\"3\":1}}],[\"选择一种合适的方式表示词在序列中的顺序非常重要\",{\"1\":{\"5\":1}}],[\"只需将前几轮中的所有token\",{\"1\":{\"36\":1}}],[\"只需要存储一份模型参数\",{\"1\":{\"18\":1}}],[\"只需要在给模型输入的部分加上前缀\",{\"1\":{\"11\":1}}],[\"只有第一层embedding才需要被微调\",{\"1\":{\"19\":1}}],[\"只不过\",{\"1\":{\"14\":1}}],[\"只不过是某些词的顺序不同\",{\"1\":{\"5\":1}}],[\"只保留了masked\",{\"1\":{\"12\":1}}],[\"只依赖于注意力机制\",{\"1\":{\"3\":1}}],[\"output\",{\"1\":{\"34\":1}}],[\"optimization\",{\"1\":{\"35\":1}}],[\"optimized\",{\"1\":{\"20\":1}}],[\"optimizing\",{\"1\":{\"20\":1}}],[\"omer\",{\"1\":{\"20\":1}}],[\"ott\",{\"1\":{\"20\":1}}],[\"oscar\",{\"1\":{\"20\":1,\"36\":1}}],[\"on\",{\"1\":{\"20\":1,\"34\":1,\"35\":1}}],[\"one\",{\"1\":{\"5\":1,\"34\":1}}],[\"org\",{\"1\":{\"20\":1}}],[\"of\",{\"1\":{\"5\":2,\"20\":6,\"34\":1}}],[\"lr\",{\"1\":{\"35\":1}}],[\"lr=2e\",{\"1\":{\"31\":1}}],[\"llama\",{\"0\":{\"21\":1},\"1\":{\"22\":4,\"23\":4,\"24\":1,\"26\":2,\"30\":1,\"34\":3,\"35\":1,\"36\":1,\"39\":5}}],[\"llion\",{\"1\":{\"20\":1}}],[\"low\",{\"1\":{\"20\":1}}],[\"lora在文生图领域应用广泛\",{\"1\":{\"17\":1}}],[\"lora同样也是一种部分参数微调的方法\",{\"1\":{\"17\":1}}],[\"lora\",{\"1\":{\"17\":1,\"20\":1}}],[\"l\",{\"0\":{\"40\":2},\"1\":{\"20\":1}}],[\"luke\",{\"1\":{\"20\":1}}],[\"lukasz\",{\"1\":{\"20\":1}}],[\"lucia\",{\"1\":{\"20\":1}}],[\"lenght\",{\"1\":{\"31\":1}}],[\"lewis\",{\"1\":{\"20\":1}}],[\"levy\",{\"1\":{\"20\":1}}],[\"lester\",{\"1\":{\"20\":1}}],[\"learn\",{\"1\":{\"20\":1}}],[\"learning\",{\"1\":{\"11\":1,\"20\":1}}],[\"lee\",{\"1\":{\"20\":2}}],[\"lm\",{\"1\":{\"19\":1}}],[\"line\",{\"1\":{\"34\":1}}],[\"linguistics\",{\"1\":{\"20\":2}}],[\"liang\",{\"1\":{\"20\":1}}],[\"lisa\",{\"1\":{\"20\":1}}],[\"limits\",{\"1\":{\"20\":1}}],[\"liu\",{\"1\":{\"20\":6}}],[\"li\",{\"1\":{\"20\":4}}],[\"li~\",{\"1\":{\"14\":1}}],[\"like\",{\"1\":{\"5\":4}}],[\"large的参数规模是340m\",{\"1\":{\"7\":1}}],[\"large\",{\"1\":{\"7\":3,\"20\":1,\"34\":1}}],[\"language\",{\"1\":{\"1\":1,\"9\":1,\"13\":1,\"14\":1,\"20\":7,\"26\":1,\"31\":1}}],[\"n个样本\",{\"1\":{\"35\":1}}],[\"nan\",{\"1\":{\"20\":1}}],[\"naman\",{\"1\":{\"20\":1}}],[\"naturally\",{\"1\":{\"34\":1}}],[\"natural\",{\"1\":{\"20\":2}}],[\"narasimhan\",{\"1\":{\"20\":1}}],[\"narang\",{\"1\":{\"20\":1}}],[\"naacl\",{\"1\":{\"20\":1}}],[\"neural\",{\"1\":{\"20\":1}}],[\"need\",{\"1\":{\"20\":1}}],[\"next\",{\"1\":{\"9\":1}}],[\"niki\",{\"1\":{\"20\":1}}],[\"nlg\",{\"1\":{\"19\":1}}],[\"nlu\",{\"1\":{\"19\":1}}],[\"nlp\",{\"1\":{\"11\":1,\"20\":1}}],[\"nlp经历了什么\",{\"0\":{\"0\":1}}],[\"nsp的位置在bert模型图中有所体现\",{\"1\":{\"9\":1}}],[\"nsp是一个二分类下句预测任务\",{\"1\":{\"9\":1}}],[\"n\",{\"1\":{\"7\":2,\"20\":1}}],[\"noah\",{\"1\":{\"20\":1}}],[\"noam\",{\"1\":{\"20\":2}}],[\"norm\",{\"1\":{\"5\":1}}],[\"not\",{\"1\":{\"5\":2}}],[\"n=6\",{\"1\":{\"5\":1}}],[\"data\",{\"1\":{\"35\":1}}],[\"danqi\",{\"1\":{\"20\":1}}],[\"das\",{\"1\":{\"20\":1}}],[\"distinct\",{\"1\":{\"34\":1}}],[\"discrete\",{\"1\":{\"34\":1}}],[\"dipanjan\",{\"1\":{\"20\":1}}],[\"ding\",{\"1\":{\"20\":2}}],[\"du\",{\"1\":{\"20\":3}}],[\"decay\",{\"1\":{\"35\":1}}],[\"decomposable\",{\"1\":{\"20\":1}}],[\"decoder结构\",{\"1\":{\"12\":1}}],[\"decoder中的multi\",{\"1\":{\"5\":1}}],[\"decoder中decoder单元数n=6\",{\"1\":{\"5\":1}}],[\"decoder和encoder最大的区别\",{\"1\":{\"5\":1}}],[\"decoder模块等\",{\"1\":{\"5\":1}}],[\"decoder\",{\"1\":{\"3\":1,\"5\":1}}],[\"detailed\",{\"1\":{\"34\":1}}],[\"deep\",{\"1\":{\"20\":1}}],[\"devlin\",{\"1\":{\"20\":1}}],[\"d\",{\"1\":{\"5\":1,\"7\":3}}],[\"dong\",{\"1\":{\"20\":1}}],[\"dot\",{\"1\":{\"5\":1}}],[\"do\",{\"1\":{\"5\":4}}],[\"improve\",{\"1\":{\"34\":1}}],[\"improving\",{\"1\":{\"20\":1}}],[\"icml\",{\"1\":{\"20\":1}}],[\"ilya\",{\"1\":{\"20\":1}}],[\"illia\",{\"1\":{\"20\":1}}],[\"is\",{\"1\":{\"20\":1,\"34\":4}}],[\"inst可以是\",{\"1\":{\"36\":1}}],[\"inst\",{\"1\":{\"36\":1}}],[\"instruction\",{\"1\":{\"31\":1}}],[\"information\",{\"1\":{\"20\":1}}],[\"infilling\",{\"1\":{\"14\":1}}],[\"in\",{\"1\":{\"20\":9,\"34\":3}}],[\"input\",{\"0\":{\"8\":1},\"1\":{\"8\":1,\"9\":1}}],[\"i\",{\"1\":{\"5\":4}}],[\"比如自回归模型擅长自然语言生成任务\",{\"1\":{\"13\":1}}],[\"比如问题回答\",{\"1\":{\"9\":1}}],[\"比如\",{\"1\":{\"5\":1,\"6\":1,\"9\":1,\"11\":1,\"17\":1,\"18\":1}}],[\"比如百度的文心\",{\"1\":{\"1\":1}}],[\"序列的顺序中往往蕴含着一些重要信息\",{\"1\":{\"5\":1}}],[\"序列转换模型一般由循环神经网络\",{\"1\":{\"3\":1}}],[\"序列转换模型并不是transformer首次提出的\",{\"1\":{\"3\":1}}],[\"序列转换模型\",{\"0\":{\"3\":1},\"1\":{\"3\":1}}],[\"如第3\",{\"1\":{\"34\":1}}],[\"如下图左侧\",{\"1\":{\"36\":1}}],[\"如下图\",{\"1\":{\"31\":1}}],[\"如\",{\"1\":{\"14\":1}}],[\"如今\",{\"1\":{\"12\":1}}],[\"如图9\",{\"1\":{\"36\":1}}],[\"如图6\",{\"1\":{\"19\":2}}],[\"如图5\",{\"1\":{\"14\":1,\"15\":1}}],[\"如图4\",{\"1\":{\"12\":1}}],[\"如图3\",{\"1\":{\"7\":1}}],[\"如图2\",{\"1\":{\"3\":1,\"4\":2}}],[\"如果position\",{\"1\":{\"15\":1}}],[\"如果这15\",{\"1\":{\"9\":1}}],[\"如果此时模型知道了某个词之后的信息\",{\"1\":{\"5\":1}}],[\"如果不考虑词在序列中的位置\",{\"1\":{\"5\":1}}],[\"如果不经过特殊处理\",{\"1\":{\"5\":1}}],[\"拥有更加优秀的能力\",{\"1\":{\"5\":1}}],[\"已经是广为应用的做法\",{\"1\":{\"5\":1}}],[\"experiences\",{\"1\":{\"35\":1}}],[\"exploring\",{\"1\":{\"20\":1}}],[\"eps=10−5\",{\"1\":{\"35\":1}}],[\"eps=10e\",{\"1\":{\"27\":1}}],[\"energy\",{\"1\":{\"35\":1}}],[\"encoder单元中的另一个部分是一个前馈网络\",{\"1\":{\"5\":1}}],[\"encoder结构图\",{\"1\":{\"5\":1}}],[\"encoder部分由n个encoder单元构成\",{\"1\":{\"5\":1}}],[\"encoder模块\",{\"1\":{\"5\":1}}],[\"encoder\",{\"1\":{\"3\":1,\"5\":1,\"6\":1}}],[\"especially\",{\"1\":{\"34\":1}}],[\"eds\",{\"1\":{\"20\":1}}],[\"emnlp\",{\"1\":{\"20\":1}}],[\"empirical\",{\"1\":{\"20\":2}}],[\"embeddings\",{\"1\":{\"25\":1}}],[\"embedding的作用便是用来区分输入序列中的不同序列\",{\"1\":{\"8\":1}}],[\"embedding部分做了一些修改\",{\"1\":{\"8\":1}}],[\"embedding部分有了一些调整\",{\"1\":{\"7\":1}}],[\"embedding也有调整\",{\"1\":{\"7\":1,\"8\":1}}],[\"embedding\",{\"0\":{\"8\":1},\"1\":{\"5\":1,\"7\":1,\"8\":3}}],[\"efficient\",{\"1\":{\"20\":1}}],[\"et\",{\"1\":{\"20\":1}}],[\"e\",{\"1\":{\"13\":3,\"20\":2}}],[\"输出进行向量化\",{\"1\":{\"5\":1}}],[\"位置编码采用了预训练的方式训练而得\",{\"1\":{\"15\":1}}],[\"位置编码采用了三角函数计算的方式得到\",{\"1\":{\"15\":1}}],[\"位置编码可以通过训练得到\",{\"1\":{\"5\":1}}],[\"位置编码\",{\"1\":{\"5\":2}}],[\"因为更高的温度使作者能够对更多样的输出进行采样\",{\"1\":{\"35\":1}}],[\"因为最大值增加\",{\"1\":{\"35\":1}}],[\"因为gelus效果更好\",{\"1\":{\"16\":1}}],[\"因为在微调的过程中\",{\"1\":{\"9\":1}}],[\"因为这里的上下文\",{\"1\":{\"9\":1}}],[\"因为三角函数具有周期性\",{\"1\":{\"5\":1}}],[\"因为transformer抛弃了rnn和cnn\",{\"1\":{\"5\":1}}],[\"因为transformer需要判断序列中词与词之间的关系强度\",{\"1\":{\"4\":1}}],[\"因此segment\",{\"1\":{\"8\":1}}],[\"因此难免需要兼顾到各种nlp任务场景下的输入\",{\"1\":{\"8\":1}}],[\"因此\",{\"1\":{\"1\":1,\"5\":1,\"6\":1,\"31\":1,\"35\":1,\"38\":1}}],[\"注意力集中在上述公式中source的内部元素\",{\"1\":{\"4\":1}}],[\"注意力机制体现在模型中是多头注意力机制\",{\"1\":{\"7\":1}}],[\"注意力机制公式如下\",{\"1\":{\"4\":1}}],[\"注意力机制就是给定一个query\",{\"1\":{\"4\":1}}],[\"注意力机制\",{\"0\":{\"4\":1},\"1\":{\"4\":2}}],[\"得到attention\",{\"1\":{\"4\":1}}],[\"得到query和key的相似度\",{\"1\":{\"4\":1}}],[\"然后可以将此指令综合连接到会话的所有用户消息\",{\"1\":{\"36\":1}}],[\"然后定义了一个指令\",{\"1\":{\"36\":1}}],[\"然后在生成之后释放内存\",{\"1\":{\"35\":1}}],[\"然后为给定提示选择最佳答案\",{\"1\":{\"35\":1}}],[\"然后作者在新的一组排序样本上微调模型\",{\"1\":{\"35\":1}}],[\"然后输出一个分数来表明这个结果的质量\",{\"1\":{\"34\":1}}],[\"然后有两种输出\",{\"1\":{\"33\":1}}],[\"然后根据提供的标准在两个采样的模型响应之间进行选择\",{\"1\":{\"33\":1}}],[\"然后再适配下游任务\",{\"1\":{\"19\":1}}],[\"然后再加上要翻译的内容即可\",{\"1\":{\"11\":1}}],[\"然后\",{\"1\":{\"18\":1}}],[\"然后模型来预测b是否为a的下一句话\",{\"1\":{\"9\":1}}],[\"然后归一化相似度\",{\"1\":{\"4\":1}}],[\"然而无论是rnn也好\",{\"1\":{\"3\":1}}],[\"然而\",{\"1\":{\"1\":1,\"26\":1,\"35\":1,\"36\":1}}],[\"首先就是要收集大量的高质量sft数据\",{\"1\":{\"31\":1}}],[\"首先\",{\"1\":{\"4\":1,\"5\":1}}],[\"计算attention\",{\"1\":{\"4\":1}}],[\"实际上\",{\"1\":{\"4\":1}}],[\"sampling\",{\"1\":{\"35\":1}}],[\"samples\",{\"1\":{\"34\":1}}],[\"safety\",{\"1\":{\"34\":1}}],[\"salimans\",{\"1\":{\"20\":1}}],[\"similar\",{\"1\":{\"34\":1}}],[\"size\",{\"1\":{\"31\":1,\"35\":2}}],[\"size的增加\",{\"1\":{\"26\":1}}],[\"smaller\",{\"1\":{\"34\":1}}],[\"sft\",{\"0\":{\"31\":1},\"1\":{\"35\":1}}],[\"summarization类似\",{\"1\":{\"35\":1}}],[\"supervised\",{\"0\":{\"31\":1}}],[\"sutskever\",{\"1\":{\"20\":1}}],[\"step\",{\"1\":{\"35\":1}}],[\"steps\",{\"1\":{\"27\":1,\"34\":1}}],[\"stoyanov\",{\"1\":{\"20\":1}}],[\"story\",{\"1\":{\"5\":2}}],[\"swiglu\",{\"1\":{\"25\":1}}],[\"songhao\",{\"1\":{\"20\":1}}],[\"shown\",{\"1\":{\"34\":1}}],[\"shen\",{\"1\":{\"20\":1}}],[\"sharded\",{\"1\":{\"35\":1}}],[\"sharan\",{\"1\":{\"20\":1}}],[\"shazeer\",{\"1\":{\"20\":2}}],[\"specia\",{\"1\":{\"20\":1}}],[\"systems\",{\"1\":{\"20\":1}}],[\"s~z<i~指的是partb的部分\",{\"1\":{\"14\":1}}],[\"s~2~对应着\",{\"1\":{\"14\":1}}],[\"s\",{\"1\":{\"14\":6,\"20\":1}}],[\"search\",{\"1\":{\"35\":1}}],[\"separable\",{\"1\":{\"34\":1}}],[\"self\",{\"1\":{\"12\":3}}],[\"sentence\",{\"1\":{\"9\":1}}],[\"seq2seq\",{\"1\":{\"3\":1}}],[\"sequence\",{\"1\":{\"3\":2,\"31\":1}}],[\"scalar\",{\"1\":{\"34\":1}}],[\"scaling\",{\"1\":{\"31\":1,\"35\":1}}],[\"scale\",{\"1\":{\"20\":1}}],[\"scales\",{\"1\":{\"20\":1}}],[\"scaled\",{\"1\":{\"5\":1}}],[\"scott\",{\"1\":{\"20\":1}}],[\"score的过程如下\",{\"1\":{\"4\":1}}],[\"score\",{\"1\":{\"4\":2,\"34\":1}}],[\"从现在起始终充当拿破仑\",{\"1\":{\"36\":1}}],[\"从两个不同的模型变量中对给定提示的两个响应进行采样\",{\"1\":{\"33\":1}}],[\"从而将较大模型的能力提取到较小的模型中\",{\"1\":{\"35\":1}}],[\"从而加快注意力计算\",{\"1\":{\"26\":1}}],[\"从而得到attention\",{\"1\":{\"4\":1}}],[\"从前\",{\"1\":{\"17\":1}}],[\"从全称中可以看出\",{\"1\":{\"6\":1}}],[\"从bert到glm\",{\"0\":{\"0\":1}}],[\"经过一系列的key来获取value\",{\"1\":{\"4\":1}}],[\"vicuna\",{\"1\":{\"39\":1}}],[\"v3之后的两个轴上都优于chatgpt\",{\"1\":{\"38\":1}}],[\"volume\",{\"1\":{\"20\":1}}],[\"veselin\",{\"1\":{\"20\":1}}],[\"vaswani\",{\"1\":{\"20\":1}}],[\"value\",{\"1\":{\"4\":1}}],[\"v是encoder的输出计算的\",{\"1\":{\"5\":1}}],[\"v\",{\"1\":{\"4\":1,\"26\":1}}],[\"v21\",{\"1\":{\"20\":1}}],[\"v2好\",{\"1\":{\"19\":1}}],[\"v2几乎一致\",{\"1\":{\"19\":1}}],[\"v2进行微调\",{\"1\":{\"19\":1}}],[\"v2进行介绍\",{\"1\":{\"17\":1}}],[\"v2对glm进行微调\",{\"1\":{\"19\":1}}],[\"v2也是清华大学发布的\",{\"1\":{\"19\":1}}],[\"v2还包括以下改进\",{\"1\":{\"19\":1}}],[\"v2共有50\",{\"1\":{\"19\":1}}],[\"v2可以微调的参数变多了\",{\"1\":{\"19\":1}}],[\"v2实际上就是prefix\",{\"1\":{\"19\":1}}],[\"v2的关键所在就是引入了prefix\",{\"1\":{\"19\":1}}],[\"v2并不是一个全新的方法\",{\"1\":{\"19\":1}}],[\"v2应该属于提示微调\",{\"1\":{\"18\":1}}],[\"v2属于部分参数微调\",{\"1\":{\"18\":1}}],[\"v2基本上是同一时期发布的\",{\"1\":{\"17\":1}}],[\"v2是一种对预训练语言模型进行高效微调的方法\",{\"1\":{\"0\":1}}],[\"v2\",{\"0\":{\"17\":1,\"19\":1},\"1\":{\"0\":1,\"1\":2,\"17\":1,\"19\":2}}],[\"kl惩罚\",{\"1\":{\"35\":1}}],[\"karthik\",{\"1\":{\"20\":1}}],[\"katherine\",{\"1\":{\"20\":1}}],[\"kaiser\",{\"1\":{\"20\":1}}],[\"kristina\",{\"1\":{\"20\":1}}],[\"kenton\",{\"1\":{\"20\":1}}],[\"key\",{\"1\":{\"4\":1}}],[\"k\",{\"1\":{\"4\":1,\"26\":1}}],[\"我们设置β=0\",{\"1\":{\"35\":1}}],[\"我们使用512的批量大小\",{\"1\":{\"35\":1}}],[\"我们可以看到\",{\"1\":{\"34\":1,\"36\":1}}],[\"我们可以看图5\",{\"1\":{\"14\":1}}],[\"我们可以使用\",{\"1\":{\"18\":1}}],[\"我们可以将样本与提示\",{\"1\":{\"18\":1}}],[\"我们选择p\",{\"1\":{\"17\":1}}],[\"我们介绍微调相关技术\",{\"1\":{\"16\":1}}],[\"我们称之为masked\",{\"1\":{\"14\":1}}],[\"我们给定一个文本序列\",{\"1\":{\"14\":1}}],[\"我们来介绍glm中的自回归填空思想\",{\"1\":{\"13\":1}}],[\"我们来讨论多头注意力机制\",{\"1\":{\"5\":1}}],[\"我们将在下面介绍这一部分\",{\"1\":{\"7\":1}}],[\"我们无法评价自回归\",{\"1\":{\"6\":1}}],[\"我们知道\",{\"1\":{\"6\":1,\"13\":1}}],[\"我们希望自注意力机制是\",{\"1\":{\"5\":1}}],[\"我们进一步讨论transformer中应用的注意力机制\",{\"1\":{\"5\":1}}],[\"我们对注意力机制有了初步了解\",{\"1\":{\"5\":1}}],[\"我们往往会讨论q\",{\"1\":{\"4\":1}}],[\"我们首先讨论一下注意力机制\",{\"1\":{\"3\":1}}],[\"这些步骤生成了一个sft数据集\",{\"1\":{\"36\":1}}],[\"这些都是根据上下文\",{\"1\":{\"9\":1}}],[\"这在使用o\",{\"1\":{\"35\":1}}],[\"这种温度重新缩放都会发生恒定数量的步骤\",{\"1\":{\"35\":1}}],[\"这两个强化学习\",{\"1\":{\"35\":1}}],[\"这与constitutional\",{\"1\":{\"35\":1}}],[\"这可以让模型支持记住更多对话历史信息\",{\"1\":{\"26\":1}}],[\"这一点是不同于p\",{\"1\":{\"19\":1}}],[\"这部电影是\",{\"1\":{\"18\":1}}],[\"这对很多企业和个人仍然是不能接受的\",{\"1\":{\"17\":1}}],[\"这是一种受上下文蒸馏\",{\"1\":{\"36\":1}}],[\"这是普通公司和个人难以承受的\",{\"1\":{\"12\":1}}],[\"这是我们不希望发生的\",{\"1\":{\"5\":1}}],[\"这里\",{\"1\":{\"17\":1}}],[\"这里我们不做过多介绍\",{\"1\":{\"9\":1}}],[\"这里不再赘述\",{\"1\":{\"5\":1}}],[\"这会造成预训练和微调之间产生一个不匹配的情况\",{\"1\":{\"9\":1}}],[\"这个增量随着样本的增加而增加\",{\"1\":{\"35\":1}}],[\"这个奖励模型优于所有base\",{\"1\":{\"34\":1}}],[\"这个都是可以学习的地方\",{\"1\":{\"34\":1}}],[\"这个现在都在用\",{\"1\":{\"25\":1}}],[\"这个比例在t5模型的论文中证明\",{\"1\":{\"14\":1}}],[\"这个关系是没有办法被语言模型直接捕获到的\",{\"1\":{\"9\":1}}],[\"这个是什么呢\",{\"1\":{\"8\":1}}],[\"这个d其实就是模型最大能接受的token长度\",{\"1\":{\"7\":1}}],[\"这限制了预训练\",{\"1\":{\"6\":1}}],[\"这样可以进一步提高预测能力\",{\"1\":{\"14\":1}}],[\"这样预测的概率可能不准确\",{\"1\":{\"9\":1}}],[\"这样看来\",{\"1\":{\"7\":1,\"19\":1}}],[\"这样会导致softmax产生非常小的值\",{\"1\":{\"5\":1}}],[\"这样\",{\"1\":{\"5\":1,\"11\":1}}],[\"这也是transformer成功的关键\",{\"1\":{\"3\":1}}],[\"这绝对可以称之为\",{\"1\":{\"1\":1}}],[\"彻底抛弃了复杂的rnn和cnn\",{\"1\":{\"3\":1}}],[\"引入序列转换模型\",{\"1\":{\"3\":1}}],[\"mpt\",{\"1\":{\"39\":1}}],[\"m\",{\"0\":{\"40\":1},\"1\":{\"34\":1}}],[\"mqa\",{\"1\":{\"26\":1}}],[\"mha\",{\"1\":{\"26\":1}}],[\"myle\",{\"1\":{\"20\":1}}],[\"meta\",{\"1\":{\"34\":2}}],[\"meta总共收集了27540个好数据\",{\"1\":{\"31\":1}}],[\"methods\",{\"1\":{\"20\":2}}],[\"meeting\",{\"1\":{\"20\":1}}],[\"mechanism\",{\"1\":{\"3\":1,\"4\":1}}],[\"mini\",{\"1\":{\"35\":1}}],[\"ming\",{\"1\":{\"20\":4}}],[\"mike\",{\"1\":{\"20\":1}}],[\"michael\",{\"1\":{\"20\":1}}],[\"margin\",{\"1\":{\"34\":3}}],[\"marie\",{\"1\":{\"20\":1}}],[\"mandar\",{\"1\":{\"20\":1}}],[\"mach\",{\"1\":{\"20\":1}}],[\"matena\",{\"1\":{\"20\":1}}],[\"masked\",{\"1\":{\"20\":1}}],[\"mask掉的15\",{\"1\":{\"14\":1}}],[\"mask\",{\"1\":{\"9\":4,\"14\":3,\"18\":1}}],[\"mlm根据整个句子信息来推断被遮住的token\",{\"1\":{\"9\":1}}],[\"mlm的要做的事就是根据上下文来预测被遮住的token应该是什么\",{\"1\":{\"9\":1}}],[\"mlm\",{\"1\":{\"9\":1,\"14\":1}}],[\"mlm会随机地遮住输入的某些token\",{\"1\":{\"9\":1}}],[\"multi\",{\"1\":{\"5\":3,\"12\":2,\"26\":1}}],[\"more\",{\"1\":{\"34\":2}}],[\"moens\",{\"1\":{\"20\":1}}],[\"movie\",{\"1\":{\"5\":2}}],[\"models\",{\"1\":{\"20\":2,\"26\":1,\"31\":1,\"35\":1}}],[\"model即隐层维数\",{\"1\":{\"5\":1}}],[\"model\",{\"1\":{\"1\":1,\"9\":1,\"13\":1,\"14\":1,\"20\":2,\"26\":1,\"34\":2}}],[\"后续有详细介绍\",{\"1\":{\"22\":1}}],[\"后续有介绍\",{\"1\":{\"22\":1}}],[\"后续的语言模型\",{\"1\":{\"1\":1,\"5\":1}}],[\"后来\",{\"1\":{\"3\":1}}],[\"导致序列转换模型的效率不高\",{\"1\":{\"3\":1}}],[\"clipping\",{\"1\":{\"35\":1}}],[\"cls\",{\"1\":{\"19\":1}}],[\"choose\",{\"1\":{\"34\":1}}],[\"chen\",{\"1\":{\"20\":2}}],[\"chat模型与chatgpt具有竞争力\",{\"1\":{\"39\":1}}],[\"chat模型与开源模型\",{\"1\":{\"39\":1}}],[\"chat模型在单回合和多回合提示上都显著优于开源模型\",{\"1\":{\"39\":1}}],[\"chat获得了超过60\",{\"1\":{\"38\":1}}],[\"chat输出在gpt\",{\"1\":{\"38\":1}}],[\"chat生成它\",{\"1\":{\"36\":1}}],[\"chat会忘记指示\",{\"1\":{\"36\":1}}],[\"chat进行拒绝采样\",{\"1\":{\"35\":1}}],[\"chat使用占总步数3\",{\"1\":{\"34\":1}}],[\"chat使用与基本模型相同的优化器参数\",{\"1\":{\"34\":1}}],[\"chat的胜率不那么明显\",{\"1\":{\"38\":1}}],[\"chat的性能\",{\"1\":{\"34\":1}}],[\"chat的最大学习率为5×10−6\",{\"1\":{\"34\":1}}],[\"chat的微调方法\",{\"1\":{\"22\":1}}],[\"chat在训练数据上训练一个epoch\",{\"1\":{\"34\":1}}],[\"chat基于llama\",{\"1\":{\"30\":1}}],[\"chat是基于llama\",{\"1\":{\"22\":1}}],[\"chat\",{\"1\":{\"22\":1,\"35\":3,\"36\":1,\"38\":1,\"39\":5}}],[\"chatgpt和palm\",{\"1\":{\"39\":1}}],[\"chatgpt和llama\",{\"1\":{\"38\":1}}],[\"chatgpt是基于gpt\",{\"1\":{\"12\":1}}],[\"chatgpt的成功不是偶然的\",{\"1\":{\"0\":1}}],[\"chatgpt悄然进入大众的视线\",{\"1\":{\"0\":1}}],[\"chang\",{\"1\":{\"20\":1}}],[\"counterpart\",{\"1\":{\"34\":1}}],[\"constitutional\",{\"1\":{\"36\":1}}],[\"constant\",{\"1\":{\"20\":1}}],[\"conference\",{\"1\":{\"20\":1}}],[\"continuous\",{\"1\":{\"20\":1}}],[\"component\",{\"1\":{\"34\":1}}],[\"completion\",{\"1\":{\"34\":1}}],[\"computational\",{\"1\":{\"20\":2}}],[\"comparable\",{\"1\":{\"20\":1}}],[\"colin\",{\"1\":{\"20\":1}}],[\"can\",{\"1\":{\"20\":1,\"34\":2}}],[\"cast\",{\"1\":{\"5\":2}}],[\"cnn也罢\",{\"1\":{\"3\":1}}],[\"cnn\",{\"1\":{\"3\":1}}],[\"二维位置编码\",{\"0\":{\"15\":1}}],[\"二\",{\"0\":{\"2\":1}}],[\"该数据集具有消息列表\",{\"1\":{\"36\":1}}],[\"该模型为给定提示探索k个样本\",{\"1\":{\"35\":1}}],[\"该模型号称是\",{\"1\":{\"1\":1}}],[\"该算法是rlhf文献的标准\",{\"1\":{\"35\":1}}],[\"该论文篇幅巨大\",{\"1\":{\"21\":1}}],[\"该编码没有整合进模型\",{\"1\":{\"5\":1}}],[\"该编码不是一个单一的数值\",{\"1\":{\"5\":1}}],[\"该脉络基本涉及了现代语言模型学习的全过程\",{\"1\":{\"1\":1}}],[\"该方法微调效果可以与全参数微调媲美\",{\"1\":{\"1\":1}}],[\"等部分参数微调方法\",{\"1\":{\"1\":1}}],[\"和10e−6的恒定学习率\",{\"1\":{\"35\":1}}],[\"和llama\",{\"1\":{\"31\":1,\"35\":1}}],[\"和lora\",{\"1\":{\"1\":1}}],[\"和value\",{\"1\":{\"26\":1}}],[\"和字符标签\",{\"1\":{\"19\":1}}],[\"和\",{\"1\":{\"14\":1,\"34\":1}}],[\"和超多的预训练语料\",{\"1\":{\"12\":1}}],[\"和一个前馈网络\",{\"1\":{\"5\":1}}],[\"和一个解码器\",{\"1\":{\"3\":1}}],[\"和p\",{\"1\":{\"0\":1,\"17\":1}}],[\"95\",{\"1\":{\"27\":1,\"35\":1}}],[\"9\",{\"1\":{\"1\":1,\"17\":1,\"20\":1,\"27\":1,\"35\":1}}],[\"8字符\",{\"1\":{\"28\":1}}],[\"8\",{\"1\":{\"1\":1,\"7\":1,\"17\":1,\"20\":1}}],[\"7b\",{\"1\":{\"39\":1}}],[\"7b模型在60\",{\"1\":{\"39\":1}}],[\"70b模型在很大程度上优于palm\",{\"1\":{\"39\":1}}],[\"70b模型相对于chatgpt的胜率为36\",{\"1\":{\"39\":1}}],[\"70b模型上的ppo每次迭代平均耗时≈330秒\",{\"1\":{\"35\":1}}],[\"70b参数llama\",{\"1\":{\"34\":1}}],[\"70b四种参数规模\",{\"1\":{\"22\":1}}],[\"768\",{\"1\":{\"7\":1}}],[\"7\",{\"1\":{\"1\":1,\"17\":1,\"20\":1}}],[\"国内外研究者先后提出了p\",{\"1\":{\"1\":1}}],[\"大大减少了训练时的存储和内存使用\",{\"1\":{\"18\":1}}],[\"大家首先想起的一定是chatgpt\",{\"1\":{\"12\":1}}],[\"大家的参数量还没有那么夸张\",{\"1\":{\"7\":1}}],[\"大部分个人和团队都已经没有能力去做模型的全参数微调了\",{\"1\":{\"1\":1}}],[\"大模型\",{\"1\":{\"1\":1}}],[\"抛开预训练不谈\",{\"1\":{\"1\":1}}],[\"千亿\",{\"1\":{\"1\":1}}],[\"百亿\",{\"1\":{\"1\":1}}],[\"的提示上优于mpt\",{\"1\":{\"39\":1}}],[\"的黄金标准\",{\"1\":{\"39\":1}}],[\"的胜率\",{\"1\":{\"38\":1}}],[\"的loss设置为0\",{\"1\":{\"36\":1}}],[\"的测试集\",{\"1\":{\"34\":1}}],[\"的warm\",{\"1\":{\"34\":1}}],[\"的输入\",{\"1\":{\"14\":1}}],[\"的思想\",{\"1\":{\"14\":1}}],[\"的token来进行训练\",{\"1\":{\"9\":1}}],[\"的\",{\"1\":{\"5\":1}}],[\"的能力不是很强\",{\"1\":{\"1\":1}}],[\"的任务\",{\"1\":{\"1\":1}}],[\"低于10b参数\",{\"1\":{\"1\":1}}],[\"小模型\",{\"1\":{\"1\":1}}],[\"目前\",{\"1\":{\"1\":1}}],[\"意为通用语言模型\",{\"1\":{\"1\":1}}],[\"清华大学发布的glm\",{\"1\":{\"13\":1}}],[\"清华大学语言模型glm发布\",{\"1\":{\"1\":1}}],[\"清华大学在语言模型上研究较早\",{\"1\":{\"1\":1}}],[\"放眼国内\",{\"1\":{\"1\":1}}],[\"而中值保持不变\",{\"1\":{\"35\":1}}],[\"而对ppo只进行一次生成\",{\"1\":{\"35\":1}}],[\"而对glm进行微调同样还可以使用lora\",{\"1\":{\"19\":1}}],[\"而不是用指令来增加所有上下文对话回合\",{\"1\":{\"36\":1}}],[\"而不会导致性能大幅下降\",{\"1\":{\"26\":1}}],[\"而不能看到它之后的token\",{\"1\":{\"6\":1}}],[\"而如果改为使用p\",{\"1\":{\"19\":1}}],[\"而另外10\",{\"1\":{\"14\":1}}],[\"而另一边的openai也不甘落后\",{\"1\":{\"1\":1}}],[\"而glm中可能会masked掉连续的多个字\",{\"1\":{\"14\":1}}],[\"而gpt模型是自回归模型\",{\"1\":{\"12\":1}}],[\"而与bert等masked不同\",{\"1\":{\"14\":1}}],[\"而每一个文本域都会被一个单独的token\",{\"1\":{\"14\":1}}],[\"而自编码模型擅长自然语言理解任务\",{\"1\":{\"13\":1}}],[\"而自编码模型更加适合自然语言理解任务\",{\"1\":{\"6\":1}}],[\"而bert的位置编码\",{\"1\":{\"8\":1}}],[\"而首个将transformer应用到实际语言模型中的\",{\"1\":{\"6\":1}}],[\"而多头注意力机制是由缩放点积注意力机制\",{\"1\":{\"5\":1}}],[\"而是在每一层都进行微调\",{\"1\":{\"19\":1}}],[\"而是将其放到了part\",{\"1\":{\"14\":1}}],[\"而是做了一些修改\",{\"1\":{\"12\":1}}],[\"而是其decoder部分\",{\"1\":{\"12\":1}}],[\"而是采用的mlm\",{\"1\":{\"9\":1}}],[\"而是只用到了encoder部分\",{\"1\":{\"7\":1}}],[\"而是用这个向量让每个词具有它在句子序列中的位置信息\",{\"1\":{\"5\":1}}],[\"而是包含句子中特定位置信息的d维向量\",{\"1\":{\"5\":1}}],[\"而在计算方式上\",{\"1\":{\"4\":1}}],[\"而transformer\",{\"1\":{\"3\":1}}],[\"而2020年\",{\"1\":{\"1\":1}}],[\"其余参数为1×10−5\",{\"1\":{\"34\":1}}],[\"其余基本没有变化\",{\"1\":{\"12\":1}}],[\"其主要结果如下\",{\"1\":{\"19\":1}}],[\"其主要的修改如下\",{\"1\":{\"7\":1}}],[\"其事实上是将文本生成的prefix\",{\"1\":{\"19\":1}}],[\"其在挖掘语言模型的潜在能力上有着不错的成绩\",{\"1\":{\"17\":1}}],[\"其实在bert中也是这样做的\",{\"1\":{\"14\":1}}],[\"其实multi\",{\"1\":{\"5\":1}}],[\"其通用性体现在哪里\",{\"1\":{\"13\":1}}],[\"其优良的效果源于其创新的思想和持续的研究\",{\"1\":{\"13\":1}}],[\"其参数规模进一步增大\",{\"1\":{\"12\":1}}],[\"其参数量已经达到了175b\",{\"1\":{\"1\":1}}],[\"其参数量有1\",{\"1\":{\"1\":1}}],[\"其参数量有117m\",{\"1\":{\"1\":1}}],[\"其成功的关键在于超大的参数规模\",{\"1\":{\"12\":1}}],[\"其能力大家有目共睹\",{\"1\":{\"10\":1,\"12\":1}}],[\"其本质就是通过一个普通的词嵌入来区分每一个序列所处的位置\",{\"1\":{\"8\":1}}],[\"其计算公式如下\",{\"1\":{\"5\":1}}],[\"其计算过程与注意力机制一致\",{\"1\":{\"5\":1}}],[\"其比独热编码\",{\"1\":{\"5\":1}}],[\"其结构可以分为输入输出嵌入向量\",{\"1\":{\"5\":1}}],[\"其抛弃了rnn和cnn作为encoder和decoder\",{\"1\":{\"5\":1}}],[\"其中un和an分别对应于回合n的用户和助手消息\",{\"1\":{\"36\":1}}],[\"其中β1=0\",{\"1\":{\"35\":1}}],[\"其中n∈\",{\"1\":{\"35\":1}}],[\"其中奖励被视为能量函数\",{\"1\":{\"35\":1}}],[\"其中只有80\",{\"1\":{\"14\":1}}],[\"其中x~corrupt~就是parta\",{\"1\":{\"14\":1}}],[\"其中s~1~对应着\",{\"1\":{\"14\":1}}],[\"其中每个文本域s~i\",{\"1\":{\"14\":1}}],[\"其中的情况b确实为a的下一句话\",{\"1\":{\"9\":1}}],[\"其中蓝色阴影部分\",{\"1\":{\"7\":1}}],[\"其中因为内容相对重复\",{\"1\":{\"0\":1}}],[\"其中transformer是一种全新的序列转换模型\",{\"1\":{\"0\":1}}],[\"了\",{\"1\":{\"1\":1}}],[\"是llama\",{\"1\":{\"22\":1}}],[\"是被masked标注的预测概率来预测样本的标签\",{\"1\":{\"18\":1}}],[\"是transfer\",{\"1\":{\"11\":1}}],[\"是效果最好的\",{\"1\":{\"9\":1}}],[\"是\",{\"1\":{\"9\":1}}],[\"是这三个嵌入式张量的和\",{\"1\":{\"8\":1}}],[\"是训练出来的\",{\"1\":{\"8\":1}}],[\"是由三角函数计算出来的\",{\"1\":{\"8\":1}}],[\"是decoder单元中多了一层masked\",{\"1\":{\"5\":1}}],[\"是因为对于输入的d大值\",{\"1\":{\"5\":1}}],[\"是人们在机器学习模型中嵌入的一种特殊结构\",{\"1\":{\"4\":1}}],[\"是bert\",{\"1\":{\"1\":1}}],[\"是其总结前人经验\",{\"1\":{\"0\":1}}],[\"文本到文本\",{\"1\":{\"1\":1}}],[\"在我们的提示集上\",{\"1\":{\"39\":1}}],[\"在对话的大部分时间里保持了对系统消息的大量注意力激活\",{\"1\":{\"36\":1}}],[\"在对话设置中\",{\"1\":{\"36\":1}}],[\"在为训练数据构建最终系统消息时\",{\"1\":{\"36\":1}}],[\"在初始版本的rlhf模型中\",{\"1\":{\"36\":1}}],[\"在有限的计算预算下\",{\"1\":{\"35\":1}}],[\"在迭代模型更新过程中\",{\"1\":{\"35\":1}}],[\"在不同温度下的最大回报曲线\",{\"1\":{\"35\":1}}],[\"在图8中\",{\"1\":{\"35\":1}}],[\"在图5\",{\"1\":{\"14\":2}}],[\"在每个迭代过程\",{\"1\":{\"35\":1}}],[\"在每一batch用于奖励建模的人类偏好注释上\",{\"1\":{\"34\":1}}],[\"在应用类似于sft的微调之前\",{\"1\":{\"35\":1}}],[\"在拒绝采样微调中\",{\"1\":{\"35\":1}}],[\"在拒绝采样中\",{\"1\":{\"35\":1}}],[\"在步骤t的训练期间\",{\"1\":{\"35\":1}}],[\"在更明显的反应上\",{\"1\":{\"34\":1}}],[\"在早期的实验中\",{\"1\":{\"34\":1}}],[\"在这组评估中\",{\"1\":{\"38\":1}}],[\"在这种二元排名损失的基础上\",{\"1\":{\"34\":1}}],[\"在这里也可以看出\",{\"1\":{\"14\":1}}],[\"在这里\",{\"1\":{\"5\":1,\"35\":1}}],[\"在第3节\",{\"1\":{\"22\":1}}],[\"在第2节\",{\"1\":{\"22\":1}}],[\"在多个公开可获得数据上进行训练\",{\"1\":{\"22\":1}}],[\"在多头注意力机制和前馈网络完成后\",{\"1\":{\"5\":1}}],[\"在ppo中\",{\"1\":{\"35\":1}}],[\"在p\",{\"1\":{\"19\":1}}],[\"在prefix部分\",{\"1\":{\"19\":1}}],[\"在330m到10b参数规模的语言模型上\",{\"1\":{\"19\":1}}],[\"在文本序列中采样的文本域为\",{\"1\":{\"14\":1}}],[\"在其中采用多个文本域\",{\"1\":{\"14\":1}}],[\"在现在看\",{\"1\":{\"13\":1}}],[\"在现在看来\",{\"1\":{\"6\":1}}],[\"在glm之前\",{\"1\":{\"13\":1}}],[\"在gpt中\",{\"1\":{\"12\":1}}],[\"在国内\",{\"1\":{\"13\":1}}],[\"在t5模型中\",{\"1\":{\"11\":1}}],[\"在transformer的介绍中我们提到过\",{\"1\":{\"7\":1}}],[\"在transformer中\",{\"1\":{\"4\":1,\"5\":1,\"7\":2,\"15\":1}}],[\"在transformer之前\",{\"1\":{\"3\":1}}],[\"在实验中\",{\"1\":{\"9\":1}}],[\"在bert中\",{\"1\":{\"7\":3,\"14\":2,\"15\":1}}],[\"在解码阶段\",{\"1\":{\"5\":1}}],[\"在2\",{\"1\":{\"5\":1}}],[\"在自注意力机制中\",{\"1\":{\"4\":1}}],[\"在注意力机制中\",{\"1\":{\"4\":1}}],[\"在介绍transformer之前\",{\"1\":{\"3\":1}}],[\"在如此大的规模下\",{\"1\":{\"1\":1}}],[\"在当年属于标准大小\",{\"1\":{\"1\":1}}],[\"在chatgpt大火之后\",{\"1\":{\"1\":1}}],[\"轰动一时\",{\"1\":{\"1\":1}}],[\"bpe\",{\"1\":{\"28\":1}}],[\"bison聊天模型\",{\"1\":{\"39\":1}}],[\"bison\",{\"1\":{\"39\":1}}],[\"billion\",{\"1\":{\"26\":1}}],[\"bidirectional\",{\"1\":{\"20\":1}}],[\"batch\",{\"1\":{\"31\":1,\"35\":2}}],[\"bao\",{\"1\":{\"20\":1}}],[\"based\",{\"1\":{\"35\":1}}],[\"base的参数规模是110m\",{\"1\":{\"7\":1}}],[\"base的100倍大小\",{\"1\":{\"1\":1}}],[\"base\",{\"1\":{\"7\":3}}],[\"brian\",{\"1\":{\"20\":1}}],[\"brain就发布了一个全新的序列转换模型\",{\"1\":{\"1\":1}}],[\"be\",{\"1\":{\"20\":1,\"34\":1}}],[\"bert通过这种方式来保留一些被masked的原始信息\",{\"1\":{\"14\":1}}],[\"bert模型是自编码模型\",{\"1\":{\"12\":1}}],[\"bert模型结构图\",{\"1\":{\"7\":1}}],[\"bert在预训练中加入了nsp\",{\"1\":{\"9\":1}}],[\"bert在当时的条件下认为\",{\"1\":{\"6\":1}}],[\"bert把这15\",{\"1\":{\"9\":1}}],[\"bert的效果遥遥领先于同期其他语言模型\",{\"1\":{\"9\":1}}],[\"bert的论文中没有提为什么是15\",{\"1\":{\"9\":1}}],[\"bert的预训练没有采用传统的自左向右或者自右向左语言模型来训练bert\",{\"1\":{\"9\":1}}],[\"bert的基础模型有110m参数\",{\"1\":{\"1\":1}}],[\"bert对positional\",{\"1\":{\"8\":1}}],[\"bert对encoder进行了一些修改\",{\"1\":{\"7\":1}}],[\"bert相对transformer来说\",{\"1\":{\"8\":1}}],[\"bert只用了transformer的encoder部分\",{\"1\":{\"7\":1}}],[\"bert并没有把transformer拿来直接用\",{\"1\":{\"7\":1}}],[\"bert与transformer\",{\"0\":{\"7\":1}}],[\"bert诞生了\",{\"1\":{\"6\":1}}],[\"bert是一个双向编码模型\",{\"1\":{\"6\":1}}],[\"bert全称bidirectional\",{\"1\":{\"6\":1}}],[\"bert和t5都是google的精彩操作\",{\"1\":{\"1\":1}}],[\"bert\",{\"0\":{\"6\":1},\"1\":{\"0\":2,\"7\":8,\"8\":1,\"13\":1,\"20\":2}}],[\"by\",{\"1\":{\"20\":1}}],[\"b部分\",{\"1\":{\"14\":1}}],[\"b\",{\"1\":{\"14\":1,\"19\":1}}],[\"blank\",{\"1\":{\"14\":1}}],[\"but\",{\"1\":{\"5\":2}}],[\"早在2017年\",{\"1\":{\"1\":1}}],[\"讯飞的星火等\",{\"1\":{\"1\":1}}],[\"各种语言模型层出不穷\",{\"1\":{\"1\":1}}],[\"一些指示应适用于所有的对话轮数中\",{\"1\":{\"36\":1}}],[\"一般masked掉的是一个词\",{\"1\":{\"14\":1}}],[\"一般会mask掉句子的15\",{\"1\":{\"9\":1}}],[\"一个多卡并行训练的论文\",{\"1\":{\"26\":1}}],[\"一个字可能意义不如多个字组成的意义大\",{\"1\":{\"14\":1}}],[\"一个双向编码语言模型\",{\"1\":{\"6\":1}}],[\"一个encoder单元\",{\"1\":{\"5\":1}}],[\"一个好的位置编码方案需要满足以下几个条件\",{\"1\":{\"5\":1}}],[\"一个序列转换\",{\"1\":{\"3\":1}}],[\"一\",{\"0\":{\"1\":1}}],[\"即最后一个回合之前的所有中间辅助消息\",{\"1\":{\"36\":1}}],[\"即使在使用大批量和kv缓存时也是如此\",{\"1\":{\"35\":1}}],[\"即使是少量的\",{\"1\":{\"31\":1}}],[\"即每batch\",{\"1\":{\"34\":1}}],[\"即选择和拒绝\",{\"1\":{\"34\":1}}],[\"即可接受的输入长度\",{\"1\":{\"22\":1}}],[\"即对parta部分和partb部分都进行编码\",{\"1\":{\"15\":1}}],[\"即自回归的\",{\"1\":{\"14\":1}}],[\"即带mask部分的句子\",{\"1\":{\"14\":1}}],[\"即masked\",{\"1\":{\"9\":1}}],[\"即此时segment词表的长度为2\",{\"1\":{\"8\":1}}],[\"即8个缩放点积注意力叠加而成\",{\"1\":{\"7\":1}}],[\"即\",{\"1\":{\"6\":1,\"14\":1,\"35\":1}}],[\"即除以d的开方\",{\"1\":{\"5\":1}}],[\"即通过注入词的顺序信息来增强模型的输入\",{\"1\":{\"5\":1}}],[\"即所有的自然语言理解任务都可划分为\",{\"1\":{\"1\":1}}],[\"即结构\",{\"1\":{\"0\":1}}],[\"即transformer\",{\"1\":{\"0\":1}}],[\"ppo介绍\",{\"1\":{\"35\":1}}],[\"ppo\",{\"1\":{\"35\":1}}],[\"pseudo\",{\"1\":{\"20\":1}}],[\"piao\",{\"1\":{\"20\":1}}],[\"percy\",{\"1\":{\"20\":1}}],[\"peter\",{\"1\":{\"20\":1}}],[\"pairs\",{\"1\":{\"34\":1}}],[\"papers\",{\"1\":{\"20\":1}}],[\"pages\",{\"1\":{\"20\":3}}],[\"parallel\",{\"1\":{\"35\":1}}],[\"parallelism\",{\"1\":{\"26\":1}}],[\"parameter\",{\"1\":{\"20\":1,\"26\":1}}],[\"parikh\",{\"1\":{\"20\":1}}],[\"parmar\",{\"1\":{\"20\":1}}],[\"parta和partb\",{\"1\":{\"15\":1}}],[\"partb可以自回归地看到已经走过的partb和全部的parta\",{\"1\":{\"14\":1}}],[\"policy\",{\"1\":{\"35\":1}}],[\"polosukhin\",{\"1\":{\"20\":1}}],[\"power\",{\"1\":{\"20\":1}}],[\"position\",{\"1\":{\"15\":1}}],[\"positional\",{\"1\":{\"7\":1,\"25\":1}}],[\"prints\",{\"1\":{\"20\":1}}],[\"preference\",{\"1\":{\"34\":1}}],[\"preferred\",{\"1\":{\"34\":1}}],[\"prefixtuning\",{\"1\":{\"20\":1}}],[\"prefix参数进行微调\",{\"1\":{\"19\":1}}],[\"prefix为前缀\",{\"1\":{\"19\":1}}],[\"prefix\",{\"1\":{\"1\":1,\"17\":2,\"19\":4}}],[\"preprint\",{\"1\":{\"20\":3}}],[\"pretraining\",{\"1\":{\"20\":2}}],[\"pre\",{\"1\":{\"12\":1,\"20\":3}}],[\"prediction\",{\"1\":{\"9\":1}}],[\"proximal\",{\"1\":{\"35\":1}}],[\"proceedings\",{\"1\":{\"20\":1}}],[\"processing\",{\"1\":{\"20\":3}}],[\"proc\",{\"1\":{\"20\":1}}],[\"product\",{\"1\":{\"5\":1}}],[\"prompts\",{\"1\":{\"20\":1}}],[\"prompt\",{\"1\":{\"1\":1,\"17\":2,\"18\":1,\"20\":2,\"34\":1}}],[\"p\",{\"0\":{\"17\":1,\"19\":1,\"26\":1},\"1\":{\"0\":1,\"1\":1,\"17\":1,\"18\":1,\"19\":7,\"20\":2}}],[\"几个方面来介绍\",{\"1\":{\"0\":1}}],[\"64的小批量大小\",{\"1\":{\"35\":1}}],[\"64\",{\"1\":{\"31\":1}}],[\"642\",{\"1\":{\"20\":1}}],[\"652\",{\"1\":{\"20\":1}}],[\"68\",{\"1\":{\"20\":1}}],[\"61\",{\"1\":{\"20\":1}}],[\"67\",{\"1\":{\"20\":1}}],[\"60th\",{\"1\":{\"20\":1}}],[\"6010\",{\"1\":{\"20\":1}}],[\"6000\",{\"1\":{\"20\":1}}],[\"6\",{\"0\":{\"18\":1,\"19\":1},\"1\":{\"0\":1,\"1\":1,\"5\":2,\"17\":1,\"20\":1}}],[\"turbo\",{\"1\":{\"39\":1}}],[\"tuning与p\",{\"1\":{\"19\":1}}],[\"tuning中\",{\"1\":{\"19\":1}}],[\"tuning的\",{\"1\":{\"19\":1}}],[\"tuning将预训练lm参数固定\",{\"1\":{\"19\":1}}],[\"tuning示意图\",{\"1\":{\"19\":1}}],[\"tuning最开始应用在自然语言生成\",{\"1\":{\"19\":1}}],[\"tuning技术首次拓展到序列标注等复杂自然语言理解\",{\"1\":{\"19\":1}}],[\"tuning技术适配到自然语言理解任务中\",{\"1\":{\"19\":1}}],[\"tuning相似的性能\",{\"1\":{\"19\":1}}],[\"tuning进行了修改和优化\",{\"1\":{\"17\":1}}],[\"tuning和p\",{\"1\":{\"17\":1}}],[\"tuning虽然实现了部分参数调优\",{\"1\":{\"17\":1}}],[\"tuning\",{\"0\":{\"17\":1,\"19\":1,\"30\":1,\"31\":1},\"1\":{\"0\":2,\"1\":5,\"17\":8,\"18\":3,\"19\":15,\"20\":4,\"35\":1}}],[\"two\",{\"1\":{\"34\":1}}],[\"this\",{\"1\":{\"34\":1}}],[\"those\",{\"1\":{\"34\":1}}],[\"that\",{\"1\":{\"34\":1}}],[\"the\",{\"1\":{\"5\":6,\"20\":5,\"34\":5}}],[\"trillion\",{\"1\":{\"23\":1}}],[\"training\",{\"1\":{\"12\":1,\"20\":3,\"26\":1}}],[\"transfer\",{\"1\":{\"20\":1}}],[\"transfer来自transfer\",{\"1\":{\"11\":1}}],[\"transformer架构\",{\"1\":{\"25\":1}}],[\"transformer即我们在第二节中提到的\",{\"1\":{\"11\":1}}],[\"transformers\",{\"1\":{\"6\":1,\"20\":1}}],[\"transformer是一个seq2seq模型\",{\"1\":{\"6\":1}}],[\"transformer是seq2seq的全新尝试\",{\"1\":{\"5\":1}}],[\"transformer对后来语言模型的影响十分深远\",{\"1\":{\"5\":1}}],[\"transformer在这里设计的前馈网络比较简单\",{\"1\":{\"5\":1}}],[\"transformer认为\",{\"1\":{\"5\":1}}],[\"transformer中的位置编码\",{\"1\":{\"8\":1}}],[\"transformer中的位置编码采用公式计算得到\",{\"1\":{\"5\":1}}],[\"transformer中多头注意力机制是由h\",{\"1\":{\"7\":1}}],[\"transformer中对注意力机制的体现在多头注意力机制\",{\"1\":{\"5\":1}}],[\"transformer的decoder结构中\",{\"1\":{\"12\":1}}],[\"transformer的decoder和encoder十分相似\",{\"1\":{\"5\":1}}],[\"transformer的简写\",{\"1\":{\"11\":1}}],[\"transformer的encoder层是由n=6的单元构成的\",{\"1\":{\"7\":1}}],[\"transformer的encoder模块\",{\"1\":{\"7\":1}}],[\"transformer的模型结构已经介绍完成\",{\"1\":{\"5\":1}}],[\"transformer的编码器结构如图2\",{\"1\":{\"5\":1}}],[\"transformer的位置编码简单但是有创新性\",{\"1\":{\"5\":1}}],[\"transformer没有办法表示序列的顺序\",{\"1\":{\"5\":1}}],[\"transformer模型整体结构如图2\",{\"1\":{\"5\":1}}],[\"transformer模型\",{\"0\":{\"5\":1},\"1\":{\"5\":1}}],[\"transformer\",{\"0\":{\"2\":1},\"1\":{\"1\":1,\"20\":1}}],[\"täckström\",{\"1\":{\"20\":1}}],[\"table\",{\"1\":{\"34\":2}}],[\"tau\",{\"1\":{\"20\":1}}],[\"tasks\",{\"1\":{\"20\":3}}],[\"tang\",{\"1\":{\"20\":2}}],[\"tim\",{\"1\":{\"20\":1}}],[\"text的形式\",{\"1\":{\"11\":1}}],[\"text\",{\"1\":{\"11\":5,\"20\":2,\"35\":1}}],[\"too\",{\"1\":{\"20\":1}}],[\"toutanova\",{\"1\":{\"20\":1}}],[\"token上进行训练\",{\"1\":{\"24\":1}}],[\"token\",{\"1\":{\"14\":1,\"22\":1}}],[\"to\",{\"1\":{\"3\":1,\"11\":4,\"20\":2}}],[\"t5一样\",{\"1\":{\"16\":1}}],[\"t5模型相同\",{\"1\":{\"12\":1}}],[\"t5模型\",{\"0\":{\"11\":1},\"1\":{\"11\":1}}],[\"t5和gpt\",{\"0\":{\"10\":1}}],[\"t5论文中有对比实验\",{\"1\":{\"9\":1}}],[\"t5基础模型参数量为220m\",{\"1\":{\"1\":1}}],[\"t5\",{\"1\":{\"0\":2,\"10\":1,\"11\":1}}],[\"5的对话聊天机器人\",{\"1\":{\"12\":1}}],[\"512\",{\"1\":{\"7\":1}}],[\"5b\",{\"1\":{\"1\":1}}],[\"5\",{\"0\":{\"14\":1,\"15\":1,\"16\":1},\"1\":{\"0\":1,\"5\":2,\"20\":1,\"27\":1,\"31\":1,\"39\":2}}],[\"gradient\",{\"1\":{\"35\":1}}],[\"grouped\",{\"1\":{\"22\":1,\"26\":1}}],[\"gatt方法\",{\"1\":{\"36\":1}}],[\"gatt允许对多轮进行对话控制\",{\"1\":{\"36\":1}}],[\"gatt\",{\"1\":{\"30\":1,\"36\":1}}],[\"gao\",{\"1\":{\"20\":1}}],[\"ghost\",{\"1\":{\"30\":1}}],[\"gqa\",{\"1\":{\"23\":1,\"26\":1}}],[\"goyal\",{\"1\":{\"20\":1}}],[\"gomez\",{\"1\":{\"20\":1}}],[\"google公开了t5模型\",{\"1\":{\"1\":1}}],[\"google公开了以transformer作为基础的语言模型bert\",{\"1\":{\"1\":1}}],[\"google\",{\"1\":{\"1\":1}}],[\"g\",{\"1\":{\"13\":3}}],[\"generation也提出了同样的llm重新排序策略\",{\"1\":{\"35\":1}}],[\"generation\",{\"1\":{\"20\":2}}],[\"generative\",{\"1\":{\"12\":1,\"20\":1}}],[\"general\",{\"1\":{\"1\":1,\"13\":1,\"20\":1}}],[\"glm的基本结构已经介绍完毕\",{\"1\":{\"16\":1}}],[\"glm同样只使用了transformer的encoder部分\",{\"1\":{\"16\":1}}],[\"glm同样对transformer的结构进行了修改\",{\"1\":{\"16\":1}}],[\"glm同样是基于transformer的结构\",{\"1\":{\"16\":1}}],[\"glm与transformer\",{\"0\":{\"16\":1}}],[\"glm二维编码\",{\"1\":{\"15\":1}}],[\"glm采用了二维位置编码\",{\"1\":{\"15\":1}}],[\"glm采用自回归的方式尝试还原它们\",{\"1\":{\"14\":1}}],[\"glm仍然是以transformer为基础的结构\",{\"1\":{\"15\":1}}],[\"glm自回归填空示意图二\",{\"1\":{\"14\":1}}],[\"glm自回归填空示意图之一\",{\"1\":{\"14\":1}}],[\"glm把输入的文本分为两个部分\",{\"1\":{\"15\":1}}],[\"glm把masked掉的信息全部保留在了partb\",{\"1\":{\"14\":1}}],[\"glm把s~1~和s~2~对应的x部分替换为一个\",{\"1\":{\"14\":1}}],[\"glm预测的条件比bert多了一个partb\",{\"1\":{\"14\":1}}],[\"glm随机masked掉的比例为15\",{\"1\":{\"14\":1}}],[\"glm随机masked掉一些文本\",{\"1\":{\"14\":1}}],[\"glm没有选择直接丢失这些x\",{\"1\":{\"14\":1}}],[\"glm模型应用了名为自回归填空\",{\"1\":{\"14\":1}}],[\"glm创新地应用了自回归填空思想\",{\"1\":{\"13\":1}}],[\"glm也尝试能够同时处理自然语言理解和自然语言生成等多种nlp任务\",{\"1\":{\"13\":1}}],[\"glm意为通用语言模型\",{\"1\":{\"13\":1}}],[\"glm\",{\"0\":{\"13\":1},\"1\":{\"0\":1}}],[\"gpt似乎有能力处理自然语言理解和自然语言生成任务\",{\"1\":{\"13\":1}}],[\"gpt已经迎来了第四个大版本\",{\"1\":{\"12\":1}}],[\"gpt也并没有将transformer的decoder拿过来直接用\",{\"1\":{\"12\":1}}],[\"gpt的核心部分是n=12的transformer\",{\"1\":{\"12\":1}}],[\"gpt采用的不是transformer的encoder部分\",{\"1\":{\"12\":1}}],[\"gpt同样也抛弃了传统的rnn和cnn\",{\"1\":{\"12\":1}}],[\"gpt模型结构\",{\"1\":{\"12\":1}}],[\"gpt模型与bert模型不同\",{\"1\":{\"12\":1}}],[\"gpt模型\",{\"0\":{\"12\":1}}],[\"gpt是一个自回归\",{\"1\":{\"6\":1}}],[\"gpt和glm均为预训练语言模型\",{\"1\":{\"0\":1}}],[\"gpt\",{\"1\":{\"0\":1,\"1\":3,\"12\":1,\"13\":1,\"20\":1}}],[\"40b型号相比\",{\"1\":{\"39\":1}}],[\"4096\",{\"1\":{\"31\":1}}],[\"4提示中出现的顺序是随机交换的\",{\"1\":{\"38\":1}}],[\"4额外计算最终结果\",{\"1\":{\"38\":1}}],[\"46\",{\"0\":{\"26\":1}}],[\"4186\",{\"1\":{\"20\":1}}],[\"4171\",{\"1\":{\"20\":1}}],[\"4\",{\"0\":{\"11\":1,\"12\":1,\"37\":1,\"38\":1,\"39\":1},\"1\":{\"0\":1,\"5\":3,\"7\":1,\"12\":1,\"13\":1,\"20\":1,\"34\":1}}],[\"33b和falcon\",{\"1\":{\"39\":1}}],[\"34b与同等尺寸的vicuna\",{\"1\":{\"39\":1}}],[\"34b\",{\"1\":{\"22\":1}}],[\"3059\",{\"1\":{\"20\":1}}],[\"3045\",{\"1\":{\"20\":1}}],[\"3公布\",{\"1\":{\"1\":1}}],[\"3\",{\"0\":{\"5\":1,\"7\":1,\"8\":1,\"9\":2,\"16\":1,\"29\":1,\"30\":1,\"31\":1,\"32\":1,\"33\":1,\"34\":1,\"35\":2,\"36\":2,\"37\":1,\"38\":1,\"39\":1},\"1\":{\"0\":1,\"4\":2,\"5\":1,\"7\":1,\"12\":1,\"15\":2,\"16\":1,\"19\":1,\"20\":1,\"34\":2,\"35\":1,\"39\":1}}],[\"28\",{\"1\":{\"34\":1}}],[\"27\",{\"1\":{\"34\":1}}],[\"2微调而来\",{\"1\":{\"30\":1}}],[\"2选择使用gqa而不是mqa\",{\"1\":{\"26\":1}}],[\"2模型\",{\"1\":{\"26\":1}}],[\"2万亿\",{\"1\":{\"23\":1}}],[\"2t\",{\"1\":{\"23\":1}}],[\"2的ppo剪辑阈值\",{\"1\":{\"35\":1}}],[\"2的训练数据来自混合后的公开数据\",{\"1\":{\"24\":1}}],[\"2的推理可扩展性\",{\"1\":{\"23\":1}}],[\"2的预训练特性如下\",{\"1\":{\"23\":1}}],[\"2的预训练方法\",{\"1\":{\"22\":1}}],[\"2的预训练和微调过程\",{\"1\":{\"21\":1}}],[\"2执行了更稳健的数据清理\",{\"1\":{\"23\":1}}],[\"2基本还是采用llama\",{\"1\":{\"23\":1}}],[\"2预训练模型微调得来的\",{\"1\":{\"22\":1}}],[\"2预训练的语料库增加了40\",{\"1\":{\"22\":1}}],[\"2和基于它的微调模型llama\",{\"1\":{\"22\":1}}],[\"2共包括两大版本\",{\"1\":{\"22\":1}}],[\"2106\",{\"1\":{\"20\":1}}],[\"2101\",{\"1\":{\"20\":1}}],[\"2103\",{\"1\":{\"20\":2}}],[\"21\",{\"1\":{\"20\":2}}],[\"2表示被masked的词在partb中的位置\",{\"1\":{\"15\":1}}],[\"24\",{\"1\":{\"7\":1}}],[\"2节中\",{\"1\":{\"5\":1}}],[\"2公开\",{\"1\":{\"1\":1}}],[\"2000\",{\"1\":{\"27\":1}}],[\"20\",{\"1\":{\"20\":1}}],[\"2022\",{\"1\":{\"20\":1}}],[\"2022年底\",{\"1\":{\"0\":1}}],[\"2021\",{\"1\":{\"20\":5}}],[\"2021年\",{\"1\":{\"1\":1}}],[\"2020\",{\"1\":{\"20\":4}}],[\"2016\",{\"1\":{\"20\":1}}],[\"2018a\",{\"1\":{\"20\":1}}],[\"2018年\",{\"1\":{\"1\":2}}],[\"2019\",{\"1\":{\"20\":3}}],[\"2019年\",{\"1\":{\"1\":2}}],[\"2017\",{\"1\":{\"20\":1}}],[\"2\",{\"0\":{\"3\":1,\"4\":2,\"5\":1,\"8\":1,\"12\":1,\"15\":1,\"19\":1,\"21\":1,\"23\":1,\"24\":1,\"25\":2,\"26\":1,\"29\":1,\"32\":1,\"33\":1,\"34\":2,\"35\":1,\"39\":1},\"1\":{\"0\":1,\"4\":2,\"5\":1,\"7\":1,\"8\":1,\"12\":2,\"13\":1,\"14\":2,\"15\":1,\"16\":1,\"19\":4,\"20\":2,\"22\":4,\"23\":1,\"30\":1,\"34\":6,\"35\":5,\"36\":3,\"38\":4,\"39\":7}}],[\"1节所示\",{\"1\":{\"34\":1}}],[\"1一致\",{\"1\":{\"31\":1}}],[\"1一样\",{\"1\":{\"28\":1}}],[\"1相同的标记器\",{\"1\":{\"28\":1}}],[\"1差异\",{\"1\":{\"25\":1}}],[\"1基本一致\",{\"1\":{\"25\":1}}],[\"1论文链接\",{\"1\":{\"23\":1}}],[\"1的权重衰减\",{\"1\":{\"35\":1}}],[\"1的权重衰减和1\",{\"1\":{\"27\":1}}],[\"1的变化内容介绍\",{\"0\":{\"26\":1}}],[\"1的训练方法\",{\"1\":{\"23\":1}}],[\"1的升级版本\",{\"1\":{\"22\":1}}],[\"140\",{\"1\":{\"20\":1}}],[\"1~\",{\"1\":{\"14\":1}}],[\"13b\",{\"1\":{\"22\":1}}],[\"130b进行全参数微调\",{\"1\":{\"17\":1,\"19\":1}}],[\"13\",{\"1\":{\"13\":1,\"20\":1}}],[\"1750亿\",{\"1\":{\"12\":1}}],[\"16\",{\"1\":{\"7\":1}}],[\"12=600个参数需要微调\",{\"1\":{\"19\":1}}],[\"12\",{\"1\":{\"7\":2,\"13\":1,\"20\":1}}],[\"119\",{\"1\":{\"20\":1}}],[\"11\",{\"1\":{\"3\":1,\"20\":1}}],[\"100\",{\"1\":{\"35\":1}}],[\"10385\",{\"1\":{\"20\":1}}],[\"10360\",{\"1\":{\"20\":1}}],[\"1024行\",{\"1\":{\"34\":1}}],[\"1024\",{\"1\":{\"7\":1}}],[\"10\",{\"1\":{\"1\":1,\"9\":2,\"14\":1,\"17\":1,\"20\":1}}],[\"1公开\",{\"1\":{\"1\":1}}],[\"1\",{\"0\":{\"3\":1,\"7\":1,\"11\":1,\"14\":1,\"18\":1,\"22\":1,\"24\":1,\"26\":1,\"31\":1,\"33\":1,\"38\":1},\"1\":{\"0\":1,\"3\":2,\"5\":1,\"7\":3,\"11\":1,\"14\":4,\"15\":1,\"16\":1,\"19\":5,\"20\":2,\"22\":1,\"31\":1,\"35\":5}}],[\"反复打磨多年才得以形成的\",{\"1\":{\"0\":1}}]],\"serializationVersion\":2}";