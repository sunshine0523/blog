<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.66" />
    <meta name="theme" content="VuePress Theme Hope" />
    <title>LLaMA 2 论文笔记 | KindBrave</title><meta name="description" content="LLaMA 2 论文笔记">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d1e1f;
      }

      html,
      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="preload" href="/blog/assets/style-a9992be0.css" as="style"><link rel="stylesheet" href="/blog/assets/style-a9992be0.css">
    <link rel="modulepreload" href="/blog/assets/app-2bc3c870.js"><link rel="modulepreload" href="/blog/assets/LLaMA2_paper.html-c2e34b2f.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-c27b6911.js"><link rel="modulepreload" href="/blog/assets/LLaMA2_paper.html-20dcc904.js"><link rel="prefetch" href="/blog/assets/adb_restart.html-aac07c57.js" as="script"><link rel="prefetch" href="/blog/assets/mi_freeform_system_server.html-b3041de2.js" as="script"><link rel="prefetch" href="/blog/assets/arraylist_linkedlist.html-b566d799.js" as="script"><link rel="prefetch" href="/blog/assets/concurrenthashmap.html-dd57e7c7.js" as="script"><link rel="prefetch" href="/blog/assets/copyonwritearraylist.html-3276ded7.js" as="script"><link rel="prefetch" href="/blog/assets/equals_hashcode.html-131eb17d.js" as="script"><link rel="prefetch" href="/blog/assets/fz_jc_dt.html-558e449a.js" as="script"><link rel="prefetch" href="/blog/assets/jvm_garbage_collection.html-33aecdcd.js" as="script"><link rel="prefetch" href="/blog/assets/jvm_memory.html-3f7dc23e.js" as="script"><link rel="prefetch" href="/blog/assets/lock.html-0dc0ebc8.js" as="script"><link rel="prefetch" href="/blog/assets/map.html-86a4798c.js" as="script"><link rel="prefetch" href="/blog/assets/meituan-1.html-3e5afdc8.js" as="script"><link rel="prefetch" href="/blog/assets/thread.html-3cedecc8.js" as="script"><link rel="prefetch" href="/blog/assets/thread_pool.html-5c766e55.js" as="script"><link rel="prefetch" href="/blog/assets/unsafe.html-5dd950a3.js" as="script"><link rel="prefetch" href="/blog/assets/MLOps.html-ca118480.js" as="script"><link rel="prefetch" href="/blog/assets/from_bert_to_glm.html-16c6d298.js" as="script"><link rel="prefetch" href="/blog/assets/glm_tuning_on_mac.html-a2780d3f.js" as="script"><link rel="prefetch" href="/blog/assets/milvus.html-37223629.js" as="script"><link rel="prefetch" href="/blog/assets/retnet.html-b84c8700.js" as="script"><link rel="prefetch" href="/blog/assets/join.html-1ae0461a.js" as="script"><link rel="prefetch" href="/blog/assets/mvcc.html-15a50fe2.js" as="script"><link rel="prefetch" href="/blog/assets/mysql_index.html-8da45921.js" as="script"><link rel="prefetch" href="/blog/assets/mysql_index_not_use.html-cf461dd9.js" as="script"><link rel="prefetch" href="/blog/assets/read_write_subtable.html-9f9ace1c.js" as="script"><link rel="prefetch" href="/blog/assets/transaction_isolation_level.html-ab9270cf.js" as="script"><link rel="prefetch" href="/blog/assets/http_code.html-5c476b5d.js" as="script"><link rel="prefetch" href="/blog/assets/tcp.html-b86fb40c.js" as="script"><link rel="prefetch" href="/blog/assets/tcp_1.html-1ef77a45.js" as="script"><link rel="prefetch" href="/blog/assets/tcp_udp.html-e8227a68.js" as="script"><link rel="prefetch" href="/blog/assets/tmp.html-33bbeefa.js" as="script"><link rel="prefetch" href="/blog/assets/deadlock.html-275d20e8.js" as="script"><link rel="prefetch" href="/blog/assets/io.html-a57e4b4c.js" as="script"><link rel="prefetch" href="/blog/assets/process_schedule.html-2ade4e14.js" as="script"><link rel="prefetch" href="/blog/assets/redis.html-51f959aa.js" as="script"><link rel="prefetch" href="/blog/assets/redis_datatype.html-1004ad3a.js" as="script"><link rel="prefetch" href="/blog/assets/annotation-config-application-context.html-372eb618.js" as="script"><link rel="prefetch" href="/blog/assets/spring-1.html-54e74edd.js" as="script"><link rel="prefetch" href="/blog/assets/spring-2.html-bc7a29c1.js" as="script"><link rel="prefetch" href="/blog/assets/spring-3.html-fda31fa3.js" as="script"><link rel="prefetch" href="/blog/assets/spring-start.html-f768c34a.js" as="script"><link rel="prefetch" href="/blog/assets/spring.html-93811b6c.js" as="script"><link rel="prefetch" href="/blog/assets/169.html-2c578cf3.js" as="script"><link rel="prefetch" href="/blog/assets/26.html-41a4d4ce.js" as="script"><link rel="prefetch" href="/blog/assets/27.html-a1755213.js" as="script"><link rel="prefetch" href="/blog/assets/80.html-22d78cba.js" as="script"><link rel="prefetch" href="/blog/assets/88.html-0a84b913.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-f04ced3e.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-01579e42.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1acd11e1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-e7a2b372.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cbed5c04.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-c98bf757.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0939841b.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-b7f29d71.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5df7b7b4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-a2aeccdb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-694e9269.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-206a6cbf.js" as="script"><link rel="prefetch" href="/blog/assets/adb_restart.html-23e370a0.js" as="script"><link rel="prefetch" href="/blog/assets/mi_freeform_system_server.html-98ecfd3c.js" as="script"><link rel="prefetch" href="/blog/assets/arraylist_linkedlist.html-83868f5c.js" as="script"><link rel="prefetch" href="/blog/assets/concurrenthashmap.html-052d330f.js" as="script"><link rel="prefetch" href="/blog/assets/copyonwritearraylist.html-3c6385d4.js" as="script"><link rel="prefetch" href="/blog/assets/equals_hashcode.html-72ffc81d.js" as="script"><link rel="prefetch" href="/blog/assets/fz_jc_dt.html-98cd915c.js" as="script"><link rel="prefetch" href="/blog/assets/jvm_garbage_collection.html-fcd4d7dd.js" as="script"><link rel="prefetch" href="/blog/assets/jvm_memory.html-8ee619ec.js" as="script"><link rel="prefetch" href="/blog/assets/lock.html-d10edb7b.js" as="script"><link rel="prefetch" href="/blog/assets/map.html-02aa8304.js" as="script"><link rel="prefetch" href="/blog/assets/meituan-1.html-c7103d9c.js" as="script"><link rel="prefetch" href="/blog/assets/thread.html-af63c419.js" as="script"><link rel="prefetch" href="/blog/assets/thread_pool.html-e2abf34e.js" as="script"><link rel="prefetch" href="/blog/assets/unsafe.html-dc97b8cd.js" as="script"><link rel="prefetch" href="/blog/assets/MLOps.html-29a941ec.js" as="script"><link rel="prefetch" href="/blog/assets/from_bert_to_glm.html-22238850.js" as="script"><link rel="prefetch" href="/blog/assets/glm_tuning_on_mac.html-75bd2b81.js" as="script"><link rel="prefetch" href="/blog/assets/milvus.html-4c02e534.js" as="script"><link rel="prefetch" href="/blog/assets/retnet.html-97a033da.js" as="script"><link rel="prefetch" href="/blog/assets/join.html-1f35b445.js" as="script"><link rel="prefetch" href="/blog/assets/mvcc.html-b83b9ff2.js" as="script"><link rel="prefetch" href="/blog/assets/mysql_index.html-99e7971a.js" as="script"><link rel="prefetch" href="/blog/assets/mysql_index_not_use.html-e87f0b0d.js" as="script"><link rel="prefetch" href="/blog/assets/read_write_subtable.html-af9553b0.js" as="script"><link rel="prefetch" href="/blog/assets/transaction_isolation_level.html-5a7e0836.js" as="script"><link rel="prefetch" href="/blog/assets/http_code.html-6f4158b6.js" as="script"><link rel="prefetch" href="/blog/assets/tcp.html-ddf2f554.js" as="script"><link rel="prefetch" href="/blog/assets/tcp_1.html-62837359.js" as="script"><link rel="prefetch" href="/blog/assets/tcp_udp.html-cb1360cf.js" as="script"><link rel="prefetch" href="/blog/assets/tmp.html-8c1439f7.js" as="script"><link rel="prefetch" href="/blog/assets/deadlock.html-a0029573.js" as="script"><link rel="prefetch" href="/blog/assets/io.html-1fcac865.js" as="script"><link rel="prefetch" href="/blog/assets/process_schedule.html-e154c1d3.js" as="script"><link rel="prefetch" href="/blog/assets/redis.html-9d9046f0.js" as="script"><link rel="prefetch" href="/blog/assets/redis_datatype.html-3c9ce992.js" as="script"><link rel="prefetch" href="/blog/assets/annotation-config-application-context.html-3fbb1446.js" as="script"><link rel="prefetch" href="/blog/assets/spring-1.html-581045b9.js" as="script"><link rel="prefetch" href="/blog/assets/spring-2.html-11c51001.js" as="script"><link rel="prefetch" href="/blog/assets/spring-3.html-69936f56.js" as="script"><link rel="prefetch" href="/blog/assets/spring-start.html-99c11b20.js" as="script"><link rel="prefetch" href="/blog/assets/spring.html-9b0d347b.js" as="script"><link rel="prefetch" href="/blog/assets/169.html-6bebc150.js" as="script"><link rel="prefetch" href="/blog/assets/26.html-538cb9cf.js" as="script"><link rel="prefetch" href="/blog/assets/27.html-7a48561d.js" as="script"><link rel="prefetch" href="/blog/assets/80.html-97e73620.js" as="script"><link rel="prefetch" href="/blog/assets/88.html-bf162207.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-7d926f2e.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-aff19a7c.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-c7bb808d.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-8793a58e.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-b34ae44e.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-d60e9989.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-83fc68bc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-a5fdb9d8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4aa30f36.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-aed53e9c.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-e8f4b753.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-25f4e776.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-5794cde2.js" as="script"><link rel="prefetch" href="/blog/assets/SearchResult-fdc574bb.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><!--[--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand" href="/blog/"><!----><!----><span class="vp-site-name">KindBrave</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><!----><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/sunshine0523/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!--[--><button type="button" class="search-pro-button" role="search" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">搜索</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable active" type="button"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="simple-icons:openai" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">LLM</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><!--[--><a class="vp-link nav-link active vp-sidebar-link vp-sidebar-page active" href="/blog/llm/LLaMA2_paper.html"><!---->LLaMA 2 论文笔记<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_1-介绍"><!---->1 介绍<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_2-预训练方法"><!---->2 预训练方法<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_2-1-训练数据"><!---->2.1 训练数据<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_2-2-训练详情"><!---->2.2 训练详情<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#a-2-1-额外的预训练信息之与llama-1的变化内容介绍-p-46"><!---->A.2.1 额外的预训练信息之与LLaMA 1的变化内容介绍 p.46<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#预训练超参设置"><!---->预训练超参设置<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#预训练tokenizer"><!---->预训练Tokenizer<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_2-3-评测"><!---->2.3 评测<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_3-fine-tuning"><!---->3. Fine-tuning<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_3-1-supervised-fine-tuning-sft"><!---->3.1 Supervised Fine-Tuning (SFT)<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_3-2-rlhf"><!---->3.2 RLHF<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_3-2-1-人类偏好数据收集"><!---->3.2.1 人类偏好数据收集<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_3-2-2-奖励建模"><!---->3.2.2 奖励建模<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_3-2-3-迭代微调"><!---->3.2.3 迭代微调<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_3-3-多轮一致性的指令"><!---->3.3 多轮一致性的指令<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_3-4-rlhf结果"><!---->3.4 RLHF结果<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_3-4-1-基于模型的评估"><!---->3.4.1 基于模型的评估<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a class="vp-link nav-link vp-sidebar-link vp-heading" href="/blog/llm/LLaMA2_paper.html#_3-4-2-人类评价"><!---->3.4.2 人类评价<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li></ul></li></ul><!--]--></li><li><!--[--><a class="vp-link nav-link vp-sidebar-link vp-sidebar-page" href="/blog/llm/MLOps.html"><!---->MLOps<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a class="vp-link nav-link vp-sidebar-link vp-sidebar-page" href="/blog/llm/retnet.html"><!---->RetNet 论文笔记<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a class="vp-link nav-link vp-sidebar-link vp-sidebar-page" href="/blog/llm/from_bert_to_glm.html"><!---->从BERT到GLM，NLP经历了什么？<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a class="vp-link nav-link vp-sidebar-link vp-sidebar-page" href="/blog/llm/glm_tuning_on_mac.html"><!---->在Apple芯片上微调GLM模型<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a class="vp-link nav-link vp-sidebar-link vp-sidebar-page" href="/blog/llm/milvus.html"><!---->在Docker中使用Milvus<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="material-symbols:android" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">Android</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="ri:java-fill" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">Java</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="mdi:spring" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">Spring</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="mdi:network" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">计算机网络</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="mdi:os" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">操作系统</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="simple-icons:mysql" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">MySQL</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="simple-icons:redis" width="1em" height="1em"></iconify-icon><span class="vp-sidebar-title">Redis</span><span class="vp-arrow end"></span></button><!----></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->LLaMA 2 论文笔记</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/sunshine0523" target="_blank" rel="noopener noreferrer">KindBrave</a></span><span property="author" content="KindBrave"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2024-02-15T03:42:42.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 16 分钟</span><meta property="timeRequired" content="PT16M"></span><!----><!----></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2" href="/blog/#_1-介绍">1 介绍</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2" href="/blog/#_2-预训练方法">2 预训练方法</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/blog/#_2-1-训练数据">2.1 训练数据</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/blog/#_2-2-训练详情">2.2 训练详情</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level4" href="/blog/#a-2-1-额外的预训练信息之与llama-1的变化内容介绍-p-46">A.2.1 额外的预训练信息之与LLaMA 1的变化内容介绍 p.46</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level4" href="/blog/#预训练超参设置">预训练超参设置</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level4" href="/blog/#预训练tokenizer">预训练Tokenizer</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/blog/#_2-3-评测">2.3 评测</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2" href="/blog/#_3-fine-tuning">3. Fine-tuning</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/blog/#_3-1-supervised-fine-tuning-sft">3.1 Supervised Fine-Tuning (SFT)</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/blog/#_3-2-rlhf">3.2 RLHF</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level4" href="/blog/#_3-2-1-人类偏好数据收集">3.2.1 人类偏好数据收集</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level4" href="/blog/#_3-2-2-奖励建模">3.2.2 奖励建模</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level4" href="/blog/#_3-2-3-迭代微调">3.2.3 迭代微调</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/blog/#_3-3-多轮一致性的指令">3.3 多轮一致性的指令</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3" href="/blog/#_3-4-rlhf结果">3.4 RLHF结果</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level4" href="/blog/#_3-4-1-基于模型的评估">3.4.1 基于模型的评估</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level4" href="/blog/#_3-4-2-人类评价">3.4.2 人类评价</a></li><!----><!--]--></ul></li><!--]--></ul></li><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!----><div class="theme-hope-content"><h1 id="llama-2-论文笔记" tabindex="-1"><a class="header-anchor" href="#llama-2-论文笔记" aria-hidden="true">#</a> LLaMA 2 论文笔记</h1><p>该论文篇幅巨大，非常详细地介绍了LLaMA 2的预训练和微调过程，本篇笔记对其内容进行简要记录。<a href="https://arxiv.org/abs/2307.09288" target="_blank" rel="noopener noreferrer">论文链接<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><h2 id="_1-介绍" tabindex="-1"><a class="header-anchor" href="#_1-介绍" aria-hidden="true">#</a> 1 介绍</h2><p>LLaMA 2共包括两大版本：预训练模型LLaMA 2和基于它的微调模型LLaMA 2-Chat。共有7B 13B 34B 70B四种参数规模。</p><p>LLaMA 2，是LLaMA 1的升级版本，在多个公开可获得数据上进行训练。相比LLaMA 1，LLaMA 2预训练的语料库增加了40%，上下文长度（即可接受的输入长度）增加一倍（最多支持4K token)，并且采用了分组查询注意力（grouped-query，后续有介绍）机制。</p><p>LLaMA 2-Chat是基于LLaMA 2预训练模型微调得来的，后续有详细介绍。</p><p>在第2节，本文介绍LLaMA 2的预训练方法；在第3节，本文介绍LLaMA 2-Chat的微调方法；</p><h2 id="_2-预训练方法" tabindex="-1"><a class="header-anchor" href="#_2-预训练方法" aria-hidden="true">#</a> 2 预训练方法</h2><p>LLaMA 2的预训练特性如下：</p><ul><li>LLaMA 2基本还是采用LLaMA 1的训练方法：<a href="https://arxiv.org/abs/2302.13971" target="_blank" rel="noopener noreferrer">LLaMA 1论文链接<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li>优化的自回归Transformer（归一化、激活函数等有变化，下面有介绍）</li><li>具体而言，LLaMA 2执行了更稳健的数据清理，更新了数据混合，对总token进行了40%以上的训练，将上下文长度增加了一倍，并使用分组查询注意力（GQA）来提高LLaMA 2的推理可扩展性</li></ul><p><img src="/blog/assets/1689901807800-063ba982.png" alt="1689901807800" title="训练数据，注意LR"></p><p>（2T = 2 trillion = 2万亿）</p><h3 id="_2-1-训练数据" tabindex="-1"><a class="header-anchor" href="#_2-1-训练数据" aria-hidden="true">#</a> 2.1 训练数据</h3><p>LLaMA 2的训练数据来自混合后的公开数据，共在2T token上进行训练。</p><h3 id="_2-2-训练详情" tabindex="-1"><a class="header-anchor" href="#_2-2-训练详情" aria-hidden="true">#</a> 2.2 训练详情</h3><p>预训练的设置和模型架构和LLaMA 1基本一致：</p><ul><li>Transformer架构</li><li>归一化 RMSNorm</li><li>激活函数 SwiGLU</li><li>旋转位置Embedding RoPE rotary positional embeddings，这个现在都在用，包括GLM</li><li>与LLaMA 1差异：增加了上下文长度、增加了GQA</li></ul><h4 id="a-2-1-额外的预训练信息之与llama-1的变化内容介绍-p-46" tabindex="-1"><a class="header-anchor" href="#a-2-1-额外的预训练信息之与llama-1的变化内容介绍-p-46" aria-hidden="true">#</a> A.2.1 额外的预训练信息之与LLaMA 1的变化内容介绍 p.46</h4><ol><li><strong>上下文长度</strong> 更长的上下文长度可以让模型处理更多信息，这可以让模型支持记住更多对话历史信息、更多的总结任务、理解更长的文本</li><li><strong>Grouped-Query Attention</strong> 自回归解码的标准做法是缓存序列中先前token的key（K）和value（V）对，从而加快注意力计算。然而，随着context window或batch size的增加，与多头注意力（MHA）模型中的KV缓存大小相关的<strong>内存成本</strong>显著增长。对于KV缓存大小成为瓶颈的大型模型，<strong>可以在多个头之间共享key和value预测</strong>，而不会导致性能大幅下降。可以使用具有单个KV投影的原始多查询格式(MQA)或具有8KV投影的分组查询注意力(GQA)变体。基于消融结果和易于缩放推断，对于34B和70B Llama 2模型，LLaMA 2选择使用GQA而不是MQA。</li><li><strong>额外发现</strong>：一个多卡并行训练的论文：Training multi-billion parameter language models using model parallelism</li></ol><h4 id="预训练超参设置" tabindex="-1"><a class="header-anchor" href="#预训练超参设置" aria-hidden="true">#</a> 预训练超参设置</h4><ul><li>AdamW优化器 β1=0.9，β2=0.95，eps=10e-5</li><li>余弦学习率，warmup 2000 steps</li><li>将最终学习率降低到峰值学习率的10%</li><li>使用0.1的权重衰减和1.0的梯度剪裁</li></ul><h4 id="预训练tokenizer" tabindex="-1"><a class="header-anchor" href="#预训练tokenizer" aria-hidden="true">#</a> 预训练Tokenizer</h4><p>使用与LLaMA 1相同的标记器；它采用了字节对编码（BPE）算法，使用了来自SentencePiece的实现。与LLaMA 1一样，将所有数字拆分为单个数字，并使用字节分解未知的UTF-8字符。总词汇大小为32k个标记。</p><h3 id="_2-3-评测" tabindex="-1"><a class="header-anchor" href="#_2-3-评测" aria-hidden="true">#</a> 2.3 评测</h3><p>评测结果如下：</p><p><img src="/blog/assets/1689906367237-4915f5e8.png" alt="1689906367237"></p><p><img src="/blog/assets/1689906551035-d9b16f32.png" alt="1689906551035"></p><h2 id="_3-fine-tuning" tabindex="-1"><a class="header-anchor" href="#_3-fine-tuning" aria-hidden="true">#</a> 3. Fine-tuning</h2><ul><li>LLaMA 2-Chat基于LLaMA 2微调而来，包括指令微调和RLHF</li><li>本节报告了使用监督微调以及初始和迭代奖励建模和RLHF进行的实验和发现。</li><li>提出一种新技术，Ghost Attention (GAtt)，用于帮助控制多轮对话流</li></ul><h3 id="_3-1-supervised-fine-tuning-sft" tabindex="-1"><a class="header-anchor" href="#_3-1-supervised-fine-tuning-sft" aria-hidden="true">#</a> 3.1 Supervised Fine-Tuning (SFT)</h3><p>开始：用公开可获得的<strong>指令微调数据</strong>，和LLaMA 1一致，使用了 <a href="https://arxiv.org/abs/2210.11416" target="_blank" rel="noopener noreferrer">Scaling Instruction-Finetuned Language Models<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>公开的指令微调数据质量参差不齐，首先就是要收集大量的高质量SFT数据，如下图：</p><p><img src="/blog/assets/1689907443725-2179d9b4.png" alt="1689907443725"></p><p>高质量指令微调数据，即使是少量的，也可以让结果很好。万级别的好的数据足够了，Meta总共收集了27540个好数据。</p><ul><li>对于监督微调，使用了余弦学习率，LR=2e-5，权重衰减=0.1，batch size = 64，sequence lenght = 4096</li><li>对于微调过程，每个样本都包含一个提示和一个答案。为了确保模型序列长度正确填充，作者<strong>将训练集中的所有提示和答案连接起来</strong>。使用一个特殊的令牌来分隔提示段和应答段。作者使用自回归目标，并从用户提示中消除令牌的损失，因此，作者只对回答令牌进行反向传播。最后，作者对模型进行了2个epochs的微调。</li></ul><h3 id="_3-2-rlhf" tabindex="-1"><a class="header-anchor" href="#_3-2-rlhf" aria-hidden="true">#</a> 3.2 RLHF</h3><h4 id="_3-2-1-人类偏好数据收集" tabindex="-1"><a class="header-anchor" href="#_3-2-1-人类偏好数据收集" aria-hidden="true">#</a> 3.2.1 人类偏好数据收集</h4><p>与其他方案相比，作者选择了二进制比较协议，主要是因为它使作者能够最大限度地提高收集到的提示的多样性。作者的注释过程如下。作者要求注释器首先编写一个提示，然后根据提供的标准在两个采样的模型响应之间进行选择。为了最大限度地提高多样性，从两个不同的模型变量中对给定提示的两个响应进行采样，并改变<strong>温度</strong>超参数。除了给参与者一个被迫的选择之外，作者还要求注释者标注他们更喜欢自己选择的回答而不是选择的程度：要么他们的选择明显更好，要么更好，要么稍微好一点，要么好到可以忽略不计/不确定。</p><p>（就是给个输入，然后有两种输出，看哪个更符合标准）</p><ul><li><p>用到的一些人类偏好开源数据集</p><p><img src="/blog/assets/1690115105928-7943dd00.png" alt="1690115105928"></p></li></ul><h4 id="_3-2-2-奖励建模" tabindex="-1"><a class="header-anchor" href="#_3-2-2-奖励建模" aria-hidden="true">#</a> 3.2.2 奖励建模</h4><p>奖励建模就是拿一个模型的结果和它相关的Prompt作为输入，然后输出一个分数来表明这个结果的质量（有用性、安全性等），用这个分数，就可以在RLHF中优化模型了</p><p><strong>为了训练奖励模型</strong>，作者将收集的成对人类偏好数据转换为二元排名标签格式（即选择和拒绝），并强制选择的响应比对应的响应具有更高的分数。作者使用了二元排名损失：</p><div align="center"><img src="/blog/assets/1690116093348-d3ebcf70.png"></div><p><em>where rθ(x, y) is the scalar score output for prompt x and completion y with model weights θ. yc is the preferred response that annotators choose and yr is the rejected counterpart.</em></p><p>在这种二元排名损失的基础上，作者进一步修改它，如第3.2.1节所示，利用这些信息来明确教导奖励模型为具有更多差异的世代分配更多不一致的分数可能是有用的。为此，作者在损失中进一步添加了一个margin成分：</p><div align="center"><img src="/blog/assets/1690116380614-60f5a982.png"></div><p><em>where the margin m(r) is a discrete function of the preference rating. Naturally, we use a large margin for pairs with distinct responses, and a smaller one for those with similar responses (shown in Table 27). We found this margin component can improve Helpfulness reward model accuracy especially on samples where two responses are more separable. More detailed ablation and analysis can be found in Table 28 in Appendix A.3.3.</em></p><p><img src="/blog/assets/1690116520536-21f280e9.png" alt="1690116520536"></p><p><strong>奖励建模的训练详情</strong>。LLaMA 2-Chat在训练数据上训练一个epoch。在早期的实验中，<strong>发现训练时间过长会导致过度拟合</strong>。LLaMA 2-Chat使用与基本模型相同的优化器参数。70B参数LLaMA 2-Chat的最大学习率为5×10−6，其余参数为1×10−5。学习率按余弦学习率计划降低，降至最大学习率的10%。 LLaMA 2-chat使用占总步数3%的warm-up，最少5 steps。有效batch大小固定为512对，即每batch 1024行。</p><p><strong>奖励建模的结果</strong>。在每一batch用于奖励建模的人类偏好注释上，<strong>都拿出1000个例子作为测试集来评估模型(这个都是可以学习的地方，按他的来)</strong>。作者将相应测试集的所有提示的并集分别称为“Meta Helpfulness”和“Meta Safety”。总体而言，这个奖励模型优于所有base-line，包括GPT-4。</p><p><img src="/blog/assets/1690117393292-ff7c5806.png" alt="1690117393292"></p><p>当作者在表8中按偏好评级对分数进行分组时，我们可以看到“明显更好”的测试集，并随着比较对变得更加相似而逐渐退化（例如，“稍微好一点”）。当在两个相似的模型反应之间做出决定时，由于注释者的主观性和他们对可能区分反应的细微细节的依赖，学习对人类偏好进行建模将变得具有挑战性。作者强调，对于提高LLaMA 2-Chat的性能，更明显的响应的准确性最为重要。与相似对相比，在更明显的反应上，人类偏好注释一致率也更高。</p><p><img src="/blog/assets/1690117652948-4b32e083.png" alt="1690117652948"></p><h4 id="_3-2-3-迭代微调" tabindex="-1"><a class="header-anchor" href="#_3-2-3-迭代微调" aria-hidden="true">#</a> 3.2.3 迭代微调</h4><p>作者通过两个主要的算法来探索RLHF微调：</p><ul><li><strong>近端策略优化</strong>(Proximal Policy Optimization, PPO)，<a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener noreferrer">论文链接<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，该算法是RLHF文献的标准。</li><li><strong>拒绝采样微调</strong>(Rejection Sampling fine-tuning)，作者对模型中的K个输出进行采样，并用我们的奖励选择最佳候选者，这与<a href="https://arxiv.org/abs/2212.08073" target="_blank" rel="noopener noreferrer">Constitutional AI: Harmlessness from AI Feedback<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>保持一致。<a href="https://arxiv.org/abs/2004.11714" target="_blank" rel="noopener noreferrer">Residual Energy-Based Models for Text Generation<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>也提出了同样的LLM重新排序策略，其中奖励被视为能量函数。在这里，作者更进一步，使用选定的输出进行梯度更新。对于每个Prompt，获得最高奖励分数的样本被视为新的金标准。与<a href="https://arxiv.org/abs/2002.10375" target="_blank" rel="noopener noreferrer">Discriminative Adversarial Search for Abstractive Summarization<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>类似，然后作者在新的一组排序样本上微调模型，以增强奖励。</li></ul><p>这两个强化学习(RL)算法的主要不同：</p><ul><li>广度。在拒绝采样中，该模型为给定提示探索K个样本，而对PPO只进行一次生成。</li><li>深度。在PPO中，在步骤t的训练期间，样本是前一步骤的梯度更新后从t−1更新的模型策略的函数。在拒绝采样微调中，在应用类似于SFT的微调之前，作者在给定模型的初始策略的情况下对所有输出进行采样，以收集新的数据集。然而，由于作者应用了迭代模型更新，两种RL算法之间的基本差异就不那么明显了。</li></ul><p><strong>拒绝采样的介绍</strong>。作者只对最大的70B LLaMA 2-Chat进行拒绝采样。所有较小的模型都根据较大模型的拒绝采样数据进行微调，从而将较大模型的能力提取到较小的模型中。在每个迭代过程，作者从最新的模型中对每个Prompt采样K个答案，作者为每个样本打分，给出实验时可访问的最佳奖励模型，然后为给定提示选择最佳答案。</p><p>作者在图7中说明了拒绝采样的好处。最大曲线和中值曲线之间的增量可以解释为对最佳输出进行微调的潜在增益。正如预期的那样，这个增量随着样本的增加而增加，因为最大值增加（即，更多的样本，产生良好轨迹的机会更多），而中值保持不变。探索和作者能在样本中获得的最大回报之间有着直接的联系。<strong>温度</strong>参数对勘探也起着重要作用，因为更高的温度使作者能够对更多样的输出进行采样。</p><p><img src="/blog/assets/1690180686481-aa4ae24d.png" alt="1690180686481"></p><p>在图8中，作者报告了LLaMA 2-Chat-SFT（左）和LLaMA 2-Chat-RLHF（右），N个样本（其中N∈[1，…，100]）在不同温度下的最大回报曲线。可以观察到，在迭代模型更新过程中，最佳温度不是恒定的：RLHF对重新缩放温度有直接影响。对于LLaMA 2-Chat-RLHF，当在10到100个输出之间采样时，最佳温度为T∈[1.2，1.3]。因此，在有限的计算预算下，有必要逐步重新调整温度。请注意，对于每个模型，这种温度重新缩放都会发生恒定数量的步骤，并且总是从每个新RLHF版本的基本模型开始。</p><p><img src="/blog/assets/1690180884729-78350130.png" alt="1690180884729"></p><p><strong>PPO介绍</strong>。对于所有模型，作者使用AdamW优化器，其中β1=0.9，β2=0.95，eps=10−5。使用0.1的权重衰减(weight decay)、1.0的梯度剪裁(gradient clipping)和10e−6的恒定学习率(lr) 对于每个PPO迭代，我们使用512的批量大小(batch size)、0.2的PPO剪辑阈值、64的小批量大小(mini-batch size)，并且每个小批量采取一个梯度步骤(step)。对于7B和13B模型，我们设置β=0.01（KL惩罚），对于34B和70B模型，设置β=0.005。</p><p>作者为所有模型进行了200到400次迭代的训练，并对延迟的提示进行了评估，以提前停止。70B模型上的PPO每次迭代平均耗时≈330秒。为了快速进行大批量训练，作者使用<a href="https://arxiv.org/abs/2304.11277" target="_blank" rel="noopener noreferrer">PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>。这在使用O（1）正向或反向传播时是有效的，但在生成过程中会导致很大的减慢（≈20×），即使在使用大批量和KV缓存时也是如此。作者能够通过在生成之前将模型权重合并到每个节点一次，然后在生成之后释放内存，恢复训练循环的其余部分来缓解这种情况。</p><h3 id="_3-3-多轮一致性的指令" tabindex="-1"><a class="header-anchor" href="#_3-3-多轮一致性的指令" aria-hidden="true">#</a> 3.3 多轮一致性的指令</h3><p>在对话设置中，一些指示应适用于所有的对话轮数中，例如，简洁地回应，或“扮演”某个公众人物。然而，在初始版本的RLHF模型中，LLaMA 2-Chat会忘记指示，如下图左侧（右侧是使用GAtt优化后的结果）：</p><p><img src="/blog/assets/1690184734486-baa55065.png" alt="1690184734486"></p><p>为了解决这些限制，作者提出了Ghost Attention（GAtt），这是一种受<a href="https://arxiv.org/abs/2212.08073" target="_blank" rel="noopener noreferrer">上下文蒸馏，Constitutional AI: Harmlessness from AI Feedback<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>启发的非常简单的方法，它可以破解微调数据，以帮助在多阶段过程中集中注意力。GAtt允许对多轮进行对话控制，如图9（右）所示。</p><p><strong>Gatt方法</strong>。假设可以访问两个人（例如，用户和助手）之间的多回合对话数据集，该数据集具有消息列表[u1，a1，…，un，an]，其中un和an分别对应于回合n的用户和助手消息。然后定义了一个指令，inst，应该贯穿在整个对话中。例如，inst可以是“扮演...”。然后可以将此指令综合连接到会话的所有用户消息。</p><p>接下来可以使用最新的RLHF模型对这些合成数据进行采样。现在有了一个上下文对话和样本，可以在类似于拒绝采样的过程中对模型进行微调。可以在除第一个回合外的所有回合中放弃它，而不是用指令来增加所有上下文对话回合，但这会导致系统消息（即最后一个回合之前的所有中间辅助消息）与样本在训练时间不匹配。为了解决这个可能影响训练的问题，只需将前几轮中的所有token（包括助手的消息）的loss设置为0。</p><p>对于训练指令，作者创建了一些综合约束条件：爱好（“你喜欢例如网球”）、语言（“用例如法语说话”）或公众人物（“扮演例如拿破仑”）。为了获得兴趣爱好和公众人物的列表，作者要求LLaMA 2-Chat生成它，以避免教学和模型知识之间的不匹配（例如，要求模型扮演训练中没有遇到的人）。为了使指令更加复杂和多样化，作者通过随机组合上述约束来构建最终指令。在为训练数据构建最终系统消息时，作者也会在一半的时间内修改原始指令，使其不那么冗长，例如，“从现在起始终充当拿破仑”-&gt; “人物：拿破仑。”<strong>这些步骤生成了一个SFT数据集，可以在该数据集上微调LLaMA 2-Chat</strong>。</p><p>为了说明GAtt如何在微调过程中帮助重塑注意力，作者在下图中显示了模型的最大注意力激活。每个图的左侧对应系统信息（“Act as Oscar Wilde，扮演奥斯卡·王尔德”）。我们可以看到，与没有GAtt的模型（左）相比，配备GAtt的型号（右）在对话的大部分时间里保持了对系统消息的大量注意力激活。（？差别很大吗）</p><p><img src="/blog/assets/1690185760173-5f74d722.png" alt="1690185760173"></p><h3 id="_3-4-rlhf结果" tabindex="-1"><a class="header-anchor" href="#_3-4-rlhf结果" aria-hidden="true">#</a> 3.4 RLHF结果</h3><h4 id="_3-4-1-基于模型的评估" tabindex="-1"><a class="header-anchor" href="#_3-4-1-基于模型的评估" aria-hidden="true">#</a> 3.4.1 基于模型的评估</h4><p>模型的进展。图11报告了作者针对安全和帮助轴的不同SFT和RLHF版本的进展，通过Meta内部的安全和帮助奖励模型进行测量。在这组评估中，RLHF-V3之后的两个轴上都优于ChatGPT（无害和有用&gt;50%）。尽管前面提到了使用Meta的奖励作为逐点衡量标准的相关性，但可以说，它可能偏向于LLaMA 2-Chat。因此，为了进行公平的比较，作者使用GPT-4额外计算最终结果，以评估哪一代是优选的。ChatGPT和LLaMA 2-Chat输出在GPT-4提示中出现的顺序是随机交换的，以避免任何偏差。正如预期的那样，支持LLaMA 2-Chat的胜率不那么明显，尽管我们最新的LLaMA 2-Chat获得了超过60%的胜率。</p><p><img src="/blog/assets/1690198095192-227d4983.png" alt="1690198095192"></p><h4 id="_3-4-2-人类评价" tabindex="-1"><a class="header-anchor" href="#_3-4-2-人类评价" aria-hidden="true">#</a> 3.4.2 人类评价</h4><p>人类评价通常被认为是评判自然语言生成模型（包括对话模型）的黄金标准。为了评估主要模型版本的质量，作者要求人类评估人员对其有用性和安全性进行评分。作者将LLaMA 2-Chat模型与开源模型（Falcon，MPT ，Vicuna）以及4000多个单回合和多回合提示的闭源模型（ChatGPT和PaLM）进行了比较。对于ChatGPT，作者使用gpt-3.5-turbo-0301型号。对于PaLM，作者使用chat-bison-001模型。</p><p>结果如图12所示，LLaMA 2-Chat模型在单回合和多回合提示上都显著优于开源模型。特别是，LLaMA 2-Chat 7B模型在60%的提示上优于MPT-7B-Chat。LLaMA 2-Chat 34B与同等尺寸的Vicuna-33B和Falcon 40B型号相比，总体胜率超过75%。 最大的LLaMA 2-Chat模型与ChatGPT具有竞争力。LLaMA 2-Chat 70B模型相对于ChatGPT的胜率为36%，平局率为31.5%。在我们的提示集上，LLaMA 2-Chat 70B模型在很大程度上优于PaLM bison聊天模型。</p><p><img src="/blog/assets/1690198482672-90cee8bc.png" alt="1690198482672"></p></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/sunshine0523/blog/edit/main/docs/llm/LLaMA2_paper.md" rel="noopener noreferrer" target="_blank" aria-label="编辑此页" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->编辑此页<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: 1773747161@qq.com">KindBrave</span><!--]--><!--]--></div></div></footer><nav class="vp-page-nav"><!----><a class="vp-link nav-link next" href="/blog/llm/MLOps.html"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">MLOps<!----></div></a></nav><!----><!----><!--]--></main><!--]--><!----></div><!--]--><!--]--><!----><!----><!--]--></div>
    <script type="module" src="/blog/assets/app-2bc3c870.js" defer></script>
  </body>
</html>
